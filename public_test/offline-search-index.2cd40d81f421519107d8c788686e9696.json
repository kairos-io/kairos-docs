[{"body":"There are many different artifacts that Kairos produces. In this page we try to summarize them all and have a clear naming convention for them.\nInfo Architecture names are based on the Go architecture names, so amd64 is used instead of x86_64, 386 instead of i386 and arm64 instead of aarch64. Images OS images are stored in GitHub Releases, so the name of the artifact includes all the information about the image. The only exception is when the image is too big to be stored in GitHub Releases, in that case the image is stored in Quay.io.\nThe format of the name is the following:\nkairos-\u003cflavor\u003e-\u003cflavor_release\u003e-\u003cvariant\u003e-\u003carch\u003e-\u003cdevice\u003e-\u003cversion\u003e.\u003cextension\u003e Where:\n\u003cflavor\u003e: Underlying Linux distribution e.g. ubuntu, debian, fedora, alpine, etc \u003cflavor_release\u003e: The version of the above distribution e.g. 23.04 for Ubuntu \u003cvariant\u003e: core or standard \u003carch\u003e: amd64 or arm64 \u003cdevice\u003e: either the specific device name, e.g. rpi4 or generic for generic images \u003cversion\u003e: the version of Kairos e.g. v1.0.0 for Core, and can also include the K3S version e.g. v1.0.0-k3s1 for Standard \u003cextension\u003e: iso, squashfs, ipxe K3S Version Policy As mentioned before, the version for the standard image includes both the Kairos and the K3S version e.g. v3.0.1-k3sv1.28.2+k3s1. Every Kairos release ships 3 K3S versions, e.g.: 1.27.3, 1.28.2, 1.29.1.\nWhen we bump the version we bump all three of them to their latest available patch release, or if there’s a newer minor release, then we bump the smallest of all to the latest minor and the other two to the latest patch. Let’s say in our previous example that 1.30.0 is released upstream and with it versions 1.27.4, 1.28.3 and 1.29.2, then the next Kairos release would not ship 1.27.3, would add 1.30.0 and bump the two in between to 1.28.3 and 1.29.2. The resulting versions would be: v3.1.0-k3sv1.28.3+k3s1, v3.1.0-k3sv1.29.2+k3s1 and v3.1.0-k3sv1.30.0+k3s1.\nKeep in mind that:\nThis means that not all K3S releases will make it to a Kairos release. We will try not to bump the K3S minor version during Kairos patch releases, but this will depend on the release cycle and urgency of a K3S bump like in the case of security. Examples kairos-ubuntu-23.04-core-amd64-generic-master kairos-ubuntu-23.04-standard-arm64-rpi4-master-k3sv1.28.2+k3s1.iso Kernel and RAM Disk Images Kernel and RAM Disk images are stored in GitHub Releases and follow a similar convention to images, but they have no extension:\nkairos-\u003cflavor\u003e-\u003cflavor_release\u003e-\u003cvariant\u003e-\u003carch\u003e-\u003cdevice\u003e-\u003cversion\u003e-\u003ctype\u003e Where:\n\u003ctype\u003e: kernel or initrd Examples kairos-ubuntu-23.04-core-amd64-generic-master-kernel kairos-ubuntu-23.04-standard-arm64-rpi4-master-k3sv1.28.2+k3s1-initrd iPXE Images For iPXE we deliver three types of artifacts. The first one is the iPXE script, with the ipxe extension, and the other two are iPXE bootable images, with the iso and img extensions.\nExamples kairos-ubuntu-23.04-core-amd64-generic-master.ipxe kairos-ubuntu-23.04-core-amd64-generic-master-ipxe.iso kairos-ubuntu-23.04-core-amd64-generic-master-ipxe-usb.img Reports Reports are also stored in GitHub Releases and follow a similar convention to images, but they include the name of the report:\nkairos-\u003cflavor\u003e-\u003cflavor_release\u003e-\u003cvariant\u003e-\u003carch\u003e-\u003cdevice\u003e-\u003cversion\u003e-\u003creport\u003e.\u003cextension\u003e Where:\n\u003creport\u003e: trivy, sbom, grype, etc Examples kairos-alpine-3.18-core-arm64-generic-master-trivy.sarif kairos-alpine-3.18-core-arm64-generic-master-grype.json Container Images Container images are stored in Quay.io and follow the following convention:\nquay.io/kairos/\u003cflavor\u003e:\u003cflavor_release\u003e-\u003cvariant\u003e-\u003carch\u003e-\u003cdevice\u003e-\u003cversion\u003e This nomenclature for container images lacks some information for the following reasons:\nAll version information (kairos, k3s, etc) is in the tag. This makes it easier to publish new versions under the same container repository. The name Kairos is already part of the repository name, so it is not included in the image name. Examples quay.io/kairos/@flavor:@flavorRelease-core-amd64-generic-master IMG Images As mentioned before, some images are too big to be delivered via GitHub Releases, so they are stored in Quay.io. At the moment this is only for arm .img images.\nThe convention is the following:\nquay.io/kairos/\u003cflavor\u003e:\u003cflavor_release\u003e-\u003cvariant\u003e-\u003carch\u003e-\u003cdevice\u003e-\u003cversion\u003e-img Examples quay.io/kairos/@flavor:@flavorRelease-core-arm64-rpi4-master-img Binaries \u003cname\u003e-\u003cversion\u003e-\u003cos\u003e-\u003carch\u003e.\u003cextension\u003e Where:\n\u003cname\u003e: the name of the binary e.g. kairosctl, kairos-agent, provider-kairos, etc \u003cversion\u003e: the version of the binary e.g. v1.0.0 \u003cos\u003e: Linux, Windows, Darwin, etc \u003carch\u003e: amd64, arm64, 386, etc \u003cextension\u003e: tar.gz, zip, etc Examples kairosctl-v2.3.0-Linux-386.tar.gz provider-kairos-2.3.0-Windows-amd64.tar.gz kairos-agent-v2.1.10-Linux-arm64.tar.gz ","categories":"","description":"Detailed information about how we name our artifacts including repositories.","excerpt":"Detailed information about how we name our artifacts including …","ref":"/docs/reference/artifacts/","tags":"","title":"Artifact Naming Convention"},{"body":"This section contains refrences to how Kairos works internally.\nSetup process kairos node at first boot will start the kairos-agent service, you can always check what’s happening by running journalctl -fu kairos-agent.\nThis service will setup k3s and edgevpn dynamically on first-boot, once it configures the machine it does not run on boot anymore, unless /usr/local/.kairos/deployed is removed..\nThose are the steps executed in sequence by the kairos-agent service:\nWill create a edgevpn@kairos service and enabled on start. The configuration for the connection is stored in /etc/systemd/system.conf.d/edgevpn-kairos.env and depends on the cloud-init configuration file provided during installation time Automatic role negotiation starts, nodes will co-ordinate for an IP and a role Once roles are defined a node will either set the k3s or k3s-agent service. Configuration for each service is stored in /etc/sysconfig/k3s and /etc/sysconfig/k3s-agent respectively Paths The following paths are relevant for Kairos:\nPath Description /usr/local/.kairos/deployed Sentinel file written after bootstrapping is complete. Remove to retrigger automatic bootstrap /usr/local/.kairos/lease IP Lease of the node in the network. Delete to change IP address of the node ","categories":"","description":"Explore the internal design of Kairos and how its modular, container-based approach supports secure, reproducible infrastructure.","excerpt":"Explore the internal design of Kairos and how its modular, …","ref":"/docs/reference/architecture/","tags":"","title":"Architecture"},{"body":"Here you can find development notes intended for maintainers and guidance for new contributors.\nRepository structure Kairos uses Docker as its primary build system, allowing you to build Kairos images seamlessly in any environment—there is no need for a top-level Makefile. However, individual components within Kairos may use their own build systems or Makefiles as needed.\nThe main Kairos build process is managed by kairos-init, a tool that transforms a typical (non-immutable) OS image into a Kairos image. Within kairos-init, you’ll find all the essential building blocks that make up a Kairos system, such as immucore, the kairos-agent, and more. Each of these components is included at a specific, pinned version, since they are developed and released on their own schedule. The combination of these pinned versions is what defines a particular Kairos release.\nThe Kairos repository - contains the build definitions for releasing Kairos artifacts and testing changes to Kairos. The kairos-agent repository contains the kairos-agent code which is the Operations interface. IT deals with installing, upgrading, reseting and so on. The provider-kairos repository contains the kairos provider component which uses the SDK to bring up a Kubernetes cluster with k3s. Build Kairos To build a Kairos OS you only need Docker and the Dockerfile from the Kairos repo under images/Dockerfile\nBuilding Kairos is a 2 step process, on the first one we generate the OCI artifact with the actual system on it. That’s the heart of Kairos, everything on Kairos comes from an OCI artifact. Then the second step is converting that image into a consumable artifact like an ISO or a Raw Disk image.\nTo build the OCI artifact you can run a docker build with a given base image that you want your artifact to be based on and a version for internal tracking, for example:\ndocker build -t kairosDev:v1.0.0 --build-arg VERSION=v1.0.0 --build-arg BASE_IMAGE=@baseImage -f images/Dockerfile . To build a Kairos ISO, you just call AuroraBoot with the generated OCI artifact:\ndocker run --rm -v /var/run/docker.sock:/var/run/docker.sock -v $PWD/build/:/output \\ quay.io/kairos/auroraboot:v0.5.0 build-iso --output /output/ docker:myBaseKairos:v1.0.0 You can see some example of builds in either the kairos-init repo, which builds a series of OCI containers of different base images, variants and platforms or on the Kairos repo itself which generates not only different types of platforms and variants and base images but also generates different types of artifacts, like ISOs, Trusted Boot ISOs, Trusted Boot upgrade artifacts, raw disk images and so on.\nFor more information on kairos-init, see the kairos factory documentation\nNew controllers Kairos-io adopts operator-sdk.\nTo install operator-sdk locally you can use the kairos repositories:\nInstall Luet: curl https://luet.io/install.sh | sudo sh Enable the Kairos repository locally: luet repo add kairos --url quay.io/kairos/packages --type docker Install operator-sdk: luet install -y utils/operator-sdk Create the controller Create a directory and let’s init our new project it with the operator-sdk:\n$ mkdir kairos-controller-foo $ cd kairos-controller-foo $ operator-sdk init --domain kairos.io --repo github.com/kairos-io/kairos-controller-foo Create a resource To create a resource boilerplate:\n$ operator-sdk create api --group \u003cgroupname\u003e --version v1alpha1 --kind \u003cresource\u003e --resource --controller Convert to a Helm chart operator-sdk does not have direct support to render Helm charts (see issue), we use kubesplit to render Helm templates by piping kustomize manifests to it. kubesplit will split every resource and add a minimal helm templating logic, that will guide you into creating the Helm chart.\nIf you have already enabled the kairos repository locally, you can install kubesplit with:\n$ luet install -y utils/kubesplit Test with Kind Operator-sdk will generate a Makefile for the project. You can add the following and edit as needed to add kind targets:\nCLUSTER_NAME?=\"kairos-controller-e2e\" kind-setup: kind create cluster --name ${CLUSTER_NAME} || true $(MAKE) kind-setup-image kind-setup-image: docker-build kind load docker-image --name $(CLUSTER_NAME) ${IMG} .PHONY: test_deps test_deps: go install -mod=mod github.com/onsi/ginkgo/v2/ginkgo go install github.com/onsi/gomega/... .PHONY: unit-tests unit-tests: test_deps $(GINKGO) -r -v --covermode=atomic --coverprofile=coverage.out -p -r ./pkg/... e2e-tests: GINKGO=$(GINKGO) KUBE_VERSION=${KUBE_VERSION} $(ROOT_DIR)/script/test.sh kind-e2e-tests: ginkgo kind-setup install undeploy deploy e2e-tests ","categories":"","description":"Learn how to contribute to Kairos by exploring its development practices, debugging tools, and supported hardware.","excerpt":"Learn how to contribute to Kairos by exploring its development …","ref":"/docs/development/development/","tags":"","title":"Development notes"},{"body":"Kairos adopts an immutable layout and derivatives created with its toolkit, inherit the same immutability attributes.\nAn immutable OS is a carefully engineered system which boots in a restricted, permissionless mode, where certain paths of the system are not writable. For instance, after installation it’s not possible to add additional packages to the system, and any configuration change is discarded after reboot.\nA running Linux-based OS system will have the following paths:\n/usr/local - persistent ( partition label COS_PERSISTENT) /oem - persistent ( partition label COS_OEM) /var - ephemeral /etc - ephemeral /srv - ephemeral / immutable /usr/local will contain all the persistent data which will be carried over in-between upgrades, unlike the changes made to /etc which will be discarded.\nDefault persistent paths Other than the /usr/local path, Kairos will also bind mount the following paths to the persistent partition under /usr/local/.state to make them persistent and read/write:\n/etc/cni /etc/init.d /etc/iscsi /etc/k0s /etc/kubernetes /etc/modprobe.d /etc/pwx /etc/rancher /etc/runlevels /etc/ssh /etc/ssl/certs /etc/sysconfig /etc/systemd /etc/zfs /home /opt /root /usr/libexec /var/cores /var/lib/ca-certificates /var/lib/cni /var/lib/containerd /var/lib/calico /var/lib/dbus /var/lib/etcd /var/lib/extensions /var/lib/k0s /var/lib/kubelet /var/lib/longhorn /var/lib/osd /var/lib/rancher /var/lib/rook /var/lib/tailscale /var/lib/wicked /var/lib/kairos /var/log Some specific distros have additional paths, for example:\nFor Ubuntu/Debian, /snap, /var/snap and /var/lib/snapd For Red Hat/Fedora/Rockylinux/Almalinux /usr/share/pki/trust and /usr/share/pki/trust/anchors For the most up-to-date list of paths, please refer to the default rootfs configuration and the extra bind mounts configuration files.\nYou can add additional paths to the persistent partition by using the bind_mounts configuration option. This is useful for storing additional data that you want to persist across reboots or upgrades. Benefits of using an Immutable System There are many reasons why you would like to use an immutable system, in this article we’ll present two of them.\nFrom a security standpoint, it’s far more secure than traditional systems. This is because most attack vectors rely on writing on the system, or installing persistent tools after a vector has been exploited. From a maintenance perspective, configuration management tools like Chef, Puppet, or the likes aren’t needed because immutable systems only have one configuration entry point. Every other configuration is cleaned up automatically after a reboot. The benefit of rolling out the same system over a set of machines are the following:\nNo snowflakes - All the machines are based on the same image, configuration settings and behavior. This allows to have a predictable infrastructure, predictable upgrades, and homogeneous configurations across your cluster. Configuration is driven via cloud-init - There is only one source of truth for the configuration, and that happens at bootstrap time. Anything else is handled afterwards—natively via Kubernetes, so no configuration management software is required. Reduced attack surface - Immutable systems cannot be modified or tampered at runtime. This enhances the security of a running OS, as changes on the system are not allowed. Tools like Chef, Puppet, and Ansible share the same underlying issues when it comes to configuration management. That is, nodes can have different version matrices of software and OS, which makes your set of nodes inhomogeneous and difficult to maintain and orchestrate from day 1 to day 2.\nKairos tackles the issue from different angle, as can turn any distribution to an “immutable” system, distributed as a standard container image, which gets provisioned to the devices as declared. This allows to treat OSes with the same repeatable portability as containers for apps, removing snowflakes in your cluster. Container registries can be used either internally or externally to the cluster to propagate upgrades with customized versions of the OS (kernel, packages, and so on).\nDesign Kairos after installation will create the following partitions:\nA state partition that stores the container images, which are going to be booted (active and passive, stored in .img format which are loopback mounted) A recovery partition that stores the container images, used for recovery (in .squashfs format) An OEM partition (optional) that stores user configuration and cloud-config files A persistent partition to keep the data across reboots The persistent partition is mounted during boot on /usr/local, and additional paths are mount-bind to it. Those configuration aspects are defined in a cloud-config file. It is possible to override such configuration, via a custom cloud-config, during installation.\nThe Recovery system allows to perform emergency tasks, in case of failure from the active and passive images. Furthermore a fallback mechanism will take place, so that in case of failures the booting sequence will be as follows: “A -\u003e B -\u003e Recovery”.\nThe upgrade happens in a transition image and takes place only after all the necessary steps are completed. An upgrade of the ‘A/B’ partitions can be done with Kubernetes or manually. The upgrade will create a new pristine image, that will be selected as active for the next reboot, the old one will be flagged as passive. If we are performing the same from the passive system, only the active is subject to changes.\nKernel and Initrd The Kernel and Initrd are loaded from the system images and are expected to be present in the container, that is pulled down and used for upgrades. Differently from standard approaches, Kairos focuses on having static Initrds, which are generated while building images used for upgrades - in opposite of generating Initramfs locally on the node. A typical setup has kernels and initrd in a special boot partition dedicated for boot files - in Kairos instead the Kernel and Initrd are being loaded from the images, which are chainloaded from the bootloader (GRUB). This is a design choice to keep the entire OS stack confined as a single layer which gets pulled and swapped atomically during upgrades.\n","categories":"","description":"Learn about Kairos' immutable architecture design, where the system boots in a restricted, permissionless mode with read-only paths. Discover how this approach enhances security, simplifies maintenance, and enables predictable upgrades across your infrastructure.","excerpt":"Learn about Kairos' immutable architecture design, where the system …","ref":"/docs/architecture/immutable/","tags":"","title":"Immutable"},{"body":"Upgrades can be run manually from the terminal.\nKairos images are released on quay.io.\nNote Looking to upgrade from a private registry OCI image? Check the Private registry auth page. Listing available versions Using the agent, you can list all the available versions to upgrade to.\n$ sudo kairos-agent upgrade list-releases v0.57.0 v0.57.0-rc2 v0.57.0-rc1 v0.57.0-alpha2 v0.57.0-alpha1 Upgrading the active system To specify an image, use the --source flag:\nsudo kairos-agent upgrade --source \u003ctype\u003e:\u003caddress\u003e Where type can be dir or oci and address is the path to the directory in the dir case or the \u003crepo/image:tag\u003e combination in the oci case.\nFor example, if you wanted to upgrade to the latest available stable release you could run the following command:\nsudo kairos-agent upgrade --source oci:quay.io/kairos/@flavor:@flavorRelease-standard-amd64-generic-master-k3sv1.33.4-k3s1 Once you have tested the new system and are happy with it, you can upgrade the recovery system.\nUpgrading the recovery system The recovery system is there for a reason, to help you recover the active system in case of failure. This is why we don’t allow upgrading the active system and recovery one at the same time and it needs to be done in a separate step. It’s advised to also upgrade the recovery system often, to keep it close to the active one. This will make sure you have a familiar system to work with, when you boot to the recovery system instead of an old image you haven’t used for quite a long time.\nWarning Only upgrade the recovery system, when you are sure that the active system is running correctly. To make this process less error prone, the upgrade command provides an extra flag that will upgrade the recovery only. It uses the same system and flags as the normal upgrade.\nsudo kairos-agent upgrade --recovery --source oci:quay.io/kairos/@flavor:@flavorRelease-standard-amd64-generic-master-k3sv1.33.4-k3s1 What about the passive system? The passive system is the one that is not running. It is not possible to upgrade it directly. The passive system will be upgraded when the active system is rebooted.\nUpgrading single entries (trusted boot installations) On systems installed in “trusted boot” mode, it’s not possible to edit the cmdline without generating a new bootable image because the cmdline is part of the signed artifact. For this reason, custom cmdlines are generated as separate artifacts at build time. Being different artifacts though, means that they will need to be upgraded too.\nThis can be achieved by passing the name of the efi file (without the extension) to the upgrade command like this:\nkairos-agent upgrade --source oci:quay.io/kairos/@flavor:@flavorRelease-standard-amd64-generic-master-k3sv1.33.4-k3s1 --boot-entry \u003cefi_file_name_here\u003e You can find the efi file name by listing all the efi files in the installed system:\nls /efi/EFI/kairos/*.efi ","categories":"","description":"Learn how to manually upgrade Kairos Active and Recovery images","excerpt":"Learn how to manually upgrade Kairos Active and Recovery images","ref":"/docs/upgrade/manual/","tags":"","title":"Manual Upgrades"},{"body":"Kairos configuration mechanism is based on the cloud-config file given during installation, however, it’s possible to extend the configuration by providing additional cloud-configs in either /oem or /usr/local/cloud-config.\nBy default, kairos reads in lexicographic order YAML cloud-config files in the directories above, indeed, after installation you should be able to see the configuration generated by the interactive-installer as /oem/99_custom.yaml in the system.\nThis mechanism can be used to set and enable persistent configuration on boot after node deployment.\nWe are going to see how to do that manually or with Kubernetes by using the Kairos operator.\nManually SSH into the node and copy the config file you want to add into /oem. For instance, to add zram on boot we can copy the following file in /oem/100_zram.yaml or /usr/local/cloud-config/100_zram.yaml and reboot:\nstages: boot: - name: \"zram setup\" commands: - modprobe zram - echo lzo \u003e /sys/block/zram0/comp_algorithm - echo 1G \u003e /sys/block/zram0/disksize - mkswap --label zram0 /dev/zram0 - swapon --priority 100 /dev/zram0 name: \"zfs setup\" With Kubernetes To push configurations to a node, it is recommended to use the Kairos operator which provides a more integrated approach for managing Kairos nodes. In the example below, we use a NodeOp to push a swapfile configuration and restart the node afterward.\nNote The Kairos operator provides several deployment strategies that can be configured through the NodeOp resource. You can control concurrency, node selection, and failure handling. The example below shows a simple approach where the operation is applied to every host of the cluster, one-by-one in sequence. The following pushes a new cloud config over the /oem directory and reboots the node:\napiVersion: operator.kairos.io/v1alpha1 kind: NodeOp metadata: name: add-swapfile namespace: default spec: # NodeSelector to target specific nodes nodeSelector: matchLabels: kairos.io/managed: \"true\" # The container image to use image: quay.io/kairos/@flavor # Custom command to execute command: - sh - -c - | set -e # Create swapfile configuration cat \u003e /host/oem/10_swapfile.yaml \u003c\u003c 'EOF' stages: boot: - name: \"Setup swapfile\" if: \"[ ! -e /usr/local/swapfile ]\" commands: - dd if=/dev/zero of=/usr/local/swapfile bs=1M count=3K - mkswap /usr/local/swapfile - name: \"Enable swapfile\" if: \"[ -e /usr/local/swapfile ]\" commands: - swapon /usr/local/swapfile EOF sync # Note: The reboot is handled automatically by the operator when rebootOnSuccess: true # Path where the node's root filesystem will be mounted hostMountPath: /host # Whether to cordon the node before running the operation cordon: true # Drain options for pod eviction drainOptions: enabled: true force: false gracePeriodSeconds: 30 ignoreDaemonSets: true deleteEmptyDirData: false timeoutSeconds: 300 # Whether to reboot the node after successful operation rebootOnSuccess: true # Maximum number of nodes that can run the operation simultaneously concurrency: 1 # Whether to stop creating new jobs when a job fails stopOnFailure: true ","categories":"","description":"","excerpt":"Kairos configuration mechanism is based on the cloud-config file given …","ref":"/docs/advanced/after-install/","tags":"","title":"Pushing configuration to a node after installation"},{"body":" Note This is the reference documentation for AuroraBoot. For a complete guide on creating custom cloud images, including how to use AuroraBoot in the context of a full workflow, see Creating Custom Cloud Images. AuroraBoot is a tool designed to make the process of bootstrapping Kairos machines quick, simple and efficient. It is specifically designed for the Kairos operating system and provides a comprehensive solution for downloading required artifacts and provisioning a machine, both from network or manually via flashing to USB stick.\nWith AuroraBoot, you can prepare the environment for network-based bootstrapping, download the necessary release assets, and also customize the installation media for USB-based mass-installations. Whether you’re looking to install Kairos on a single machine or multiple machines, AuroraBoot makes it easy and efficient.\nAuroraBoot can be useful to:\nprepare multiple-nodes in a lab before shipment offer a simple, intuitive and streamlined way to deploy Kairos automatically and manually deploy Kairos nodes in a network segment where we can already send workload to (running AuroraBoot in an already-existing downstream cluster) build artifacts of any type (isos, Trusted Boot, container upgrades, raw images, cloud images) Scope AuroraBoot has the following scope:\nDownload release assets in order to provision one or more machines Prepare automatically the environment to boot from network Provision machines from network with a version of Kairos and cloud config Customize The installation media for installations from USB Prerequisites docker or a container engine of your choice For netbooting:\nPort 8090, 8080 and 67 free on the host running AuroraBoot The machine running AuroraBoot have to be on the same network segment of the nodes to be bootstrapped The nodes need to be configured to boot over network, or be capable of booting via USB for offline mode ProxyDHCP supported by the DHCP network attempting to netboot (see also pixiecore architecture). There should be an already running DHCP server on your network. AuroraBoot doesn’t take over the DHCP server, neither require you to do any specific configuration, however a DHCP server which is compliant to ProxyDHCP requests should be present in the same network running AuroraBoot and the machines to boot. MacOS Unfortunately for macOS systems we cannot run the netboot through docker as it’s run inside a VM, as it can’t see the host network. Building ISOs still works as long as you mount the container /tmp disk to a local dir so its exported there like so:\ndocker run --rm -ti -v \"$PWD\"/config.yaml:/config.yaml -v ${PWD}:/tmp quay.io/kairos/auroraboot \\ --set \"artifact_version=master\" \\ --set \"release_version=master\" \\ --set \"flavor=@flavor\" \\ --set \"flavor_release=@flavorRelease\" \\ --set \"repository=kairos-io/kairos\" \\ --set \"disable_http_server=true\" \\ --set \"disable_netboot=true\" \\ --cloud-config /config.yaml This will build the ISO and put the generated artifacts in the current dir under the ${PWD}/iso dir.\nFor netboot, we recommend that you run the AuroraBoot binary directly by grabbing it from the releases page. This requires just one dependency that you can install via brew with brew install xorriso\nWindows Netboot in windows is not supported, only iso creation via the docker image.\nOverview To run AuroraBoot, simply use docker or the container engine of your choice (such as podman, …). AuroraBoot images are published in quay and the source code is available in GitHub.\nThe basic usage of AuroraBoot involves passing it several parameters that define the installation environment, such as the version of Kairos you want to install, the cloud config you want to use, and other customizations you may need. You can pass these parameters either as command-line arguments, or as a full YAML configuration file.\nAuroraBoot will download the artifacts required for bootstrapping the nodes, and prepare the environment required for a zero-touch deployment.\nFor example, to netboot a machine with the latest version of Kairos and @flavor using a cloud config, you would run the following command:\ndocker run --rm -ti --net host quay.io/kairos/auroraboot \\ --set \"artifact_version=master\" \\ --set \"release_version=master\" \\ --set \"flavor=@flavor\" \\ --set repository=\"kairos-io/kairos\" \\ --cloud-config https://... This command will download the necessary artifacts and start the provisioning process. The machine will attempt to boot from network, and will be configured with the specified version of Kairos.\nNetwork-based bootstrapping By default AuroraBoot will automatically attempt to bootstrap other machines, which are configured to boot from network, within the same network. No further configuration or settings necessary.\nThere are only 3 steps involved in the process:\nSelect the release of Kairos that you want to deploy and optionally a cloud config (see also our examples) Run AuroraBoot in your workstation with the appropriate CLI args Boot up other nodes, already configured to boot from network 1. Selecting a release AuroraBoot can bootstrap container images or released assets from our GitHub release process.\nTo use GitHub releases set a release version with --set release_version (the GitHub release), an artifact version with --set artifact_version (the artifact version) a flavor with --set flavor and a repository with --set repository. Kairos has releases with (standard) and without (core) k3s both can be downloaded in the release page at kairos.\nTo use a container image, you can use the Kairos released images or customized by specifying --set container_image instead with the container image of choice.\n2. Run AuroraBoot Now we can run AuroraBoot with the version we selected, either from GitHub releases or directly from a container image.\nIn the example below we selected master-k3sv1.33.4+k3s1, @flavor flavor, so we would run either one of the following:\nContainer image Container Image, with dockerd Github releases By indicating a container_image, AuroraBoot will pull the image locally and start to serve it for network booting.\nYou can use the Kairos released images or your own.\ndocker run --rm -ti --net host quay.io/kairos/auroraboot \\ --set \"container_image=quay.io/kairos/@flavor:@flavorRelease-standard-amd64-generic-master-k3sv1.33.4-k3s1\" By indicating a container_image prefixed with docker://, AuroraBoot will pull the image from the local daemon and start to serve it for network booting.\nThis implies that the host has a docker daemon, and we have to give access to its socket with -v /var/run/docker.sock:/var/run/docker.sock.\ndocker pull quay.io/kairos/@flavor:@flavorRelease-standard-amd64-generic-master-k3sv1.33.4-k3s1 # This will use the container image from the host's docker daemon docker run --rm -ti -v /var/run/docker.sock:/var/run/docker.sock --net host quay.io/kairos/auroraboot \\ --set \"container_image=docker://quay.io/kairos/@flavor:@flavorRelease-standard-amd64-generic-master-k3sv1.33.4-k3s1\" By indicating a artifact_version, a release_version, a flavor and a repository, AuroraBoot will use GitHub released assets.\ndocker run --rm -ti --net host quay.io/kairos/auroraboot \\ --set \"artifact_version=master-k3sv1.33.4-k3s1\" \\ --set \"release_version=master\" \\ --set \"flavor=@flavor\" \\ --set \"flavor_release=@flavorRelease\" \\ --set \"repository=kairos-io/provider-kairos\" To specify a cloud config, you can set it with --cloud-config. See the sections below for further examples.\n3. Start nodes Generic hardware based netbooting is out of scope for this document.\nNodes need to be configured to boot over network, and after AuroraBoot is started should be ready to accept a connection, a typical output of a successfull run is:\n2023/02/08 14:27:30 DHCP: Offering to boot 08:00:27:54:1a:d1 2023/02/08 14:27:30 TFTP: Sent \"08:00:27:54:1a:d1/4\" to 192.168.68.113:6489 2023/02/08 14:27:36 DHCP: Offering to boot 08:00:27:54:1a:d1 2023/02/08 14:27:36 HTTP: Sending ipxe boot script to 192.168.68.113:45435 2023/02/08 14:27:36 HTTP: Sent file \"kernel\" to 192.168.68.113:45435 2023/02/08 14:27:36 HTTP: Sent file \"initrd-0\" to 192.168.68.113:45435 2023/02/08 14:27:49 HTTP: Sent file \"other-0\" to 192.168.68.113:43044 If trying on a VM, for instance on VirtualBox or QEMU, a typical setup might be:\nSet Netboot as first boot in the boot process order Use bridge networking with the host (if running AuroraBoot and the VM in the same host) USB-based bootstrapping AuroraBoot by default prepares an ISO with the custom cloud init prepared for being flashed to an USB stick either with dd or with BalenaEtcher.\nTo disable netboot and provide only offline artifacts, run auroraboot with --set disable_netboot=true.\n1. Node configuration Create a cloud config file, see our documentation for ready-to use examples, but a minimal configuration that automatically installs, and allows us to login afterward can be the following:\n#cloud-config install: auto: true device: \"auto\" reboot: true # Define the user accounts on the node. users: - name: kairos # The username for the user. passwd: kairos # The password for the user. groups: - admin ssh_authorized_keys: # A list of SSH keys to add to the user's authorized keys. # - github:mudler # A key from the user's GitHub account. # - \"ssh-rsa AAA...\" # A raw SSH key. Save the file locally or remotely, you can pass it by in the arguments with --cloud-config to AuroraBoot. Note that can also be a remote http(s) path.\n2. Create an offline ISO Run AuroraBoot with a cloud-config to create an ISO with the embedded configuration:\nContainer image Github releases Check we have the cloud config file:\nls # config.yaml Build the ISO:\ndocker run -v \"$PWD\"/config.yaml:/config.yaml \\ -v \"$PWD\"/build:/tmp/auroraboot \\ --rm -ti quay.io/kairos/auroraboot \\ --set container_image=quay.io/kairos/@flavor:@flavorRelease-core-amd64-generic-master \\ --set \"disable_http_server=true\" \\ --set \"disable_netboot=true\" \\ --cloud-config /config.yaml \\ --set \"state_dir=/tmp/auroraboot\" Results should be available under build/ in the current directory:\nsudo ls -liah build/iso # # total 778M # 34648528 drwx------ 2 root root 4.0K Feb 8 16:39 . # 34648526 drwxr-xr-x 5 root root 4.0K Feb 8 16:38 .. # 34648529 -rw-r--r-- 1 root root 253 Feb 8 16:38 config.yaml # 34649370 -rw-r--r-- 1 root root 389M Feb 8 16:38 kairos.iso # 34649372 -rw-r--r-- 1 root root 389M Feb 8 16:39 kairos.iso.custom.iso # 34649371 -rw-r--r-- 1 root root 76 Feb 8 16:39 kairos.iso.sha256 Check we have the cloud config file:\nls # config.yaml Build the ISO:\ndocker run -v \"$PWD\"/build:/tmp/auroraboot -v /var/run/docker.sock:/var/run/docker.sock --rm -ti quay.io/kairos/auroraboot \\ --set \"artifact_version=master-k3sv1.33.4-k3s1\" \\ --set \"release_version=master\" \\ --set \"flavor=@flavor\" \\ --set \"flavor_release=@flavorRelease\" \\ --set \"repository=kairos-io/provider-kairos\" \\ --set \"disable_http_server=true\" \\ --set \"disable_netboot=true\" \\ --cloud-config /config.yaml \\ --set \"state_dir=/tmp/auroraboot\" Results should be available under build/ in the current directory:\nsudo ls -liah build/iso # # total 778M # 34648528 drwx------ 2 root root 4.0K Feb 8 16:39 . # 34648526 drwxr-xr-x 5 root root 4.0K Feb 8 16:38 .. # 34648529 -rw-r--r-- 1 root root 253 Feb 8 16:38 config.yaml # 34649370 -rw-r--r-- 1 root root 389M Feb 8 16:38 kairos.iso # 34649372 -rw-r--r-- 1 root root 389M Feb 8 16:39 kairos.iso.custom.iso # 34649371 -rw-r--r-- 1 root root 76 Feb 8 16:39 kairos.iso.sha256 The result process will write an iso kairos.iso.custom.iso under build/iso. That is the iso with our embedded cloud-config.\n2. Run the image The iso now is ready to be written to USB stick with either dd or with BalenaEtcher, or attached to a VM.\nMachine: Bare-Metal QEMU When deploying on a bare metal server, directly flash the image into a USB stick. There are multiple ways to do this:\nFrom the command line using the dd command\ndd if=build/kairos.iso.custom.iso of=/path/to/dev bs=4MB or with BalenaEtcher.\nWarning Make sure you have KVM enabled, this will improve the performance of your VM significantly! This would be the way to start it via the command line, but you can also use the GUI virt-install --name my-first-kairos-vm \\ --vcpus 1 \\ --memory 1024 \\ --cdrom build/kairos.iso.custom.iso \\ --disk size=30 \\ --os-variant opensuse-factory \\ --virt-type kvm Immediately after open a viewer so you can interact with the boot menu: virt-viewer my-first-kairos-vm Configuration The AuroraBoot configuration file reference is the following:\n# Corresponding artifact versions from the kairos release page (e.g. kubernetes version included) artifact_version: \"v...\" # Version of the release in github release_version: \"master\" # Flavor flavor: \"@flavor\" # Github repository repository: \"kairos-io/kairos\" # Container image (takes over) container_image: \"...\" # Disable netboot disable_netboot: true # Disable http server for serving offline generated ISOs disable_http_server: true # Specify a directory that will be used by auroraboot to download artifacts # Reuse the same to cache artifacts. state_dir: \"/tmp/auroraboot\" # Default http binding port for offline ISO generation listen_addr: \":8080\" # Cloud config to use when booting the machine. cloud_config: | Option Description artifact_version Corresponding artifact versions from the Kairos release page (e.g. Kubernetes version included). release_version Version of the release in GitHub. flavor The Kairos flavor to use. See the Kairos support matrix for a list. repository Github repository to use. This can either be kairos-io/kairos or kairos-io/provider-kairos for images with k3s prior to v2.4.0. container_image Container image. If prefixed with docker:// it will try to pull from the local docker daemon. If a container_image is specified, artifact_version, flavor and release_version are ignored. disable_netboot Disable netboot. disable_http_server Disable http server for serving offline generated ISOs. netboot_http_port Specify a netboot HTTP port (defaults to 8090). state_dir Specify a directory that will be used by auroraboot to download artifacts and reuse the same to cache artifacts. listen_addr Default http binding port for offline ISO generation. cloud_config Cloud config path to use for the machines. A URL can be specified, use - to pass-by the cloud-config from STDIN iso.data Defines a path to be embedded into the resulting iso. When booting, the files will be accessible at /run/initramfs/live netboot.cmdline Override the automatically generated cmdline with a custom one to use during netboot. config_url and rootfs are automatically constructed. A reasonable value can be netboot.cmdline=rd.neednet=1 ip=dhcp rd.cos.disable netboot install-mode console=tty0 disk.state_size Set the minimum size (in MB) for the state partition in raw disk images. By default, the state partition is sized to 3 times the size of the current image plus some additional space for system files. Use this option to override the default size if you need to accommodate larger future images. disk.efi Generate an EFI-compatible raw disk image (suitable for AWS and QEMU). When set to true, the image will be created with EFI boot support. disk.size Set the final size (in MB) of the generated disk image. By default, the image size is calculated based on the actual data size. Use this option to create a larger disk image (e.g. disk.size=16000 will create a 16GB image). disk.bios Generate a BIOS-compatible raw disk image. When set to true, the image will be created with legacy BIOS boot support instead of EFI. disk.vhd Generate an Azure-compatible VHD image. When set to true, the image will be created in VHD format suitable for Azure cloud platform. disk.gce Generate a Google Cloud Engine (GCE) compatible image. When set to true, the image will be created in a format suitable for Google Cloud Platform. To use the configuration file with AuroraBoot, run AuroraBoot specifying the file or URL of the config as first argument: docker run --rm -ti -v \"$PWD\"/config.yaml:/config.yaml --net host quay.io/kairos/auroraboot /config.yaml The CLI options can be used in place of specifying a file, and to set fields of it. Any field of the YAML file, excluding cloud_config can be configured with the --set for instance, to disable netboot we can run AuroraBoot with:\ndocker run --rm -ti --net host quay.io/kairos/auroraboot .... --set \"disable_netboot=true\" To specify a cloud config file instead, use --cloud-config (can be also url):\ndocker run --rm -ti -v \"$PWD\"/config.yaml:/config.yaml --net host quay.io/kairos/auroraboot .... --cloud-config /config.yaml Both the config file and the cloud-config file can be a URL.\nCloud config A custom cloud configuration file can be passed either with the --cloud-config flag, or in the AuroraBoot configuration file under the cloud_config key.\nIt is possible to apply templating to a cloud config. Indeed any value passed to --set is accessible as a template in the cloud config file with the [[ and ]] delimiter, for instance consider the following cloud config file, which allows to set a password for the kairos user and a GitHub handle allowed to login to the machine:\n#cloud-config install: auto: true device: \"auto\" reboot: true # Define the user accounts on the node. users: - name: \"kairos\" # The username for the user. passwd: \"[[.kairos.password]]\" # The password for the user. groups: - admin ssh_authorized_keys: # A list of SSH keys to add to the user's authorized keys. - github:[[.github.user]] We would then set the user to mudler and the password to foobar when running AuroraBoot like the following:\ndocker run --rm -ti -v \"$PWD\"/config.yaml:/config.yaml --net host \\ quay.io/kairos/auroraboot \\ --cloud-config /config.yaml \\ --set \"github.user=mudler\" \\ --set \"kairos.password=foobar\" Config files can be also hosted remotely, and given as URLs to AuroraBoot.\nWe can indeed use the template in the example folder with the command above:\ndocker run --rm -ti --net host \\ quay.io/kairos/auroraboot \\ --cloud-config https://raw.githubusercontent.com/kairos-io/kairos/master/examples/auroraboot/master-template.yaml \\ --set \"github.user=mudler\" \\ --set \"kairos.password=foobar\" To pass-by a cloud-config via pipes, set --cloud-config -, for example:\ncat \u003c\u003cEOF | docker run --rm -i --net host quay.io/kairos/auroraboot \\ --cloud-config - \\ --set \"container_image=quay.io/kairos/@flavor:@flavorRelease-standard-amd64-generic-master-k3sv1.33.4-k3s1\" #cloud-config install: device: \"auto\" auto: true reboot: true hostname: metal-bundle-test-{{ trunc 4 .MachineID }} users: - name: kairos # Change to your pass here passwd: kairos groups: - admin ssh_authorized_keys: # Replace with your github user and un-comment the line below: - github:mudler k3s: enabled: true # Specify the bundle to use bundles: - targets: - run://quay.io/kairos/community-bundles:cert-manager_latest - run://quay.io/kairos/community-bundles:kairos_latest kairos: entangle: enable: true EOF Examples Note The example below are implying a config.yaml cloud config file to be present in the current directory. Offline ISO build from local container image First make sure we have the image locally with:\ndocker pull \u003cIMAGE\u003e Build the custom ISO with the cloud config:\ndocker run -v \"$PWD\"/config.yaml:/config.yaml \\ -v \"$PWD\"/build:/tmp/auroraboot \\ -v /var/run/docker.sock:/var/run/docker.sock \\ --rm -ti quay.io/kairos/auroraboot \\ --set container_image=docker://\u003cIMAGE\u003e \\ --set \"disable_http_server=true\" \\ --set \"disable_netboot=true\" \\ --cloud-config /config.yaml \\ --set \"state_dir=/tmp/auroraboot\" Offline ISO build from container images Build the custom ISO with the cloud config:\ndocker run -v \"$PWD\"/config.yaml:/config.yaml \\ -v \"$PWD\"/build:/tmp/auroraboot \\ --rm -ti quay.io/kairos/auroraboot \\ --set container_image=quay.io/kairos/@flavor:@flavorRelease-core-amd64-generic-master \\ --set \"disable_http_server=true\" \\ --set \"disable_netboot=true\" \\ --cloud-config /config.yaml \\ --set \"state_dir=/tmp/auroraboot\" Override GRUB config file It is possible to override the default GRUB config file of the ISO by creating a directory that contains the files that we want to add or replace in it.\nFor example, to override the GRUB config file:\nmkdir -p data/boot/grub2 # You can replace this step with your own grub config. This GRUB configuration is the boot menu of the ISO wget https://raw.githubusercontent.com/kairos-io/packages/main/packages/livecd/grub2/config/grub_live_bios.cfg -O data/boot/grub2/grub.cfg docker run -v \"$PWD\"/config.yaml:/config.yaml \\ -v \"$PWD\"/data:/tmp/data \\ -v \"$PWD\"/build:/tmp/auroraboot \\ --rm -ti quay.io/kairos/auroraboot \\ --set container_image=quay.io/kairos/@flavor:@flavorRelease-core-amd64-generic-master \\ --set \"disable_http_server=true\" \\ --set \"disable_netboot=true\" \\ --cloud-config /config.yaml \\ --set \"state_dir=/tmp/auroraboot\" \\ --set \"iso.data=/tmp/data\" Prepare ISO for Airgap installations See the Airgap example in the examples section.\nNetboot with core images from Github releases docker run -v \"$PWD\"/config.yaml:/config.yaml --rm -ti --net host quay.io/kairos/auroraboot \\ --set \"artifact_version=master\" \\ --set \"release_version=master\" \\ --set \"flavor=@flavor\" \\ --set repository=\"kairos-io/kairos\" \\ --cloud-config /config.yaml Netboot with k3s images from Github releases docker run -v \"$PWD\"/config.yaml:/config.yaml --rm -ti --net host quay.io/kairos/auroraboot \\ --set \"artifact_version=master-k3sv1.33.4-k3s1\" \\ --set \"release_version=master\" \\ --set \"flavor=@flavor\" \\ --set \"flavor_release=@flavorRelease\" \\ --set \"repository=kairos-io/provider-kairos\" \\ --cloud-config /config.yaml Netboot from container images docker run -v \"$PWD\"/config.yaml:/config.yaml --rm -ti --net host quay.io/kairos/auroraboot \\ --set container_image=quay.io/kairos/@flavor:@flavorRelease-core-amd64-generic-master --cloud-config /config.yaml Generate RAW disk images Note This method differs from the ones documented in the Build raw images with QEMU section: this method is suitable if you need to create appliances that have to run a full-installation. AuroraBoot will create instead images pre-installed which will skip the usual Kairos installation process in runtime Consider the following example:\ndocker run --privileged -v /var/run/docker.sock:/var/run/docker.sock \\ -v $PWD:/aurora --rm -ti quay.io/kairos/auroraboot \\ --debug \\ --set \"disable_http_server=true\" \\ --set \"disable_netboot=true\" \\ --set \"disk.efi=true\" \\ --set \"container_image=quay.io/kairos/@flavor:@flavorRelease-standard-amd64-generic-master-k3sv1.33.4-k3s1\" \\ --set \"state_dir=/aurora\" The raw disk image will be available in the current directory with the .raw image extension. By default, the state partition will be sized to 3 times the size of the current image plus some additional space for system files.\nIf you need to ensure the state partition can accommodate larger future images, you can override the default size with disk.state_size:\ndocker run --privileged -v /var/run/docker.sock:/var/run/docker.sock \\ -v $PWD:/aurora --rm -ti quay.io/kairos/auroraboot \\ --debug \\ --set \"disable_http_server=true\" \\ --set \"disable_netboot=true\" \\ --set \"disk.efi=true\" \\ --set \"disk.state_size=6000\" \\ --set \"container_image=quay.io/kairos/@flavor:@flavorRelease-standard-amd64-generic-master-k3sv1.33.4-k3s1\" \\ --set \"state_dir=/aurora\" To generate a BIOS raw image just change disk.efi=true for disk.bios=true\nTo generate GCE and VHD images set disk.gce=true or disk.vhd=true respectively in the AuroraBoot command. For example:\n# Build a GCE-compatible image docker run --privileged -v /var/run/docker.sock:/var/run/docker.sock \\ -v $PWD:/aurora --rm -ti quay.io/kairos/auroraboot \\ --debug \\ --set \"disable_http_server=true\" \\ --set \"container_image=quay.io/kairos/@flavor:@flavorRelease-standard-amd64-generic-master-k3sv1.33.4-k3s1\" \\ --set \"disable_netboot=true\" \\ --cloud-config /aurora/config.yaml \\ --set \"disk.gce=true\" \\ --set \"state_dir=/aurora\" or for VHD images:\n# Build a VHD image compatible with Azure docker run --privileged -v /var/run/docker.sock:/var/run/docker.sock \\ -v $PWD:/aurora --rm -ti quay.io/kairos/auroraboot \\ --debug \\ --set \"disable_http_server=true\" \\ --set \"container_image=quay.io/kairos/@flavor:@flavorRelease-standard-amd64-generic-master-k3sv1.33.4-k3s1\" \\ --set \"disable_netboot=true\" \\ --cloud-config /aurora/config.yaml \\ --set \"disk.vhd=true\" \\ --set \"state_dir=/aurora\" Note Note that for creating raw images, the --privileged flag is used as the process creates loop devices, which requires elevated privileges. This is not needed for the other methods of using AuroraBoot. Use the config file Write down an aurora config file as aurora.yaml:\ncontainer_image: \"quay.io/kairos/@flavor:@flavorRelease-core-amd64-generic-master\" cloud_config: | #cloud-config install: auto: true device: \"auto\" reboot: true # Define the user accounts on the node. users: - name: kairos # The username for the user. passwd: kairos # The password for the user. groups: - admin ssh_authorized_keys: # A list of SSH keys to add to the user's authorized keys. # - github:mudler # A key from the user's GitHub account. # - \"ssh-rsa AAA...\" # A raw SSH key. And then run:\ndocker run -v \"$PWD\"/aurora.yaml:/aurora.yaml --rm -ti --net host quay.io/kairos/auroraboot /aurora.yaml Booting with UKI and Secure Boot via HTTP Boot Version support This feature is available on AuroraBoot v0.8.1 and later, and requires Kairos version 3.5.x or higher. Currently, only HTTP boot has been tested and validated for this setup. PXE booting with UKI and Secure Boot is not yet supported and probably wont be. AuroraBoot supports netbooting Trusted Boot Images (UKI). This process relies on using HTTP Boot to enroll the necessary keys and boot the UKI ISO directly as a virtual CDROM which is a UEFI 2.5 feature.\nHow it Works Info To use this feature, the machine needs to start in “setup” mode, which means secure boot is disabled and there are no keys enrolled in the firmware. Otherwise the machine won’t boot the UKI ISO, as it will not be able to enroll the keys. The machine boots HTTP Boot and loads the ISO. The ISO is loaded on memory and booted and systemd-boot enrolls the keys from the ISO into the firmware, allowing the UKI to boot and enabling Secure Boot. The machine reboots and HTTP boots the UKI ISO again, this time with Secure Boot enabled. The system boots fully from the UKI ISO into memory. The Kernel maps the ISO from the memory and presents it to the OS like any other media, allowing the OS to use it as an install source like if we booted from a USB stick or a CD-ROM. Preparing the Boot Environment You must provide:\nA Trusted Boot Kairos ISO For example if your ISO is at /opt/images/kairos-uki.iso, start AuroraBoot with:\ndocker run --privileged -v /opt/images/:/aurora --rm -ti quay.io/kairos/auroraboot uki-pxe /aurora/kairos-uki.iso Resources kairos-agent PR #791 AuroraBoot PR #276 efi-key-enroller repo ","categories":"","description":"Reference documentation for AuroraBoot, a tool for generating bootable images","excerpt":"Reference documentation for AuroraBoot, a tool for generating bootable …","ref":"/docs/reference/auroraboot/","tags":"","title":"AuroraBoot"},{"body":"Check the Signatures Optional Step This is an optional but strongly encouraged step for security reasons. Our ISO releases have sha256 files to checksum the validity of the artifacts. At the same time, our sha256 files are signed automatically in the CI during the release workflow to verify that they haven’t been tampered with, adding an extra step to the supply chain.\nIt is recommended that before starting any installation the whole security chain is validated by verifying our sha256 signature and validating that the checksum matches with the download artifacts.\nTo validate the whole chain you need:\nsha256sum which is usually installed by default on most linux distributions. cosign to verify the signatures of the sha256 file. You can install cosign via their installation docs sha256, certificate and signature files that you want to verify kairos-@flavor-@flavorRelease-standard-amd64-generic-v3.5.2-k3sv1.33.4+k3s1.iso.sha256 kairos-@flavor-@flavorRelease-standard-amd64-generic-v3.5.2-k3sv1.33.4+k3s1.iso.sha256.pem kairos-@flavor-@flavorRelease-standard-amd64-generic-v3.5.2-k3sv1.33.4+k3s1.iso.sha256.sig In this example we will use the master version and @flavor flavor and @flavorRelease flavor release.\nFirst we check that we have all needed files:\n$ ls kairos-@flavor-@flavorRelease-core-amd64-generic-master.iso kairos-@flavor-@flavorRelease-core-amd64-generic-master.iso.sha256.pem kairos-@flavor-@flavorRelease-core-amd64-generic-master.iso.sha256 kairos-@flavor-@flavorRelease-core-amd64-generic-master.iso.sha256.sig Cosign version Step We recommend using the latest cosign version, at the time of writing, 2.5.0 Then we verify that the sha256 checksums haven’t been tampered with (substitute $VERSION with the exact Kairos version you are verifying as the certificate identity is the release job that signs it):\n$ cosign verify-blob --cert kairos-@flavor-@flavorRelease-core-amd64-generic-master.iso.sha256.pem --signature kairos-@flavor-@flavorRelease-core-amd64-generic-master.iso.sha256.sig --certificate-identity https://github.com/kairos-io/kairos/.github/workflows/reusable-release.yaml@refs/tags/$VERSION --certificate-oidc-issuer https://token.actions.githubusercontent.com kairos-@flavor-@flavorRelease-core-amd64-generic-master.iso.sha256 Verified OK Once we see that Verified OK we can be sure that the file hasn’t been tampered with, and we can continue verifying the iso checksum.\nFor an example of a failure validation see below:\n$ cosign verify-blob --cert kairos-@flavor-@flavorRelease-core-amd64-generic-master.iso.sha256.pem --signature kairos-@flavor-@flavorRelease-core-amd64-generic-master.iso.sha256.sig --certificate-identity https://github.com/kairos-io/kairos/.github/workflows/reusable-release.yaml@refs/tags/$VERSION --certificate-oidc-issuer https://token.actions.githubusercontent.com kairos-@flavor-@flavorRelease-core-amd64-generic-master.iso.sha256.modified Error: verifying blob [kairos-@flavor-@flavorRelease-core-amd64-generic-master.iso.sha256.modified]: invalid signature when validating ASN.1 encoded signature main.go:62: error during command execution: verifying blob [kairos-@flavor-@flavorRelease-core-amd64-generic-master.iso.sha256.modified]: invalid signature when validating ASN.1 encoded signature Now we can verify that the integrity of the ISO hasnt been compromise:\n$ sha256sum -c kairos-@flavor-@flavorRelease-core-amd64-generic-master.iso.sha256 kairos-@flavor-@flavorRelease-core-amd64-generic-master.iso: OK Once we reached this point, we can be sure that from the ISO hasn’t been tampered with since it was created by our release workflow.\n","categories":"","description":"Verify the integrity of the ISO by checking the signatures.\n","excerpt":"Verify the integrity of the ISO by checking the signatures.\n","ref":"/docs/installation/check-the-signatures/","tags":"","title":"Check the Signatures"},{"body":"Kairos configuration mechanism for partitions is based on the cloud-config file given during installation to override the default values set by the installer.\nWe allow certain flexibility in the sizes and filesystems used for the default install and allow to create extra partitions as well.\nFor example, the following cloud-config will make the oem partition have a size of 512Mb and an ext4 filesystem, recovery with a size of 10000Mb and a ext4 filesystem, while leaving the rest of the partitions to their default sizes and filesystems.\n#cloud-config install: device: \"/dev/sda\" auto: true partitions: oem: size: 512 fs: ext4 recovery: size: 10000 fs: ext4 The partitions that can be configured are: oem, recovery, state and persistent.\nAnd the following config will leave the default partitions as is, but create 2 new extra partitions with the given sizes, filesystems and labels:\n#cloud-config install: device: \"/dev/sda\" auto: true extra-partitions: - name: first_partition size: 512 fs: ext3 - name: second_partition size: 100 fs: ext2 label: PARTITION_TWO Only one partition can expand to the rest of the disk. Either persistent or one of the extra-partitions. In case you want the latter, you need to specify the size of persistent to a fixed value.\nAn example of this would be as follows:\n#cloud-config install: device: \"/dev/sda\" auto: true partitions: persistent: size: 500 extra-partitions: - name: big_partition size: 0 fs: ext3 Note that there are some caveats in the extra partitions setup:\nOnly size, fs, name and label are used for the partition creation, the name is currently used for the partition label. If a partition has no fs set, the partition will be created, but it will not be formatted. No mounting of any type is done during installation to the extra partitions. That part should be done on the stages of the cloud-config manually, with something like the following step: initramfs: - name: \"Mount PARTITION_TWO under /opt/extra\" commands: - mkdir -p /opt/extra - mount -o rw /dev/disk/by-label/PARTITION_TWO /opt/extra Manual partitioning In some cases, it’s desired that the user has full control over the partitioning of the disk. This can be achieved by setting the no-format option to true and making sure the disk is prepared with the desired partitions before running the installer.\nHere is an example config that instructs the installer to use the “second” disk (/dev/vdb) and makes sure the disk is prepared with the desired partitions:\n#cloud-config install: no-format: true auto: false poweroff: false reboot: false users: - name: \"kairos\" passwd: \"kairos\" stages: kairos-install.pre.before: - if: '[ -e \"/dev/vdb\" ]' name: \"Create partitions\" commands: - | parted --script --machine -- \"/dev/vdb\" mklabel gpt # Legacy bios sgdisk --new=1:2048:+1M --change-name=1:'bios' --typecode=1:EF02 /dev/vdb layout: device: path: \"/dev/vdb\" add_partitions: # For efi (comment out the legacy bios partition above) #- fsLabel: COS_GRUB # size: 64 # pLabel: efi # filesystem: \"fat\" - fsLabel: COS_OEM size: 64 pLabel: oem - fsLabel: COS_RECOVERY size: 8500 pLabel: recovery - fsLabel: COS_STATE size: 18000 pLabel: state - fsLabel: COS_PERSISTENT pLabel: persistent size: 0 filesystem: \"ext4\" For more information about the full config available for partitions and extra partitions see the full cloud-config page\n","categories":"","description":"","excerpt":"Kairos configuration mechanism for partitions is based on the …","ref":"/docs/advanced/configuring_partitions/","tags":"","title":"Configuring partitions"},{"body":"This guide provides a complete walkthrough for creating custom cloud images with Kairos. It covers the entire process from start to finish, using the latest tools like kairos-init and AuroraBoot.\nOverview Kairos provides several tools to create custom cloud images:\nkairos-init: A tool for creating base container images AuroraBoot: A tool for generating bootable images (ISOs, cloud images, etc.) Customization tools: Various methods to customize the images Prerequisites Before starting, ensure you have:\nDocker installed and running A Linux machine with KVM support (for testing) Basic understanding of container images and cloud configurations Step 1: Creating a Base Image with kairos-init The first step is to create a base container image using kairos-init. This tool allows you to create a custom base image from popular distributions.\nHere’s a basic example of creating a base image:\ndocker build -t my-custom-image - \u003c\u003cEOF ARG BASE_IMAGE=ubuntu:24.04 FROM quay.io/kairos/kairos-init:v0.5.20 AS kairos-init FROM ${BASE_IMAGE} AS base-kairos ARG VARIANT=core ARG MODEL=generic ARG TRUSTED_BOOT=false ARG KUBERNETES_DISTRO=k3s ARG KUBERNETES_VERSION=latest ARG VERSION COPY --from=kairos-init /kairos-init /kairos-init RUN /kairos-init -l debug -s install -m \"${MODEL}\" -v \"${VARIANT}\" -t \"${TRUSTED_BOOT}\" -k \"${KUBERNETES_DISTRO}\" --k8sversion \"${KUBERNETES_VERSION}\" --version \"${VERSION}\" RUN /kairos-init -l debug -s init -m \"${MODEL}\" -v \"${VARIANT}\" -t \"${TRUSTED_BOOT}\" -k \"${KUBERNETES_DISTRO}\" --k8sversion \"${KUBERNETES_VERSION}\" --version \"${VERSION}\" RUN rm /kairos-init EOF Step 2: Customizing the Image Once you have a base image, you can customize it by:\nAdding packages Modifying configurations Adding custom services Including additional files See the Customizing Images guide for detailed instructions.\nStep 3: Building Bootable Images with AuroraBoot After creating and customizing your base image, you can use AuroraBoot to create bootable images:\ndocker run -v \"$PWD\"/build:/tmp/auroraboot \\ -v /var/run/docker.sock:/var/run/docker.sock \\ --rm -ti quay.io/kairos/auroraboot:v0.13.0 \\ --set container_image=my-custom-image \\ --set \"disable_http_server=true\" \\ --set \"disable_netboot=true\" \\ --cloud-config /path/to/cloud-config.yaml \\ --set \"state_dir=/tmp/auroraboot\" Note For more details about AuroraBoot options and configurations, see the AuroraBoot documentation. State Partition Sizing When creating cloud images, it’s important to consider the size of the state partition. The state partition is created at image build time and cannot be resized later. This partition needs to accommodate all images (passive, active, and transition image for upgrades) that might be used with the system.\nBy default, AuroraBoot sets the state partition size to 3 times the size of the current image plus some additional space for system files. This is usually sufficient for most use cases, but if you need to ensure the state partition can accommodate larger future images, you can override this with the disk.state_size option:\ndocker run -v \"$PWD\"/build:/tmp/auroraboot \\ -v /var/run/docker.sock:/var/run/docker.sock \\ --rm -ti quay.io/kairos/auroraboot \\ --set container_image=my-custom-image \\ --set \"disable_http_server=true\" \\ --set \"disable_netboot=true\" \\ --set \"disk.state_size=6000\" \\ --cloud-config /path/to/cloud-config.yaml \\ --set \"state_dir=/tmp/auroraboot\" The value is specified in megabytes (MB). When setting a custom size, make sure it’s at least 3 times the size of the largest image you plan to use, plus some additional space for system files. This ensures there’s enough space for the passive, active, and transition images, plus some overhead for future updates.\nStep 4: Cloud Configuration Your cloud configuration file (cloud-config.yaml) is crucial for defining how your image will behave. Here’s a basic example:\n#cloud-config hostname: kairos-{{ trunc 4 .MachineID }} users: - name: \"kairos\" passwd: kairos groups: - admin reset: source: \"oci:quay.io/kairos/opensuse:leap-15.6-standard-amd64-generic-master-k3sv1.32.1-rc2-k3s1\" In the example above, we are specifying a custom image that will be used during the first boot to reset the system. When you launch an instance, Kairos will boot into “auto-reset mode” by default. This means that Kairos will “install” itself on the first boot and then reboot. The reset.source field in the cloud-config specifies which image will be installed during this process.\nAs explained in the section above, sizing the state partition properly is important when using this option.\nStep 5: Building for Specific Platforms AWS To install on AWS, follow the AWS Guide.\nGoogle Cloud To install on Google Cloud, follow the Google Cloud Installation Guide.\nMicrosoft Azure For Azure deployments, see the Azure Installation Guide.\nTroubleshooting Common issues and solutions:\nDisk Size: Ensure your VM disk is large enough for all partitions. If there isn’t enough space, the auro-reset will fail and the VM will never boot into active_boot mode. State Partition Size: If you encounter issues when resetting to a new image, it might be because the state partition is too small. Use the disk.state_size option when creating the image to set an appropriate size. Next Steps Customizing Images AuroraBoot Reference Cloud Configuration Reference ","categories":"","description":"A comprehensive guide to creating custom cloud images with Kairos using the latest tools","excerpt":"A comprehensive guide to creating custom cloud images with Kairos …","ref":"/docs/advanced/creating_custom_cloud_images/","tags":"","title":"Creating Custom Cloud Images"},{"body":" Objective This guide will teach you how easy it is to deploy a Kubernetes cluster using Kairos. To make this guide quick and effective we will make some decisions for you. We will do a traditional, interactive installation, of a single node cluster on a virtual machine (VM) on x86_64 architecture. At the end of the guide we will provide you with links to do many other different setups. Ready to launch your Kubernetes cluster with ease? With Kairos, deployment is a breeze! Simply download an ISO, boot up on a VM, and let Kairos handle the rest. Whether on Linux, Windows or macOS, this guide will have you up and running in no time. Kairos can build a Kubernetes cluster for you with just a few simple steps!\nPrerequisites A Virtual Machine Manager (VMM) like VirtualBox, KVM, or VMware. At least 35+ Gb of available disk space. Do you prefer to watch a video? Download an ISO Kairos Flavor Kairos comes in many flavors. These are the underlying Linux distributions on which we build our final OS. Some examples are Alpine Kairos, openSUSE Kairos and Ubuntu Kairos. Select your Kairos Flavor from the nav bar Click the following link to download an iso: kairos-@flavor-@flavorRelease-standard-amd64-generic-v3.5.2-k3sv1.33.4+k3s1.iso Create a Virtual Machine (VM) Warning Make sure you have KVM enabled in your VMM, this will improve the performance of your VM significantly! This step will vary depending on the Virtual Machine Manager that you are using. Here are some general steps to get you started:\nOpen your VMM. Create a new VM. Select the downloaded ISO as the boot media. Configure the VM hardware We recommend at least 2 CPUs and 4GB of RAM for the best experience but Kairos can run on less. Allocate at least 35GB of disk space. Add a TPM device Start the VM. Perform an Interactive Installation The first time you boot the VM, you will be greeted with a GRUB boot menu with multiple options. Select the option that says “Interactive Install” and press Enter.\nWait for the system to boot up. You will be greeted with a Kairos logo and the interactive installation manager will ask you the following questions:\nWhat’s the target install device (e.g. /dev/vda). Kairos will detect the biggest disk available and suggest it. If you are happy with the suggestion, press Enter. Otherwise erase the suggestion and type the device you want to use, or the question mark. The question mark will show you a list of available devices. User to setup (e.g. Kairos). The default user is Kairos but it is also a required user so for now just press Enter and later on we will teach you how to add more users. Password this one doesn’t have a default, so type a password and press Enter. SSH access (e.g. github:mauromorales). This is optional but very useful. Kairos will go and fetch your public key from GitHub or GitLab and add it to the user’s authorized keys. If you don’t have a key on GitHub or GitLab, you can paste your public key here. If you don’t want to add a key, just press Enter. Do you want to setup a full mesh-support? (y/n). This step enables P2P support. For now we will not enable it, so just press Enter. Do you want to enable k3s? (y/n). This step enables the installation of K3s. Write “y” and press Enter. Are your settings ok? (y/n). If you are happy with the settings, write “y” and press Enter. Otherwise write “n” and press Enter to start again. The installation will start and you will see the Kairos’ agent different steps.\nRemember to eject the CD! Some VMMs will not eject the CD automatically. Make sure to eject the CD before rebooting. When the installation is complete you will need to reboot the system. You can do this with the following command:\nreboot First Boot After the reboot you will again see the GRUB boot menu. This time the options don’t include any installation, instead you can start the system in either active, passive (fallback) or rescue mode. We will learn more about that in the next steps. For now, just select the first option that only says “Kairos” and press Enter. If you don’t touch anything, the system will boot automatically after a few seconds.\nAfter the system finishes booting, you will see a login prompt. Go ahead and login with the user kairos and the password you set during the installation.\nSSH into the system Check your VMM Networking configuration Accessing your VM via SSH will depend on your VMM networking configuration. Make sure you have the correct network settings to access the VM. On the VM info you can find the IP associated to it, use it to SSH into the system:\nssh kairos@IP Authorized Keys If you configured the SSH key during the installation, you will be able to login without a password. Now enter the password you set during the installation.\nCheck Your Running Cluster Batteries included Along with k3s, the standard images come with kubectl and k9s pre-installed. Kubectl is the Kubernetes command-line tool, and k9s is a terminal-based UI to interact with your Kubernetes clusters. After logging in, you can check the status of the cluster with the kubectl tool. First switch to the root user with the following command:\nsudo -i If you display the pods within the kube-system namespace, you should see the coredns and local-path-provisioner pods running. E.g.:\nroot@localhost:~# kubectl get pods -n kube-system NAME READY STATUS RESTARTS AGE coredns-576bfc4dc7-nc982 1/1 Running 0 5h9m helm-install-traefik-crd-28sfl 0/1 Completed 0 5h9m helm-install-traefik-kdxmj 0/1 Completed 1 5h9m local-path-provisioner-86f46b7bf7-5fs46 1/1 Running 0 5h9m metrics-server-557ff575fb-zmdlf 1/1 Running 0 5h9m svclb-traefik-00b7a912-xh4zd 2/2 Running 0 5h8m traefik-5fb479b77-mfq7h 1/1 Running 0 5h8m Conclusion Congratulations 🎉 You have successfully deployed a Kubernetes cluster using Kairos 🚀 You can now start deploying your applications and services on your new cluster\nPlease refer to the K3s documentation, to learn more about the Kubernetes distribution that Kairos uses in the standard images.\nFrequently Asked Questions (FAQs) How do I configure the system?\nYou can configure the system by editing the cloud-config file. The cloud-config file is located at /oem/90_custom.yaml. You can edit this file to add users, SSH keys, and other configurations. See the Cloud Config documentation for more information.\nWhat is a Kairos flavor?\nA Kairos flavor is a specific version of Kairos that is built on top of a specific Linux distribution. For example, the Alpine Kairos flavor is built on top of Alpine Linux. You can choose the flavor that best suits your needs.\nCan I use Kairos without Kubernetes?\nYes, absolutely! You can use Kairos as a standalone Linux distribution without Kubernetes. Just download the Kairos Core artifacts if you don’t want to use Kubernetes, or configure the Standard artifacts with the k3s option disabled.\nCan I use a different Kubernetes distribution with Kairos?\nKairos uses providers to install Kubernetes distributions. The Kairos provider is the only one that is built and tested by the Kairos team, but there are other providers by the community and you can build your own!\nWhat’s Next? Ready to configure your newly deployed Kairos node?\nQuick configuration guide Find out about the features in Kairos, the goals that drive our project and how to join our community.\nWhat is Kairos? Want to install on bare-metal? A Raspberry Pi? A cloud provider? Check out our other installation guides.\nOther ways to install Kairos ","categories":"","description":"Learn how to deploy Kairos, the immutable Linux distribution designed for secure and resilient Kubernetes clusters at the edge.\n","excerpt":"Learn how to deploy Kairos, the immutable Linux distribution designed …","ref":"/getting-started/","tags":"","title":"Getting Started"},{"body":"This page describes how to install Kairos on AWS after you have created an AMI. Since release v3.3.0, Kairos pipeline is pushing a public AMI image to AWS which you can use. If you want to build a custom image, you can follow the instructions in the Build Kairos appliances page.\nPrerequisites An AWS account with permissions to create EC2 instances. An AMI image of Kairos. You can use the public AMI image provided by Kairos or build your own image. Find the AMI Login to your AWS account. Go to the EC2 dashboard -\u003e Images -\u003e AMIs. If you are looking for the Kairos AMI, make sure you are searching among “Public images”. Then search for “kairos” in the search bar. If you are looking for an AMI you created, make sure you are searching among “Owned by me”. Then search for your AMI. Select the AMI you want to use (e.g. kairos-ubuntu-24.04-standard-amd64-generic-v3.3.0-k3sv1.32.0-k3s1.raw). From the top menu, click on Launch instance from AMI. Verify the AMI To ensure you’re using a genuine Kairos AMI, check the AMI owner ID. The AMI should be owned by the Kairos team’s AWS account. You can find this information in the AMI details. Look for the Owner Account ID. It should be 171987620676 (The Kairos account ID).\nYou can also verify the AMI using the AWS CLI:\naws ec2 describe-images --region \u003cAMI_REGION\u003e --image-ids \u003cAMI_ID\u003e --query 'Images[0].[OwnerId,Name,Description]' Replace \u003cAMI_ID\u003e with the ID of the AMI you want to verify and the \u003cAMI_REGION\u003e with the region of that AMI. The output will show you the owner ID, name, and description of the AMI.\nLaunch the instance In this page you get to select the instance type, configure the instance, and add storage. Make sure you give the instance enough resources to run Kairos and your desired workloads (e.g. make sure the disk is at least 3 times the size of your image to allow for active, passive and recovery images).\nYou can also pass a Kairos config using the userdata field (Click on Advanced details -\u003e User data and put your Kairos config in the box).\nYou should at least specify a user and a password (or SSH key) if you need to SSH to the instance (Check the Getting started page for some examples).\nWhen you click on Launch instance the instance will be created and Kairos will boot into “auto-reset mode” by default. This means, that Kairos will “install” itself on the first boot and then reboot.\nYou can specify a different image to be installed using a block like the following in the userdata:\nreset: source: \"oci:quay.io/kairos/opensuse:leap-15.6-standard-amd64-generic-master-k3sv1.32.1-rc2-k3s1\" This will reset to the specified image on the first boot instead of the image booted.\nAccess the instance Once the instance is running, you can access it via SSH. Make sure reset has completed and the system has rebooted into “active” mode. The following command should report “active_boot”:\nkairos-agent state get boot (It it reports recovery_boot, the system is still in the installation process. Wait a few minutes and try again.)\n","categories":"","description":"Install Kairos on AWS","excerpt":"Install Kairos on AWS","ref":"/docs/installation/aws/","tags":"","title":"Installation on AWS"},{"body":"As the source for install or upgrade can be an OCI image and sometimes those are behind a private container registry, Kairos implements the default basic authentication used by docker for private registries.\nTo install/upgrade with a container image behind a registry with authentication, Kairos reads the following files in order to find about registry auth:\n${XDG_CONFIG_HOME}/.docker/config.json If set, DOCKER_CONFIG environment variable which points to a directory as per the docs. ${XDG_RUNTIME_DIR}/containers/auth.json for podman Note If you are using sudo to perform the upgrade, you have a couple of options:\nYou can create the file /root/.docker/config.json with your credentials. You can use sudo --preserve-env to give sudo access to the relevant environment variables. See the login docs for docker or the login docs for podman for more information.\nYou can also just generate that file yourself with the proper auth parameters like so:\n{ \"auths\": { \"registry.example.com\": { \"auth\": \"a2Fpcm9zOmh1bnRlcjIK\" } } } The auths map has an entry per registry, and the auth field contains your username and password encoded as HTTP ‘Basic’ Auth.\nNOTE: This means that your credentials are stored in plaintext. Have a look at the docker docs for the credentials-store\n","categories":"","description":"","excerpt":"As the source for install or upgrade can be an OCI image and sometimes …","ref":"/docs/advanced/private_registry_auth/","tags":"","title":"Private registries authentication"},{"body":"Welcome to the Kairos configuration reference page. This page provides details on the fields available in the YAML file used for installing Kairos, a Linux distribution focused on running Kubernetes. This file, written in cloud-config format, allows you to enable Kairos features, configure k3s, and set various other options.\nThe structure of the configuration file is as follows:\n#cloud-config # Additional system users users: - name: \"kairos\" passwd: \"kairos\" lock_passwd: true groups: [ \"admin\" ] # ssh_authorized_keys: # - github:mudler # enable debug logging debug: true # Logs configuration for collecting diagnostic information # Note: The following are examples of additional services and files to collect. # The system already includes comprehensive defaults for Kairos services. logs: # Additional systemd journal services to collect logs from journal: - \"my-custom-service\" - \"my-app\" # Additional log files to collect (supports glob patterns) files: - \"/var/log/myapp/*.log\" - \"/var/log/custom/*.log\" # Additional paths for look for cloud-init files cloud-init-paths: - \"/some/path\" # fail on cloud-init errors, defaults to false strict: false # The install block is to drive automatic installations without user interaction. install: # Device for automated installs # This can be either a full device path (so /dev/sda) or you can use the udev facility to identify the disk by UUID, path, label, diskseq or id (/dev/disk/by-{uuid,label,path,diskseq}) # Note that to use a disk by UUID or label, it has first to have that added from userspace, for example with `mkfs.ext4 -L LABEL -U UUID /dev/sda` otherwise disks dont come with UUID/label if they are empty device: \"/dev/sda\" # Reboot after installation reboot: true # Power off after installation poweroff: true # Set to true when installing without Pairing auto: true # Override the grub entry name grub-entry-name: Kairos # partitions setup # setting a partition size key to 0 means that the partition will take over the rest of the free space on the disk # after creating the rest of the partitions # by default the persistent partition has a value of 0 # if you want any of the extra partitions to fill the rest of the space, you will need to set the persistent partition # size to a different value, for example # partitions: # persistent: # size: 300 # default partitions # only 'oem', 'recovery', 'state' and 'persistent' objects allowed # Only size and fs should be changed here # size in MiB partitions: oem: size: 60 fs: ext4 recovery: size: 4096 fs: ext4 # note: This can also be set with dot notation like the following examples for a more condensed view: # partitions.oem.size: 60 # partitions.oem.fs: ext4 # partitions.recovery.size: 4096 # partitions.recovery.fs: ext4 # extra partitions to create during install # only size, label and fs are used # name is used for the partition label, but it's not really used during the kairos lifecycle. No spaces allowed. # if no fs is given the partition will be created but not formatted # These partitions are not automounted only created and formatted extra-partitions: - name: myPartition size: 100 fs: ext4 label: ONE_PARTITION - name: myOtherPartition size: 200 fs: ext4 label: TWO_PARTITION # no-format: true skips any disk partitioning and formatting # If set to true installation procedure will error out if expected # partitions are not already present within the disk. no-format: false # if no-format is used and Kairos is running over an existing deployment # force can be used to force installation. force: false # Creates these dirs in the rootfs during installation. As the rootfs is RO from boot, sometimes we find that we # some applications want to write to non-standard paths like /data # If that dir is not already in the rootfs it makes it difficult to create that path on an RO system # This allows to create some extra paths in the rootfs that then we count use for mounting or binding via # the cloud-config stages extra-dirs-rootfs: - /data - /src # Override image sizes for active/passive/recovery # Note that the active+passive images are stored in the state partition and # the recovery in the recovery partition, so they should be big enough to accommodate te images sizes set below # size in MiB system: size: 4096 passive: size: 4096 recovery-system: size: 5000 # note: This can also be set with dot notation like the following examples for a more condensed view: # system.size: 4096 # passive.size: 4096 # recovery-system.size: 5000 # Use a different source for the installation source: \"oci:..\" # Add bundles in runtime bundles: - ... # Set grub options grub_options: # additional Kernel option cmdline to apply extra_cmdline: \"config_url=http://\" # Same, just for active extra_active_cmdline: \"\" # Same, just for passive extra_passive_cmdline: \"\" # Change GRUB menu entry default_menu_entry: \"\" # Environmental variable to set to the installer calls env: - foo=bar # custom user mounts # bind mounts, can be read and modified, changes persist reboots bind_mounts: - /mnt/bind1 - /mnt/bind2 # ephemeral mounts, can be read and modified, changed are discarded at reboot ephemeral_mounts: # The reset block configures what happens when reset is called reset: # Reboot after reset reboot: true # Power off after reset poweroff: true # Use a different source for the reset source: \"oci:..\" # Override the grub entry name grub-entry-name: Kairos # if set to true it will format persistent partitions ('oem 'and 'persistent') reset-persistent: true reset-oem: false # Creates these dirs in the rootfs during reset. As the rootfs is RO from boot, sometimes we find that we # some applications want to write to non-standard paths like /data # If that dir is not already in the rootfs it makes it difficult to create that path on an RO system # This allows to create some extra paths in the rootfs that then we count use for mounting or binding via # the cloud-config stages extra-dirs-rootfs: - /data - /src # The upgrade block configures what happens when upgrade is called upgrade: # Reboot after upgrade reboot: true # Power off after upgrade poweroff: true # Use a different source for the upgrade source: \"oci:..\" # Override the grub entry name grub-entry-name: Kairos # if set to true upgrade command will upgrade recovery system instead # of main active system recovery: false # Override image sizes for active/recovery # Note that the active+passive images are stored in the state partition and # the recovery in the recovery partition, so they should be big enough to accommodate te images sizes set below # size in MiB # During upgrade only the active or recovery image cna be resized as those are the ones that contain the upgrade # passive image is the current system, and that its untouched during the upgrade system: size: 4096 recovery-system: size: 5000 # Creates these dirs in the rootfs during upgrade. As the rootfs is RO from boot, sometimes we find that we # some applications want to write to non-standard paths like /data # If that dir is not already in the rootfs it makes it difficult to create that path on an RO system # This allows to create some extra paths in the rootfs that then we count use for mounting or binding via # the cloud-config stages extra-dirs-rootfs: - /data - /src k3s: # Additional env/args for k3s server instances env: K3S_RESOLV_CONF: \"\" K3S_DATASTORE_ENDPOINT: \"mysql://username:password@tcp(hostname:3306)/database-name\" args: - --node-label \"\" - --data-dir \"\" # Enabling below it replaces args/env entirely # replace_env: true # replace_args: true k3s-agent: # Additional env/args for k3s agent instances env: K3S_NODE_NAME: \"foo\" args: - --private-registry \"...\" # Enabling below it replaces args/env entirely # replace_env: true # replace_args: true # The p2p block enables the p2p full-mesh functionalities. # To disable, don't specify one. p2p: # Manually set node role. Available: master, worker. Defaults auto (none). This is available role: \"master\" # User defined network-id. Can be used to have multiple clusters in the same network network_id: \"dev\" # Enable embedded DNS See also: https://mudler.github.io/edgevpn/docs/concepts/overview/dns/ dns: true # Disabling DHT makes co-ordination to discover nodes only in the local network disable_dht: true #Enabled by default # Configures a VPN for the cluster nodes vpn: create: false # defaults to true use: false # defaults to true env: # EdgeVPN environment options DHCP: \"true\" # Disable DHT (for airgap) EDGEVPNDHT: \"false\" EDGEVPNMAXCONNS: \"200\" # If DHCP is false, it's required to be given a specific node IP. Can be arbitrary ADDRESS: \"10.2.0.30/24\" # See all EDGEVPN options: # - https://github.com/mudler/edgevpn/blob/master/cmd/util.go#L33 # - https://github.com/mudler/edgevpn/blob/master/cmd/main.go#L48 # Automatic cluster deployment configuration auto: # Enables Automatic node configuration (self-coordination) # for role assignment enable: true # HA enables automatic HA roles assignment. # A master cluster init is always required, # Any additional master_node is configured as part of the # HA control plane. # If auto is disabled, HA has no effect. ha: # Enables HA control-plane enable: true # Number of HA additional master nodes. # A master node is always required for creating the cluster and is implied. # The setting below adds 2 additional master nodes, for a total of 3. master_nodes: 2 # Use an External database for the HA control plane external_db: \"external-db-string\" # network_token is the shared secret used by the nodes to co-ordinate with p2p network_token: \"YOUR_TOKEN_GOES_HERE\" ## Sets the Elastic IP used in KubeVIP. Only valid with p2p kubevip: eip: \"192.168.1.110\" # Specify a manifest URL for KubeVIP. Empty uses default manifest_url: \"\" # Enables KubeVIP enable: true # Specifies a KubeVIP Interface interface: \"ens18\" # Additional cloud init syntax can be used here. # See `stages` below. stages: network: - name: \"Setup users\" authorized_keys: kairos: - github:mudler # Standard cloud-init syntax, see: https://github.com/mudler/yip/tree/e688612df3b6f24dba8102f63a76e48db49606b2#compatibility-with-cloud-init-format growpart: devices: ['/'] runcmd: - foo hostname: \"bar\" write_files: - encoding: b64 content: CiMgVGhpcyBmaWxlIGNvbnRyb2xzIHRoZSBzdGF0ZSBvZiBTRUxpbnV4 path: /foo/bar permissions: \"0644\" owner: \"bar\" The p2p block is used to enable the p2p full-mesh functionalities of Kairos. If you do not want to use these functionalities, simply don’t specify a kairos block in your configuration file.\nInside the p2p block, you can specify the network_token field, which is used to establish the p2p full meshed network. If you do not want to use the full-mesh functionalities, don’t specify a network_token value.\nThe role field allows you to manually set the node role for your Kairos installation. The available options are master and worker, and the default value is auto (which means no role is set).\nThe network_id field allows you to set a user-defined network ID, which can be used to have multiple Kairos clusters on the same network.\nFinally, the dns field allows you to enable embedded DNS for Kairos. For more information on DNS in Kairos, see the link provided in the YAML code above.\nThat’s a brief overview of the structure and fields available in the Kairos configuration file. For more detailed information on how to use these fields, see the examples and explanations provided in the sections below.\nSyntax Kairos supports a portion of the standard cloud-init syntax, and the extended syntax which is based on yip.\nExamples using the extended notation for running K3s as agent or server can be found in the examples directory of the Kairos repository.\nHere’s an example that shows how to set up DNS at the boot stage using the extended syntax:\n#cloud-config stages: boot: - name: \"DNS settings\" dns: path: /etc/resolv.conf nameservers: - 8.8.8.8 Note Kairos does not use cloud-init. yip was created with the goal of being distro agnostic, and does not use Bash at all (with the exception of systemd configurations, which are assumed to be available). This makes it possible to run yip on minimal Linux distros that have been built from scratch.\nThe rationale behind using yip instead of cloud-init is that it allows Kairos to have very minimal requirements. The cloud-init implementation has dependencies, while yip does not, which keeps the dependency tree small. There is also a CoreOS implementation of cloud-init, but it makes assumptions about the layout of the system that are not always applicable to Kairos, making it less portable.\nThe extended syntax can also be used to pass commands through Kernel boot parameters. See the examples below for more details.\nTest your cloud configs Writing YAML files can be a tedious process, and it’s easy to make syntax or indentation errors. To make sure your configuration is correct, you can use the cloud-init commands to test your YAML files locally in a container.\nHere’s an example of how to test your configurations on the initramfs stage using a Docker container:\n# List the YAML files in your current directory $ ls -liah total 4,0K 9935 drwxr-xr-x. 2 itxaka itxaka 60 may 17 11:21 . 1 drwxrwxrwt. 31 root root 900 may 17 11:28 .. 9939 -rw-r--r--. 1 itxaka itxaka 59 may 17 11:21 00_test.yaml $ cat 00_test.yaml stages: initramfs: - commands: - echo \"hello!\" # Run the cloud-init command on your YAML files in a Docker container $ docker run -ti -v $PWD:/test --entrypoint /usr/bin/kairos-agent --rm quay.io/kairos/@flavor:@flavorRelease-core-amd64-generic-master run-stage --cloud-init-paths /test initramfs # Output from the run-stage command INFO[2023-05-17T11:32:09+02:00] kairos-agent version 0.0.0 INFO[2023-05-17T11:32:09+02:00] Running stage: initramfs.before INFO[2023-05-17T11:32:09+02:00] Done executing stage 'initramfs.before' INFO[2023-05-17T11:32:09+02:00] Running stage: initramfs INFO[2023-05-17T11:32:09+02:00] Processing stage step ''. ( commands: 1, files: 0, ... ) INFO[2023-05-17T11:32:09+02:00] Command output: hello! INFO[2023-05-17T11:32:09+02:00] Done executing stage 'initramfs' INFO[2023-05-17T11:32:09+02:00] Running stage: initramfs.after INFO[2023-05-17T11:32:09+02:00] Done executing stage 'initramfs.after' INFO[2023-05-17T11:32:09+02:00] Running stage: initramfs.before INFO[2023-05-17T11:32:09+02:00] Done executing stage 'initramfs.before' INFO[2023-05-17T11:32:09+02:00] Running stage: initramfs INFO[2023-05-17T11:32:09+02:00] Done executing stage 'initramfs' INFO[2023-05-17T11:32:09+02:00] Running stage: initramfs.after INFO[2023-05-17T11:32:09+02:00] Done executing stage 'initramfs.after' Validate Your Cloud Config Note Validation of configuration is available on Kairos v1.6.0-rc1 and later. If you’re interested in the validation rules or want to build a tool based on it, you can access them online via https://kairos.io/RELEASE/cloud-config.json e.g. v1.6.0 cloud-config.json You have two options to validate your Cloud Config, one is with the Kairos command line, and the other with the Web UI.\nConfiguration Validation via the Kairos Command Line To validate a configuration using the command line, we have introduced the validate command. As an argument you need to pass a URL or local file to be validated, e.g.:\nIf you had the following cloud-config.yaml in the current working directory\n#cloud-config users: - name: 007 You could validate it as follows\nkairos validate ./cloud-config.yaml jsonschema: '/users/0/name' does not validate with file:///home/mauro/workspace/kairos/schema.json#/properties/users/items/$ref/properties/name/type: expected string, but got number Configuration Validation via Web UI The validation in the Web UI is automatic, all you need to do is copy/paste or type your configuration on the input.\nUsing templates Fields in the Kairos cloud-init configuration can be templated, which allows for dynamic configuration. Node information is retrieved using the sysinfo library, and can be templated in the commands, file, and entity fields.\nHere’s an example of how you can use templating in your Kairos configuration:\n#cloud-config stages: foo: - name: \"echo\" commands: - echo \"{{.Values.node.hostname}}\" In addition to standard templating, sprig functions are also available for use in your Kairos configuration.\nAutomatic Hostname at scale You can also use templating to automatically generate hostnames for a set of machines. For example, if you have a single cloud-init file that you want to use for multiple machines, you can use the machine ID (which is generated for each host) to automatically set the hostname for each machine.\nHere’s an example of how you can do this:\n#cloud-config stages: initramfs: - name: \"Setup hostname\" hostname: \"node-{{ trunc 4 .MachineID }}\" This will set the hostname for each machine based on the first 4 characters of the machine ID. For example, if the machine ID for a particular machine is abcdef123456, the hostname for that machine will be set to node-abcd.\nGrub options The install.grub_options field in the Kairos configuration file allows you to set key/value pairs for GRUB options that will be set in the GRUB environment after installation.\nHere’s an example of how you can use this field to set the panic=0 boot argument:\n#cloud-config install: grub_options: extra_cmdline: \"panic=0\" The table below lists all the available options for the install.grub_options field:\nVariable Description next_entry Set the next reboot entry saved_entry Set the default boot entry default_menu_entry Set the name entries on the GRUB menu extra_active_cmdline Set additional boot commands when booting into active extra_passive_cmdline Set additional boot commands when booting into passive extra_recovery_cmdline Set additional boot commands when booting into recovery extra_cmdline Set additional boot commands for all entries default_fallback Sets default fallback logic The order of the cmdline parameters is as follows:\nExisting cmdline parameters, shipped with Kairos by default and non-modifiable 2.extra_cmdline 3.extra_active_cmdline or extra_passive_cmdline or extra_recovery_cmdline depending on the entry being booted Note that usually parameters for dracut and such are overridable, as they use the latest specified value in the cmdline.\nFor example, the rd.neednet=0 parameter is shipped with Kairos by default, but if you set rd.neednet=1 in extra_cmdline, it will override the default value and enable networking during the initramfs stage.\nAlso note that the grub_options for cmdline are only applied during installation. Changing them after installation won’t have any effect. If you want to change the GRUB options after installation, you can do so by setting those values under the /oem/grubenv file as follows:\ngrub2-editenv /oem/grubenv set extra_cmdline=\"rd.neednet=1\" As a final note, just a reminder that during GRUB menu selection, you can press e to edit the cmdline for that boot only, which is useful for testing purposes. That allows to test extra cmdline parameters during a single boot before making them permanent.\nKubernetes manifests The k3s distribution of Kubernetes allows you to automatically deploy Helm charts or Kubernetes resources after deployment.\nHere’s an example of how you can use the k3s configuration file to deploy Fleet out of the box:\nname: \"Deploy fleet out of the box\" stages: boot: - name: \"Copy fleet deployment files\" files: - path: /var/lib/rancher/k3s/server/manifests/fleet-config.yaml content: | apiVersion: v1 kind: Namespace metadata: name: cattle-system --- apiVersion: helm.cattle.io/v1 kind: HelmChart metadata: name: fleet-crd namespace: cattle-system spec: chart: https://github.com/rancher/fleet/releases/download/v0.3.8/fleet-crd-0.3.8.tgz --- apiVersion: helm.cattle.io/v1 kind: HelmChart metadata: name: fleet namespace: cattle-system spec: chart: https://github.com/rancher/fleet/releases/download/v0.3.8/fleet-0.3.8.tgz This configuration will automatically deploy the Fleet Helm chart in the cattle-system namespace after the deployment of k3s using the extended syntax.\nKernel boot parameters All the configurations can be issued via Kernel boot parameters, for instance, consider to add an user from the boot menu:\nstages.boot[0].authorized_keys.root[0]=github:mudler\nOr to either load a config url from network:\nconfig_url=http://...\nUsually secret gists are used to share such config files.\nAdditional users Kairos comes with the kairos user pre-configured, however, it is possible to configure additional users to the system via the cloud-init config mechanism\nAdd a user Consider the following example cloud-config, containing the default kairos user (which always has sudo access) and adds the testuser user to the system with admin access:\n#cloud-config install: device: /dev/sda k3s: enabled: true users: - name: \"kairos\" passwd: \"kairos\" ssh_authorized_keys: - github:mudler - name: \"testuser\" passwd: \"testuser\" ssh_authorized_keys: - github:mudler groups: - \"admin\" The above cloud config will be respected on every boot. Adding a user in the config at any point will be reflected on the next boot. The top level users: key is mapped automatically to a boot stage.\nFor this reason, the above snippet is equivalent to adding the user by explicitly defining the stage. E.g. by creating this file inside /oem:\nstages: initramfs: - name: \"Set user and password\" users: testuser: groups: - \"admin\" passwd: \"mypassword\" shell: /bin/bash homedir: \"/home/testuser\" This configuration can be either manually copied over, or can be propagated also via Kubernetes using the system upgrade controller. See the after-install section for an example.\n❯ ssh testuser@192.168.1.238 testuser@192.168.1.238's password: Welcome to kairos! Refer to https://kairos.io for documentation. localhost:~$ sudo su - localhost:~# whoami root localhost:~# exit localhost:~$ whoami testuser localhost:~$ Provider configs Providers are small binaries that can be used to extend the capabilities of Kairos. They are typically used to provide additional functionality or to integrate with external systems like k3s, k0s, rke2, or other Kubernetes distributions or even to provide additional functionality like p2p networking.\nThis allows to use the same configuration file to install different Kubernetes distributions or to enable additional features like p2p networking.\nBelow is a list of the configurations available for the current providers.\nNote that there is currently more providers available but some are community maintained. You should refer to the provider documentation for more information on how to use them.\nk3s k3s-agent k0s k0s-worker kubevip kubeadm p2p Key Description k3s.enabled Enables the k3s server instance. Accepted: true, false. k3s.env Additional environment variables for the k3s server instance. k3s.args Additional arguments for the k3s server instance. k3s.replace_env Replaces all environment variables otherwise passed to k3s by Kairos with those supplied here. Make sure you pass all the environment variables you need. k3s.replace_args Replaces all arguments otherwise passed to k3s by Kairos with those supplied here. Make sure you pass all the arguments you need. k3s.embedded_registry Enables the embedded registry in k3s. Accepted: true, false. WARNING: The K3s args are only applied when the K3s service is created, which is during installation. Changing the args key after installation won’t have any effect.\nIn order to override an existing k3s install arguments, you need to override the default service ones.\nFor Alpine services, the trick is to write via cloud config the /etc/rancher/k3s/k3s.env file to set the proper command_args like so:\nstages: initramfs: - name: \"Override k3s environment\" environment_file: /etc/rancher/k3s/k3s.env environment: command_args: server --verbose For systemd services, the usual override methods from systemd itself are available to override any services config, so we can lean on the yip plugin for systemd:\nstages: initramfs: - name: \"Expand k3s modules load\" systemctl: overrides: - service: k3s.service content: | [Service] ExecStartPre=-/sbin/modprobe nfs Key Description k3s-agent.enabled Enables the k3s agent instance. Accepted: true, false. k3s-agent.env Additional environment variables for the k3s server instance. k3s-agent.args Additional arguments for the k3s server instance. k3s-agent.replace_env Replaces all environment variables otherwise passed to k3s by Kairos with those supplied here. Make sure you pass all the environment variables you need. k3s-agent.replace_args Replaces all arguments otherwise passed to k3s by Kairos with those supplied here. Make sure you pass all the arguments you need. k3s-agent.embedded_registry Enables the embedded registry in k3s. Accepted: true, false. Key Description k0s.enabled Enables the k0s server instance. Accepted: true, false. k0s.env Additional environment variables for the k0s server instance. k0s.args Additional arguments for the k0s server instance. k0s.replace_env Replaces all environment variables otherwise passed to k3s by Kairos with those supplied here. Make sure you pass all the environment variables you need. k0s.replace_args Replaces all arguments otherwise passed to k3s by Kairos with those supplied here. Make sure you pass all the arguments you need. Key Description k0s-worker.enabled Enables the k0s worker instance. Accepted: true, false. k0s-worker.env Additional environment variables for the k0s worker instance. k0s-worker.args Additional arguments for the k0s worker instance. k0s-worker.replace_env Replaces all environment variables otherwise passed to k3s by Kairos with those supplied here. Make sure you pass all the environment variables you need. k0s-worker.replace_args Replaces all arguments otherwise passed to k0s by Kairos with those supplied here. Make sure you pass all the arguments you need. Key Description kubevip.enable Enables kubevip. Accepted: true, false. kubevip.eip VIP address to use kubevip.manifest_url Download and use the manifest from that url. You can see the default used otherwise here kubevip.interface Interface to use for the Kubevip EIP to attach to kubevip.static_pod Use a pod deployment for Kubevip instead of a daemonset. Accepted: true, false kubevip.version Set the specific Kubevip version to use Key Description cluster.cluster_token Unique string that can be used to distinguish different clusters on networks with multiple clusters. cluster.control_plane_host Host that all nodes can resolve and use for node registration. cluster.role The role of the node in the cluster. Accepted values are master, worker, and none. Defaults to none. cluster.config The configuration for the cluster. cluster.env List of environment variables to be set on the cluster. cluster.local_images_path Path to the local archive images to import. As P2P is a very complex topic, we have a dedicated P2P documentation page that explains how to use it with deep details.\nStages The stages key is a map that allows to execute blocks of cloud-init directives during the lifecycle of the node stages.\nA full example of a stage is the following:\n#cloud-config stages: # \"boot\" is the stage boot: - systemd_firstboot: keymap: us - files: - path: /tmp/bar content: | test permissions: 0777 owner: 1000 group: 100 if: \"[ ! -e /tmp/bar ]\" - files: - path: /tmp/foo content: | test permissions: 0777 owner: 1000 group: 100 commands: - echo \"test\" modules: - nvidia environment: FOO: \"bar\" systctl: debug.exception-trace: \"0\" hostname: \"foo\" systemctl: enable: - foo disable: - bar start: - baz mask: - foobar authorized_keys: user: - \"github:mudler\" - \"ssh-rsa ....\" dns: path: /etc/resolv.conf nameservers: - 8.8.8.8 ensure_entities: - path: /etc/passwd entity: | kind: \"user\" username: \"foo\" password: \"pass\" uid: 0 gid: 0 info: \"Foo!\" homedir: \"/home/foo\" shell: \"/bin/bash\" delete_entities: - path: /etc/passwd entity: | kind: \"user\" username: \"foo\" password: \"pass\" uid: 0 gid: 0 info: \"Foo!\" homedir: \"/home/foo\" shell: \"/bin/bash\" datasource: path: \"/usr/local/etc\" providers: - \"digitalocean\" - \"aws\" - \"gcp\" Note multiple stages can be specified, to execute blocks into different stages, consider:\n#cloud-config stages: boot: - commands: - echo \"hello from the boot stage\" initramfs: - commands: - echo \"hello from the boot stage\" - commands: - echo \"so much wow, /foo/bar bar exists!\" if: \"[ -e /foo/bar ]\" Logs Configuration The logs configuration allows you to specify additional systemd journal services and log files to collect when using the kairos-agent logs command. This is useful for debugging and issue reporting.\nThe system already includes comprehensive defaults for Kairos services and common log files. You only need to specify additional services or files that are not covered by the defaults.\nDefault Services The following services are automatically included:\nkairos-agent, kairos-installer, kairos-webui cos-setup-boot, cos-setup-fs, cos-setup-network, cos-setup-reconcile k3s, k3s-agent, k0scontroller, k0sworker Default Files The following log file patterns are automatically included:\n/var/log/kairos/*.log /var/log/*.log /run/immucore/*.log journal A list of additional systemd journal service names to collect logs from. Only specify services that are not already included in the defaults.\nfiles A list of additional log file paths to collect. Supports glob patterns for matching multiple files. Only specify files that are not already covered by the default patterns.\nExample configuration for adding custom services and files:\n#cloud-config logs: journal: - \"my-custom-service\" - \"my-app\" files: - \"/var/log/myapp/*.log\" - \"/var/log/custom/*.log\" Below you can find a list of all the supported fields. Mind to replace with the appropriate stage you want to hook into.\nFiltering stages by node hostname Stages can be filtered using the node key with a hostname value:\n#cloud-config stages: foo: - name: \"echo\" commands: - echo hello node: \"the_node_hostname_here\" # Node hostname Filtering stages with if statement Stages can be skipped based on if statements:\n#cloud-config stages: foo: - name: \"echo\" commands: - echo hello if: \"cat /proc/cmdline | grep debug\" name: \"Test yip!\" The expression inside the if will be evaluated in bash and, if specified, the stage gets executed only if the condition returns successfully (exit 0).\nname A description of the stage step. Used only when printing output to console.\nnode If defined, the node hostname where this stage has to run, otherwise it skips the execution. The node can also be a regexp in the Golang format.\n#cloud-config stages: boot: - name: \"Setup logging\" node: \"bastion\" Modules For each stage, a number of modules are available, that implement various useful functions. Read more about them in this page: Stage modules\nRunning commands on different shells By default, all commands are executed in the sh shell. However, it is possible to run commands in a different shell by prefixing the command with the executable.\nFor example, to run a command in the bash shell, you can use the following syntax:\n#cloud-config stages: boot.after: - name: \"do something\" commands: - bash /path/to/script.sh ","categories":"","description":"Welcome to the Kairos configuration reference page. This page provides details on the fields available in the YAML file used for installing Kairos, a Linux distribution focused on running Kubernetes. This file, written in cloud-config format, allows you to enable Kairos features, configure k3s, and set various other options.","excerpt":"Welcome to the Kairos configuration reference page. This page provides …","ref":"/docs/reference/configuration/","tags":"","title":"Configuration"},{"body":"Kairos is a container-based operating system (OS).\nA container-based operating system is an OS that is shipped via containers. Indeed, if it happens to be based on Linux (most probably), you can run the container image as well on your Docker daemon. The image being booted is the container, which contains all the required pieces in order to boot (Kernel, Initrd, Init system). There is no real container runtime running the image. The container is used to construct an image internally that is then used to boot the system in an A/B fashion, so there is no overhead introduced. The system being booted is actually a snapshot of the container.\nSingle-image The OS is a single container image which contains all the OS components, including Kernel and Initrd. Tamper-proof upgrades Upgrades are atomic, A/B swaps with fallback mechanisms and automatic boot assessment. Distributed via container registries Bootable images are standard OCI artifacts that can be hosted in any container registry. Platform Engineer-friendly Adapt the infrastructure to your needs by plugging images into your already-existing workflow pipeline. Customizing an immutable OS becomes as easy as writing a Dockerfile. A/B Upgrades Upgrades are atomic operations that can be triggered manually or via Kubernetes. The node will create a transition image that will be swapped for the Active system, and the Active system becomes Passive. This ensures tamper-proof upgrades and automated fallback and boot assessment strategies are in place to automatically boot from the fallback system. The recovery image can be furthermore exploited to completely automatize node recovery.\nBenefits Container registries are already widely supported and used by anyone. Reduce infrastructure drift, by pushing upgrades as single images, with atomic upgrades. If you are operating a Kubernetes cluster and deploying applications on top, chances are that you already have a container registry deployed somewhere and configured to store them or manage your infrastructure stack. By using container images, you can reuse the same infrastructure to propagate upgrades to the nodes and handle customizations.\nContainer images can be extended after a build by using standard container building practices and can seamlessly plug into your existing pipelines. Kairos allows you to seamlessly upgrade to container images that are derived from other versions.\nWe believe that bringing rollbacks, or incremental patches upgrades increases the exposure to infrastructure drift. In opposition, immutable, single images are deployed to the nodes as they were apps - no more discrepancies in your nodes - no need of configuration management tools like Chef, Ansible, or alikes.\nThis means that to customize a Kairos version, all that is required is to build a standard container image with a plain Dockerfile—plus, the bits that are actually needed - we can’t touch a system as we are typically used to.\nIf you are familiar with Dockerfiles, then you are good to go to roll your own custom OS version to provision in the nodes. That removes any friction to questions like, “How do I add this package to my nodes?”, or more complex ones as, “How can I replace with my own Kernel?”.\nContainer Image based OS The Image support matrix in here lists all the container images built from our CI on every release of Kairos.\nTo inspect an image and run it locally, you can use a container engine like Docker or Podman:\ndocker pull quay.io/kairos/@flavor:@flavorRelease-core-amd64-generic-master We can run it locally with docker as a container to inspect it, as it is runnable:\n$ docker run -ti --rm quay.io/kairos/@flavor:@flavorRelease-core-amd64-generic-master $ cat /etc/os-release ... KAIROS_NAME=\"kairos-core-@flavor\" KAIROS_VERSION=\"master\" KAIROS_ID=\"kairos\" KAIROS_ID_LIKE=\"kairos-core-@flavor\" KAIROS_VERSION_ID=\"master\" KAIROS_PRETTY_NAME=\"kairos-core-@flavor master\" KAIROS_BUG_REPORT_URL=\"https://github.com/kairos-io/kairos/issues\" KAIROS_HOME_URL=\"https://github.com/kairos-io/kairos\" KAIROS_IMAGE_REPO=\"quay.io/kairos/@flavor:@flavorRelease-core-amd64-generic-master\" KAIROS_IMAGE_LABEL=\"latest\" KAIROS_GITHUB_REPO=\"kairos-io/kairos\" KAIROS_VARIANT=\"core\" KAIROS_FLAVOR=\"@flavor\" And check out things like what’s the kernel inside:\n$ ls -liah /boot/ total 102M 6692018 drwxr-xr-x 2 root root 4.0K Apr 16 2020 . 6817515 drwxr-xr-x 1 root root 4.0K Oct 10 16:11 .. 6692019 -rw-r--r-- 1 root root 65 Apr 16 2020 .vmlinuz-5.14.21-150400.24.21-default.hmac 6692020 -rw-r--r-- 1 root root 4.9M Apr 16 2020 System.map-5.14.21-150400.24.21-default 6692021 -rw-r--r-- 1 root root 1.7K Apr 16 2020 boot.readme 6692022 -rw-r--r-- 1 root root 245K Apr 16 2020 config-5.14.21-150400.24.21-default 6692023 lrwxrwxrwx 1 root root 35 Apr 16 2020 initrd -\u003e initrd-5.14.21-150400.24.21-default 6692024 -rw------- 1 root root 69M Apr 16 2020 initrd-5.14.21-150400.24.21-default 6692025 -rw-r--r-- 1 root root 443K Apr 16 2020 symvers-5.14.21-150400.24.21-default.gz 6692026 -rw-r--r-- 1 root root 484 Apr 16 2020 sysctl.conf-5.14.21-150400.24.21-default 6692027 -rw-r--r-- 1 root root 17M Apr 16 2020 vmlinux-5.14.21-150400.24.21-default.gz 6692028 lrwxrwxrwx 1 root root 36 Apr 16 2020 vmlinuz -\u003e vmlinuz-5.14.21-150400.24.21-default 6692029 -rw-r--r-- 1 root root 11M Apr 16 2020 vmlinuz-5.14.21-150400.24.21-default The CI process generates bootable medium by the container images, and similarly, we can modify this image to introduce our changes and remaster an ISO as described in Automated installation, but that can be resumed in the following steps:\n$ docker run -ti --name custom-container quay.io/kairos/@flavor:@flavorRelease-core-amd64-generic-master # # Do your changes inside the container.. # echo \"foo\" \u003e /foo # ... # exit $ docker commit custom-container custom-image \u003e sha256:37176f104a870480f9c3c318ab51f6c456571b6612b6a47b96af71b95a0a27c7 # Builds an ISO from it $ docker run -v $PWD:/cOS -v /var/run/docker.sock:/var/run/docker.sock -i --rm quay.io/kairos/auroraboot:v0.13.0 --debug build-iso --name \"custom-iso\" --date=false --output /cOS/ custom-image \u003e ... \u003e ... \u003e xorriso : UPDATE : Writing: 147456s 84.0% fifo 100% buf 50% 60.5xD \u003e ISO image produced: 175441 sectors \u003e Written to medium : 175472 sectors at LBA 48 \u003e Writing to '/cOS/custom-iso.iso' completed successfully. $ ls custom-iso.iso custom-iso.iso.sha256 In order to go further and upgrade nodes using this image, now the only requirement is to push it in a container registry and upgrade the nodes using that container image.\nFor upgrading to a container image see manual upgrades and kubernetes upgrades.\nSee also ISO remastering ","categories":"","description":"Discover how Kairos delivers its entire OS as a container image, enabling predictable upgrades and simple version control.","excerpt":"Discover how Kairos delivers its entire OS as a container image, …","ref":"/docs/architecture/container/","tags":"","title":"Container based"},{"body":"Kairos upgrades can be performed either manually or via Kubernetes if the cluster is composed of Kairos nodes. The recommended approach is to use the Kairos operator, which provides a more integrated and Kairos-specific way to manage upgrades.\nPrerequisites The Kairos operator needs to be deployed on the target cluster. Read the instructions here\nUpgrading from version X to version Y with Kubernetes To trigger an upgrade, create a NodeOpUpgrade resource which refers to the image version that you want to upgrade to. This is the recommended approach for upgrading Kairos nodes.\ncat \u003c\u003c'EOF' | kubectl apply -f - --- apiVersion: operator.kairos.io/v1alpha1 kind: NodeOpUpgrade metadata: name: kairos-upgrade namespace: default spec: # The container image containing the new Kairos version image: quay.io/kairos/@flavor # Example: quay.io/kairos/debian # NodeSelector to target specific nodes (optional) nodeSelector: matchLabels: kairos.io/managed: \"true\" # Maximum number of nodes that can run the upgrade simultaneously # 0 means run on all nodes at once concurrency: 1 # Whether to stop creating new jobs when a job fails # Useful for canary deployments stopOnFailure: true # Whether to upgrade the active partition (defaults to true) upgradeActive: true # Whether to upgrade the recovery partition (defaults to false) upgradeRecovery: false # Whether to force the upgrade without version checks force: false EOF To upgrade the “recovery” partition instead of the active one, set upgradeRecovery: true and upgradeActive: false:\nspec: # ... other fields ... upgradeActive: false upgradeRecovery: true To check all the available versions, see the images available on the container registry, corresponding to the flavor/version selected.\nNote The Kairos operator provides several upgrade strategies that can be configured through the NodeOpUpgrade resource. You can control concurrency, node selection, and failure handling. The example above shows a “canary upgrade” approach where nodes are upgraded one-by-one with failure detection. Jobs will be created for each node that needs to be upgraded. You can monitor the progress:\n$ kubectl get jobs -A NAMESPACE NAME STATUS COMPLETIONS DURATION AGE default kairos-upgrade-localhost-wr26f Running 0/1 24s 24s $ kubectl get nodeopupgrades NAME AGE kairos-upgrade 5s Done! We should have all the basics to get our first cluster rolling, but there is much more we can do.\nVerify images attestation during upgrades Container images can be signed during the build phase of a CI/CD pipeline using Cosign, Kairos signs every artifact as part of the release process.\nTo ensure that the images used during upgrades match the expected signatures, Kyverno can be used to set up policies. This is done by checking if the signature is present in the OCI registry and if the image was signed using the specified key. The policy rule check fails if either of these conditions is not met.\nTo learn more about this specific Kyverno feature, you can refer to the documentation. This allows for the verification of image authenticity directly at the node level prior to upgrading.\nA Kyverno policy for standard images might look like the following:\napiVersion: kyverno.io/v1 kind: ClusterPolicy metadata: name: check-image spec: validationFailureAction: Enforce background: false webhookTimeoutSeconds: 30 failurePolicy: Fail rules: - name: check-image match: any: - resources: kinds: - Pod verifyImages: - imageReferences: - \"quay.io/kairos/@flavor*\" attestors: - entries: # See: https://kyverno.io/docs/writing-policies/verify-images/#keyless-signing-and-verification - keyless: subject: \"https://github.com/kairos-io/provider-kairos/.github/workflows/release.yaml@refs/tags/*\" issuer: \"https://token.actions.githubusercontent.com\" rekor: url: https://rekor.sigstore.dev To install Kyverno in a Kairos cluster, you can simply use the community bundles. For example, you can use the following installation cloud config file:\n#cloud-config hostname: kyverno-{{ trunc 4 .MachineID }} # Specify the bundle to use bundles: - targets: - run://quay.io/kairos/community-bundles:cert-manager_latest - run://quay.io/kairos/community-bundles:kyverno_latest users: - name: kairos passwd: kairos groups: - admin k3s: enabled: true This configuration file prepares the system with the cert-manager and kyverno bundles, enabling k3s. The Kairos operator can be deployed separately using the instructions in the Kairos Operator documentation.\nCustomize the upgrade process For advanced customization, you can use a NodeOp resource directly instead of NodeOpUpgrade. This gives you full control over the upgrade process and allows you to run custom commands before or after the upgrade:\napiVersion: operator.kairos.io/v1alpha1 kind: NodeOp metadata: name: custom-kairos-upgrade namespace: default spec: # NodeSelector to target specific nodes nodeSelector: matchLabels: kairos.io/managed: \"true\" # The container image containing the new Kairos version image: quay.io/kairos/@flavor # Example: quay.io/kairos/debian # Custom command to execute command: - sh - -c - | set -e # Custom pre-upgrade commands echo \"Running pre-upgrade tasks...\" sed -i 's/something/to/g' /host/oem/99_custom.yaml # Run the upgrade mount --rbind /host/dev /dev mount --rbind /host/run /run kairos-agent upgrade --source dir:/ # Custom post-upgrade commands echo \"Running post-upgrade tasks...\" # Add any post-upgrade logic here # Path where the node's root filesystem will be mounted hostMountPath: /host # Whether to cordon the node before running the operation cordon: true # Drain options for pod eviction drainOptions: enabled: true force: false gracePeriodSeconds: 30 ignoreDaemonSets: true deleteEmptyDirData: false timeoutSeconds: 300 # Whether to reboot the node after successful operation rebootOnSuccess: true # Maximum number of nodes that can run the operation simultaneously concurrency: 1 # Whether to stop creating new jobs when a job fails stopOnFailure: true Upgrade from c3os to Kairos If you already have a c3os deployment, upgrading to Kairos requires changing every instance of c3os to kairos in the configuration file. This can be done using a custom NodeOp resource:\napiVersion: operator.kairos.io/v1alpha1 kind: NodeOp metadata: name: c3os-to-kairos-upgrade namespace: default spec: # NodeSelector to target specific nodes nodeSelector: matchLabels: kairos.io/managed: \"true\" # The container image containing the new Kairos version image: quay.io/kairos/@flavor # Example: quay.io/kairos/debian # Custom command to execute command: - sh - -c - | set -e # Replace c3os with kairos in configuration sed -i 's/c3os/kairos/g' /host/oem/99_custom.yaml # Run the upgrade mount --rbind /host/dev /dev mount --rbind /host/run /run kairos-agent upgrade --source dir:/ # Path where the node's root filesystem will be mounted hostMountPath: /host # Whether to cordon the node before running the operation cordon: true # Drain options for pod eviction drainOptions: enabled: true force: false gracePeriodSeconds: 30 ignoreDaemonSets: true deleteEmptyDirData: false timeoutSeconds: 300 # Whether to reboot the node after successful operation rebootOnSuccess: true # Maximum number of nodes that can run the operation simultaneously concurrency: 1 # Whether to stop creating new jobs when a job fails stopOnFailure: true What’s next? Upgrade nodes manually Immutable architecture Create decentralized clusters ","categories":"","description":"Learn how to upgrade Kairos using Kubernetes","excerpt":"Learn how to upgrade Kairos using Kubernetes","ref":"/docs/upgrade/kubernetes/","tags":"","title":"Upgrading from Kubernetes"},{"body":"The interactive installation can be accessed from the LiveCD ISO and guides the user into the installation process.\nIt generates a configuration file, which is later accessible after installation in the /oem/90_custom.yaml file.\nFrom the boot menu When loading any Kairos ISOs, a GRUB menu, like the following will be displayed. To access the interactive installation, select the third entry (kairos (interactive install)).\nManually The interactive installer can be also started manually with kairos-agent interactive-install from the LiveCD.\nCustomizing the Interactive Installer The interactive installer supports customization options that allow you to modify its appearance and available options. These customizations are controlled through files placed in the /etc/kairos/branding/ directory.\nColor Scheme Customization You can customize the color scheme of the interactive installer by creating a file at /etc/kairos/branding/interactive_install_colors. This file should contain environment variable definitions for the colors you want to override. You can specify any, all, or none of these variables - they will override the corresponding default colors.\nThe available color variables are:\nKAIROS_BG - Background color KAIROS_TEXT - Text color KAIROS_HIGHLIGHT - Primary highlight color KAIROS_HIGHLIGHT2 - Secondary highlight color KAIROS_ACCENT - Accent color KAIROS_BORDER - Border color CHECK_MARK - Check mark character/symbol Color Format Colors can be specified in two formats depending on your terminal capabilities:\nFor full color terminals (24-bit/true color): Use hex triplet format in RGB:\n# /etc/kairos/branding/interactive_install_colors KAIROS_BG=\"#03153a\" # Deep blue background KAIROS_TEXT=\"#ffffff\" # White text KAIROS_HIGHLIGHT=\"#e56a44\" # Orange highlight KAIROS_ACCENT=\"#ee5007\" # Accent orange CHECK_MARK=\"✓\" For simple/dumb terminals (16 colors): Use numbers 0-9 for basic colors:\n# /etc/kairos/branding/interactive_install_colors KAIROS_BG=\"0\" # Black background KAIROS_TEXT=\"7\" # White text KAIROS_HIGHLIGHT=\"9\" # Bright red highlight KAIROS_BORDER=\"9\" # Bright red border CHECK_MARK=\"*\" Note If you set values to 0-9, those simple colors will be used even on 256-color terminals. For the best experience on modern terminals, use hex triplet format. Disabling Advanced Options If you want to hide the “Customize Further” option in the interactive installer, you can create an empty file at /etc/kairos/branding/interactive_install_advanced_disabled. When this file exists, the installer will only show the “Start Install” option, simplifying the interface for users who don’t need advanced customization.\n# Create the file to disable advanced options touch /etc/kairos/branding/interactive_install_advanced_disabled These branding files should be included in your Kairos image build process or deployed to the system before running the interactive installer.\n","categories":"","description":"Install Kairos interactively","excerpt":"Install Kairos interactively","ref":"/docs/installation/interactive/","tags":"","title":"Interactive"},{"body":" Note After the installation, the password login is disabled, users, and SSH keys to log in must be configured via cloud-init. Cloud Configuration Kairos uses yip a subset of cloud-init to configure a node. Here’s a simple example:\n#cloud-config # Define the user accounts on the node. users: - name: \"kairos\" # The username for the user. passwd: \"kairos\" # The password for the user. ssh_authorized_keys: # A list of SSH keys to add to the user's authorized keys. - github:mudler # A key from the user's GitHub account. - \"ssh-rsa AAA...\" # A raw SSH key. # Enable K3s on the node. k3s: enabled: true # Set to true to enable K3s. What do these settings do?\nThe #cloud-config at the top is not a comment. Make sure to start your configuration file with it. users: This block defines the user accounts on the node. In this example, it creates a user named kairos with the password kairos and adds two SSH keys to the user’s authorized keys. k3s: This block enables K3s on the node. Check out the full configuration reference.\nSave this file as config.yaml and pass it to the kairos agent during the installation process.\nWarning The command is disruptive and will erase any content on the drive. sudo kairos-agent manual-install --device \"auto\" config.yaml This will configure the node as a single-node Kubernetes cluster and set the default password and SSH keys as specified in the configuration file.\n","categories":"","description":"Install Kairos manually","excerpt":"Install Kairos manually","ref":"/docs/installation/manual/","tags":"","title":"Manual installation"},{"body":" Warning You will need a Standard Kairos OS image in order to use QR Code feature. By default Kairos will display a QR code after booting the ISO to install the machine:\nThe QR Code is a base64 encoded string which is an edgevpn token. For example, you can scan the following QR Code from the video Introduction to Kairos - timestamp 4:16.\nThe base64 encoded string from the QR Code looks like this:\nb3RwOgogIGRodDoKICAgIGludGVydmFsOiA5MjIzMzcyMDM2ODU0Nzc1ODA3CiAgICBrZXk6IFY0NTYzWUhKNzdNVFZaMkVNRFk1QVZINklDNk1UNkU0MjdMVE1OQ1MyTVhWM1FWR1VESVEKICAgIGxlbmd0aDogMzIKICBjcnlwdG86CiAgICBpbnRlcnZhbDogOTIyMzM3MjAzNjg1NDc3NTgwNwogICAga2V5OiBNUlZTU05KQ09WWjYyV0dETFlXRE9OUkNDUTU0TEFVMkxMRkVONURNNERHTFlGWEZYVTRBCiAgICBsZW5ndGg6IDMyCnJvb206IDI3V0pWN1lNSzdXUzdRWVUzV0xPSVNRQUxZT0dFUjRRNUpNVVRVUk1UREZKS0E1NVZZWUEKcmVuZGV6dm91czogcWZNcHRBdFRzaUhxVmZWSEJhaXRBbXZQZFdySkJEcEMKbWRuczogSFB0WmlsSUp4UFhiUVRUSE93ZHhiWGZ4S3JvVmJmZEgKbWF4X21lc3NhZ2Vfc2l6ZTogMjA5NzE1MjAK Once this base64 string is decoded, the edgevpn token looks like this:\notp: dht: interval: 9223372036854775807 key: V4563YHJ77MTVZ2EMDY5AVH6IC6MT6E427LTMNCS2MXV3QVGUDIQ length: 32 crypto: interval: 9223372036854775807 key: MRVSSNJCOVZ62WGDLYWDONRCCQ54LAU2LLFEN5DM4DGLYFXFXU4A length: 32 room: 27WJV7YMK7WS7QYU3WLOISQALYOGER4Q5JMUTURMTDFJKA55VYYA rendezvous: qfMptAtTsiHqVfVHBaitAmvPdWrJBDpC mdns: HPtZilIJxPXbQTTHOwdxbXfxKroVbfdH max_message_size: 20971520 For more information about EdgeVPN, check out the architecture section.\nTo trigger the installation process via QR code, you need to use the Kairos CLI and provide a Cloud Config, as described in the Getting started guide. You can also see some Cloud Config examples in our Examples section. The CLI is currently available only for Linux and Windows. It can be downloaded from the release artifact:\nVERSION=$(wget -q -O- https://api.github.com/repos/kairos-io/provider-kairos/releases/latest | jq -r '.tag_name') curl -L https://github.com/kairos-io/provider-kairos/releases/download/${VERSION}/kairosctl-${VERSION}-linux-amd64.tar.gz -o - | tar -xvzf - -C . The CLI allows to register a node with a screenshot, an image, or a token. During pairing, the configuration is sent over, and the node will continue the installation process.\nIn a terminal window from your desktop/workstation, run:\nkairosctl register --reboot --device /dev/sda --config config.yaml The --reboot flag will make the node reboot automatically after the installation is completed. The --device flag determines the specific drive where Kairos will be installed. Replace /dev/sda with your drive. Any existing data will be overwritten, so please be cautious. The --config flag is used to specify the config file used by the installation process. Note By default, the CLI will automatically take a screenshot to get the QR code. Make sure it fits into the screen. Alternatively, an image path or a token can be supplied via arguments (e.g. kairosctl register /img/path or kairosctl register \u003ctoken\u003e). After a few minutes, the configuration is distributed to the node and the installation starts. At the end of the installation, the system is automatically rebooted.\n","categories":"","description":"Use the QR code displayed at boot to drive the installation","excerpt":"Use the QR code displayed at boot to drive the installation","ref":"/docs/installation/qrcode/","tags":"","title":"QR Code"},{"body":"By default when running the LiveCD, or during installation, Kairos will start a WebUI in the background, listening by default on the 8080 port:\nThe WebUI has an input form that accepts the YAML config file, features a syntax highlighter and a YAML syntax checker. You can find a full example in our documentation or navigate to our examples section.\n","categories":"","description":"Use the WebUI at boot to drive the installation","excerpt":"Use the WebUI at boot to drive the installation","ref":"/docs/installation/webui/","tags":"","title":"WebUI"},{"body":" Enki Enki has been deprecated in favor of AuroraBoot. Known Issues RPi EFI booting no longer supported on kernels shipped with Ubuntu 24.04+ #2249 RPi Alpine is a bit slow to sync the date on boot Deprecation Warnings Reading of /etc/elemental/config.yaml will be deprecated in favor of /etc/kairos/config.yaml in a future release. Use /etc/kairos/config.yaml instead. #2233 Config dirs read ordering Starting on v3.2.0, we have agreed on the order the config dirs are read upon. See the cloud-config page for more info. Remarkable Changes By default, Uki artifacts (identified by the -uki suffix) no longer include Linux modules and firmware in the image. Real-world testing has shown that many EFI firmwares are very particular about the size of the EFI image, often refusing to boot if the file exceeds 300-400MB. Given the wide variety of EFI firmware implementations, predicting whether a UKI EFI file will boot on different hardware is challenging.\nTo enhance compatibility, we decided to slim down the UKI files by removing the largest components: the Linux modules and firmware packages. This results in EFI files around 200-300MB, which are much more likely to boot correctly across various EFI implementations.\nHowever, this change comes with a trade-off. Smaller images, while being more compatible with a wide range of EFI firmwares, may lack comprehensive hardware support because they do not include all the Linux modules and firmware packages. This means that certain hardware components may not function correctly or optimally when using these slimmer UKI images.\nOn the other hand, larger UKI images, which include all necessary modules and firmware for extensive hardware support, provide better functionality and compatibility with a broad range of hardware. However, these larger images are more likely to encounter boot issues due to EFI firmware limitations, as many EFI implementations refuse to boot files larger than 300-400MB.\nWe publish -uki artifacts ourselves, which are the slimmed versions, as examples of how to build a slimmer UKI artifact. While these serve as a reference, we recommend always building your own custom images to tailor them to your specific hardware needs. If you need to include those packages for full hardware support, you can create a custom artifact to add them back, as detailed in the Kairos docs.\nWe recommend keeping your UKI EFI files as small as possible to maximize boot success across different EFI firmware implementations. While smaller images offer better compatibility, they may lack full hardware support. Conversely, larger images, which include all necessary modules and firmware, provide comprehensive hardware support but may fail to boot due to EFI firmware constraints.\n","categories":"","description":"A single stop to learn about Known Issues, Deprecation Warnings and/or Remarkable Changes in this release","excerpt":"A single stop to learn about Known Issues, Deprecation Warnings and/or …","ref":"/docs/announcements/","tags":"","title":"Announcements"},{"body":" Objective This guide will teach you the basics about immutability and configuration in Kairos. We will achieve this by configuring the hostname of your Kairos node. Prerequisites A single node Kairos cluster as the one deployed in the Getting Started guide. Do you prefer to watch a video? How is Kairos immutable? An immutable OS is an operating system that limits the amounts of changes you can do after it is deployed. Full immutability is not possible, as we still need to be able to configure the OS to our needs and to save data for our applications.\nWithin Kairos you will find two ways in which immutability is accomplished: Read Only Mounts and OverlayFS.\nRead Only Mounts By default, Kairos is mounted as read-only. If you try installing a package, for example, you will get an error like the following:\nsudo apt update ... sudo apt install ruby ... dpkg: error while cleaning up: unable to remove newly-extracted version of '/usr/share/doc/libruby': Read-only file system ... OverlayFS Alternatively, some directories are mounted as read-write using OverlayFS. This allows you to write to the OS, but the changes are not persistent across reboots.\nDo the next experiment as root:\nCreate the file /etc/hostname Add the text “master-node” in it and save Reboot the node After the node has rebooted, you will notice that the file is not present\ncat /etc/hostname cat: /etc/hostname: No such file or directory Configuring Kairos So how do you configure Kairos if it is immutable? The answer is simple: you use configuration files. These files are read at boot time and the changes are applied to the OS on every boot.\nYour system is already configured this way. Have a look at the file /oem/90_custom.yaml. If you followed the Getting Started guide, you should see a file with similar content to mine:\n#cloud-config install: device: /dev/vda k3s: enabled: true name: Config generated by the installer stages: network: - users: kairos: groups: - admin name: kairos passwd: kairos ssh_authorized_keys: - github:YOUR_GITHUB_USERNAME The name of the file is not important, but the extension and the location are. You must add your configuration files to /oem/ and they must have a .yaml extension. Finally, the first line of the file must be #cloud-config to be recognized by the system.\nThe install directive is used to give instructions on how to install the system when it runs on the first boot, so let’s ignore it for now.\nThe k3s directive is used to enable or disable the Kubernetes distribution that comes with Kairos. If you set it to false, the K3s services will not start on boot.\nThe name directive can be added at every level of the configuration file. It is useful to distinguish between different steps.\nThe stages directive is used to define the different stages of the configuration. The network stage is used to configure the system when network starts. In this case, we create a user called kairos with the password kairos and an SSH key.\nConfiguring the hostname Configuring the hostname is a simple task. All we have to do is add the hostname directive at the root level of the configuration file with the value we want to give to this node. The order of the directives is not important. Let’s add it after the k3s directive:\n#cloud-config install: device: /dev/vda k3s: enabled: true hostname: \"master-node\" name: Config generated by the installer stages: network: - users: kairos: groups: - admin name: kairos passwd: kairos ssh_authorized_keys: - github:YOUR_GITHUB_USERNAME Save the file and reboot the node. After the reboot, you should see the new hostname on the prompt.\nkairos@master-node:~$ Let’s take this a bit further. In the future we want to add a second master node to our cluster. Let’s then use a value that is unique to this node and add it as a suffix.\n#cloud-config install: device: /dev/vda k3s: enabled: true hostname: \"master-{{ trunc 4 .MachineID }}\" name: Config generated by the installer stages: network: - users: kairos: groups: - admin name: kairos passwd: kairos ssh_authorized_keys: - github:YOUR_GITHUB_USERNAME The .MachineID comes from YIP which is processing our configuration file, and trunc comes from sprig which extends Golang Templating functions. Save the file and reboot the node. After the reboot, you should see something similar to the following:\nkairos@master-fb0a:~$ Conclusion Congrats! You are one step closer to mastering Kairos. In this guide, you learned the basics of immutability and configuration in Kairos. You also learned how to configure the hostname of your Kairos node.\nFrequently Asked Questions (FAQs) Why is immutability important?\nIt reduces the attack surface of the OS and the chances of having snowflakes. Learn more about Immutable Linux OS.\nCan I still use my favorite System Configuration Management System?\nKairos has been designed to be configured a Cloud Init like approach but at the end of the day it is just a Linux distribution. Keep in mind that the immutability of the system will limit the changes you can make, but the better you understand Kairos, the more likely you will be able to configure it to play well with your CMS.\nWhy cloud init and not something else?\nCloud init is a standard in the industry and is widely supported. It is also very flexible and powerful. Read more about Cloud Init.\nWhat’s next? Ok, but how do I add a package?\nBuilding And Upgrading Guide What other configuration options are available?\nConfiguration Reference Learn more about the immutable architecture of Kairos\nImmutable OS ","categories":"","description":"Learn how to configure a fresh Kairos deployment with essential settings for secure and effective operation.\n","excerpt":"Learn how to configure a fresh Kairos deployment with essential …","ref":"/getting-started/initial-configuration/","tags":"","title":"Configuring your newly deployed Kairos cluster"},{"body":" Note This guide focuses on customizing Kairos images. For a complete guide on creating custom cloud images from scratch, including when and how to apply these customizations, see Creating Custom Cloud Images. Kairos is an open source, container-based operating system. To modify Kairos and add a package, you’ll need to build a container image from the Kairos images. Here’s an example with Docker which adds figlet:\nFROM quay.io/kairos/@flavor:@flavorRelease-standard-amd64-generic-master-k3sv1.33.4-k3s1 RUN zypper in -y figlet RUN export VERSION=\"my-version\" RUN envsubst '${VERSION}' \u003c/etc/os-release After creating your Dockerfile, you can build your own image by running the following command:\n$ docker build -t docker.io/\u003cyourorg\u003e/myos:0.1 . Sending build context to Docker daemon 2.048kB Step 1/3 : FROM quay.io/kairos/@flavor:@flavorRelease-standard-amd64-generic-master-k3sv1.33.4-k3s1 ---\u003e 897dc0cddf91 Step 2/3 : RUN zypper install -y figlet ---\u003e Using cache ---\u003e d57ff48546e7 Step 3/3 : RUN MY_VERSION=\"my-version\" \u003e\u003e /etc/os-release ---\u003e Running in b7bcb24969f5 Removing intermediate container b7bcb24969f5 ---\u003e ca21930a4585 Successfully built ca21930a4585 Successfully tagged \u003cyour-org\u003e/myos:0.1 Once you have built your image, you can publish it to Docker Hub or another registry with the following command:\n$ docker push \u003cyour-org\u003e/myos:0.1 The push refers to repository [docker.io/\u003cyour-org\u003e/myos] c58930881bc4: Pushed 7111ee985500: Pushed ... You can use your custom image when upgrading nodes manually, with Kubernetes or specifying it in the cloud-config during installation. Here’s how to do it manually with the kairos-agent command:\nnode:/home/kairos # kairos-agent upgrade --image docker.io/\u003cyour-org\u003e/myos:0.1 INFO[2022-12-01T13:49:41Z] kairos-agent version v0.0.1 INFO[2022-12-01T13:49:42Z] Upgrade called INFO[2022-12-01T13:49:42Z] Applying 'before-upgrade' hook INFO[2022-12-01T13:49:42Z] Running before-upgrade hook INFO[2022-12-01T13:49:42Z] deploying image docker.io/oz123/myos:0.1 to /run/initramfs/cos-state/cOS/transition.img INFO[2022-12-01T13:49:42Z] Creating file system image /run/initramfs/cos-state/cOS/transition.img INFO[2022-12-01T13:49:42Z] Copying docker.io/oz123/myos:0.1 source... INFO[0000] Unpacking a container image: docker.io/oz123/myos:0.1 INFO[0000] Pulling an image from remote repository ... INFO[2022-12-01T13:52:33Z] Finished moving /run/initramfs/cos-state/cOS/transition.img to /run/initramfs/cos-state/cOS/active.img INFO[2022-12-01T13:52:33Z] Upgrade completed INFO[2022-12-01T13:52:33Z] Upgrade completed node:/home/kairos # which figlet which: no figlet in (/sbin:/usr/sbin:/usr/local/sbin:/root/bin:/usr/local/bin:/usr/bin:/bin) node:/home/kairos # reboot Now, reboot your OS and ssh again to it to use figlet:\n$ ssh -l kairos node: Welcome to Kairos! Refer to https://kairos.io for documentation. kairos@node2:~\u003e figlet kairos rocks! _ _ _ _ | | ____ _(_)_ __ ___ ___ _ __ ___ ___| | _____| | | |/ / _` | | '__/ _ \\/ __| | '__/ _ \\ / __| |/ / __| | | \u003c (_| | | | | (_) \\__ \\ | | | (_) | (__| \u003c\\__ \\_| |_|\\_\\__,_|_|_| \\___/|___/ |_| \\___/ \\___|_|\\_\\___(_) Customizing the Kernel Kairos allows you to customize the kernel and initrd as part of your container-based operating system. If you are using a glibc-based distribution, such as OpenSUSE or Ubuntu, you can use the distribution’s package manager to replace the kernel with the one you want, and then rebuild the initramfs with dracut.\nHere’s an example of how to do this:\n# Replace the existing kernel with a new one, depending on the base image it can differ apt-get install -y ... # Create the kernel symlink kernel=$(ls /boot/vmlinuz-* | head -n1) ln -sf \"${kernel#/boot/}\" /boot/vmlinuz # Regenerate the initrd, in openSUSE we could just use \"mkinitrd\" kernel=$(ls /lib/modules | head -n1) dracut -v -f \"/boot/initrd-${kernel}\" \"${kernel}\" ln -sf \"initrd-${kernel}\" /boot/initrd # Update the module dependencies kernel=$(ls /lib/modules | head -n1) depmod -a \"${kernel}\" After you have modified the kernel and initrd, you can use the kairos-agent upgrade command to update your nodes, or within Kubernetes.\nCustomizing the file system hierarchy using custom mounts. Bind mounts For clusters that needs to mount network block storage you might want to add custom mount point that bind mounted to your system. For example, when using Ceph file system, the OS mounts drives to /var/lib/ceph (for example).\nTo achieve this you need to add the key bind_mounts to the install section you pass the install, and specify a list of one or more bind mounts path.\ninstall: auto: true device: \"auto\" # changes persist reboot - mount as BIND bind_mounts: - /var/lib/ceph ... To do this after installation, simply add a cloud config file in the /oem folder, for instance, to make /var/lib/docker persistent:\n#cloud-config stages: rootfs: - name: \"user_custom_mount\" environment_file: \"/run/cos/custom-layout.env\" environment: CUSTOM_BIND_MOUNTS: \"/var/lib/docker\" Ephemeral mounts One can also specifying custom mounts which are ephemeral. These are writable, however changes are discarded at boot (like /etc/ already does).\ninstall: auto: true device: \"auto\" # changes persist reboot - mount as BIND bind_mounts: - /var/lib/ceph ephemeral_mounts: - /opt/scratch/ ... Note, that these paths should exist in the container file-system used to create the ISO. See ISO customization above.\nCustomizing the file system hierarchy using cloud-config. For cases in which there is specific disk or mount needs, we can leverage cloud-config to do very specific things that may not be covered by the custom mount facility that we mention above.\nFor example, if we wanted to mount an extra disk into a specific path in the root that doesn’t exists we could do it with a config like this:\n#cloud-config install: auto: true reboot: true device: /dev/vda stages: after-install-chroot: # Creates the data dir after install inside the final system chroot - \u0026createdatadir name: \"Create data dir\" commands: - mkdir -p /data # Formats the disk ONLY after-install and just once. Extra checks can be added here, so we don't reformat it # This can also go in the after-install stage, but its just important to do it just once - name: \"Format /dev/vdb\" commands: - mkfs.ext4 -F /dev/vdb # Creates the data dir after reset inside the final system chroot, just in case it's not there after-reset-chroot: - \u003c\u003c: *createdatadir # Creates the data dir after upgrade inside the final system chroot, just in case it's not there after-upgrade-chroot: - \u003c\u003c: *createdatadir initramfs: # Mounts the disk under the /data dir during initramfs on each boot, with RW. Extra options can be added to the mount here - name: \"Mount /dev/vdb under /data\" commands: - mount -o rw /dev/vdb /data This would leverage the kairos-agent stages after-install-chroot, after-upgrade-chroot and after-reset-chroot to create a new folder in the rootfs, format the given disk and mount it during the initramfs stage.\nThis works because during the after-install-chroot, after-upgrade-chroot and after-reset-chroot stages we run any commands inside the final system with a chroot AND we have RW access during that time. We could use those same stages to install extra packages for example, but in this case we use it to create an extra path. Remember that once we have installed, the system is inmmutable, so we won’t be able to create any new paths in the root filesystem during runtime, even when using cloud-config. Only ephemeral and persistent paths are RW during runtime.\n","categories":"","description":"Learn how to customize Kairos images to suit your needs","excerpt":"Learn how to customize Kairos images to suit your needs","ref":"/docs/advanced/customizing/","tags":"","title":"Customizing the system image"},{"body":"","categories":"","description":"Discover multiple ways to install Kairos across cloud, bare metal, and virtualized environments.\n","excerpt":"Discover multiple ways to install Kairos across cloud, bare metal, and …","ref":"/docs/installation/","tags":"","title":"Installation"},{"body":"Kairos is a cloud-native Linux meta-distribution for running Kubernetes. It brings the power of the public cloud to your on-premises environment. With Kairos, you can build your own cloud with complete control and no vendor lock-in.\nNote Kairos is a Cloud Native Computing Foundation (CNCF) sandbox project. Here are a few reasons why you should try Kairos:\nBuild your own cloud on-premises with complete control and no vendor lock-in Provision nodes with your own image or use Kairos releases for added flexibility Use Kairos for a wide range of use cases, from Kubernetes applications to appliances and more Simple and streamlined day-2 operations (e.g. node upgrades) What can I do with Kairos? With Kairos, you can easily spin up a Kubernetes cluster with the Linux distribution of your choice, and manage the entire cluster lifecycle with Kubernetes. Try Kairos today and experience the benefits of a unified, cloud-native approach to OS management.\nWith Kairos, you can:\nSpin up a Kubernetes cluster with any Linux distribution in just a few clicks Create an immutable infrastructure that stays consistent and free of drift with atomic upgrades Manage your cluster’s entire lifecycle with Kubernetes, from building to upgrading Automatically create multi-node, single clusters that spans across regions for maximum flexibility and scalability Try Kairos today and experience the benefits of a unified, cloud-native approach to OS management. Say goodbye to the hassle of managing multiple systems, and hello to a more streamlined and efficient way of working.\nFeatures Our key features include are\nImmutability: ensure your infrastructure stays consistent with atomic upgrades Trusted Boot: stay safe by limiting the operating systems allowed to boot on your systems Container-based: manage your nodes as apps in containers for maximum flexibility and portability P2P Mesh: self-coordinated, automated, no interaction Kubernetes deployments with Peer-2-Peer technology Meta-Distribution: choose your preferred Linux distribution. If we don’t have it, you can probably build it yourself! More features\nEasily create multi-node Kubernetes clusters with K3s, and enjoy all of K3s’s features Upgrade manually via CLI or with Kubernetes, and use container registries for distribution upgrades Enjoy the benefits of an immutable distribution that stays configured to your needs Configure nodes with a single cloud-init config file for added simplicity Upgrade even in airgap environments with in-cluster container registries Extend your image at runtime or build time with Kubernetes Native APIs Coming soon: CAPI support with full device lifecycle management and more Create private virtual network segments with a full-mesh P2P hybrid VPN network that can stretch up to 10000 km More than a Linux distribution Kairos is more than just an ISO, qcow2, or Netboot artifact. It allows you to turn any Linux distribution into a uniform and compliant distro with an immutable design. This means that any distro “converted” with Kairos will share the same common feature set and can be managed in the same way using Kubernetes Native API components. Kairos treats all OSes homogeneously and upgrades are distributed via container registries. Installations mediums and other assets required for booting bare metal or edge devices are built dynamically by Kairos’ Kubernetes Native API components.\nGoals The Kairos ultimate goal is to bridge the gap between Cloud and Edge by creating a smooth user experience. There are several areas in the ecosystem that can be improved for edge deployments to make it in pair with the cloud.\nThe Kairos project encompasses all the tools and architectural pieces needed to fill those gaps. This spans between providing Kubernetes Native API components to assemble OSes, deliver upgrades, and control nodes after deployment.\nKairos is distro-agnostic, and embraces openness: The user can provide their own underlying base image, and Kairos onboards it and takes it over to make it Cloud Native, immutable that plugs into an already rich ecosystem by leveraging containers as distribution medium.\nContribute Kairos is an open source project, and any contribution is more than welcome! The project is big and narrows to various degrees of complexity and problem space. Feel free to join our chat, discuss in our forums and join us in the Office hours. Check out the contribution guidelines to see how to get started and our governance.\nWe have an open roadmap, so you can always have a look on what’s going on, and actively contribute to it.\nUseful links:\nUpcoming releases Community You can find us at:\n#Kairos-io at matrix.org IRC #kairos in libera.chat GitHub Discussions Project Office Hours Project Office Hours is an opportunity for attendees to meet the maintainers of the project, learn more about the project, ask questions, learn about new features and upcoming updates.\nOffice hours are happening weekly on Monday - 15:30 – 16:00pm UTC. Meeting link\nAlternatives There are other projects that are similar to Kairos which are great and worth to mention, and actually Kairos took to some degree inspiration from. However, Kairos have different goals and takes completely unique approaches to the underlying system, upgrade, and node lifecycle management.\nk3os Talos FlatCar CoreOS Development Building Kairos Requirements: Docker\nFirst we need to clone the repository\ngit clone https://github.com/kairos-io/kairos.git cd kairos Then we can build an OCI artifact which is the base of all things Kairos. Our ISOs, cloud images, upgrade artifacts are all OCI artifacts and they are managed like a normal OCI artifact. They can be pushed, tagged, labelled, build on top by using it as FROM in a Dockerfile, etc..\nTo build the base artifact we can do:\ndocker build -t myBaseKairos:v1.0.0 --build-arg VERSION=v1.0.0 --build-arg BASE_IMAGE=@baseImage -f images/Dockerfile . Then we can build the ISO with the following command for a Kairos core image based on @flavor:\ndocker run --rm -v /var/run/docker.sock:/var/run/docker.sock -v $PWD/build/:/output \\ quay.io/kairos/auroraboot:v0.5.0 build-iso --output /output/ docker:myBaseKairos:v1.0.0 That will generate an ISO file under the build dir that you can use.\nWhat’s next? Different ways to install Kairos Upgrading ","categories":"","description":"Discover Kairos’ core features, guiding principles, and how to get involved with the project.\n","excerpt":"Discover Kairos’ core features, guiding principles, and how to get …","ref":"/getting-started/what-is-kairos/","tags":"","title":"What is Kairos?"},{"body":"This page describes how to install Kairos on Microsoft Azure after you have created a disk image. Since release v3.3.5, Kairos pipeline is pushing a public OS image to Azure which you can use. If you want to build a custom image, you can follow the instructions in the Creating Custom Cloud Images page.\nPrerequisites An Azure account with permissions to create VMs. An Azure compatible image of Kairos. You can use the public image provided by Kairos (see below) or build your own image (for Azure, that would be the .vhd format) and upload it to your Azure resource group (check how the Kairos CI does it). Deploy a VM Visit the Community Images page in the Azure portal and search for “kairos” in the search box. Multiple results will be returned matching multiple different regions. Click on the result matching the region you intend to use for the Virtual Machine. Find the version you want to use and click on “Create VM”.\nNote To ensure you’re using a genuine Kairos image in Azure, you should ensure that the image belongs to the Azure compute gallery kairos.io. You can inspect the image in the Web UI or you can use the Azure CLI:\naz sig image-version show --gallery-name kairos.io --gallery-image-definition kairos --resource-group kairos-cloud-images --gallery-image-version \u003cKAIROS_IMAGE_VERSION\u003e --query 'name' Replace \u003cKAIROS_IMAGE_VERSION\u003e with the version of the image (e.g. 3.3.5).\nNote As described below, it is possible to reset to any desired image on first boot. That’s the reason only one Kairos flavor is published in Google Cloud (Ubuntu 24.04). This allows us to save costs and time by not pushing unnecessary artifacts. There are multiple options to select for your VM. We suggest you choose a size that has:\n16Gb of RAM or more 2 CPUs or more An OS disk size of 30Gb or more (3 times the image size + some more for persistent storage). Don’t go with the “Image default” because the disk needs to be large enough to accommodate for active, passive and recovery system. Public inbound port for SSH (if you intend to SSH to the machine later). Under the “Advanced” tab, click on “Enable user data”. In the field that is opened, you can put your Kairos configuration.\nWhen the instance boots for the first time, it will boot into “auto-reset mode” by default. This means, that Kairos will “install” itself on the first boot and then reboot. You can specify a different image to be installed using a block like the following in the cloud config section:\nreset: source: \"oci:quay.io/kairos/opensuse:leap-15.6-standard-amd64-generic-master-k3sv1.33.4-k3s1\" Make sure you also\nWhen you are satisfied with the configuration, click on “Review + create” and then “Create”.\nAfter the VM has been created, you can either SSH to it using the credentials you specified in the Kairos config or using the serial console in the Azure Web UI.\nOn first boot, Kairos will reset to the specified image (or the booted image if none was specified for reset). Make sure reset has completed and the system has rebooted into “active” mode. The following command should report “active_boot”:\nkairos-agent state get boot (It it reports recovery_boot, the system is still in the installation process. Wait a few minutes and try again.)\n","categories":"","description":"Install Kairos on Microsoft Azure","excerpt":"Install Kairos on Microsoft Azure","ref":"/docs/installation/azure/","tags":"","title":"Installation on Microsoft Azure"},{"body":"This page describes how to install Kairos on Google Cloud after you have created a disk image. Since release v3.3.1, Kairos pipeline is pushing a public OS image to Google Cloud which you can use. If you want to build a custom image, you can follow the instructions in the Creating Custom Cloud Images page.\nPrerequisites A Google Cloud account with permissions to create VMs. A Google Cloud compatible image of Kairos. You can use the public image provided by Kairos (see below) or build your own image and upload it to your google project (check how the Kairos CI does it). Deploy a VM Unfortunately Google Cloud doesn’t allow users to search among public images in different projects. One has to know the exact project and name of the image they intend to use, even if its publicly accessible to all authenticated users. Using the public image seems to only be possible through the command line because of the above.\nMake sure you are authenticated with the cli: gcloud auth login Create a VM using the latest Kairos image: Note As described below, it is possible to reset to any desired image on first boot. That’s the reason only one Kairos flavor is published in Google Cloud (Ubuntu 24.04). This allows us to save costs and time by not pushing unnecessary artifacts. Verify the Image To ensure you’re using a genuine Kairos image in Google Cloud, make sure the image you are going to use is in the Kairos team’s Google Cloud project (palette-kairos).\nYou can verify the image using the Google Cloud CLI:\ngcloud compute images describe \u003cIMAGE_NAME\u003e --project palette-kairos --format=\"table(name,description,status)\" Replace \u003cIMAGE_NAME\u003e with the name of the image. The output will show you the name, description, and status of the image. If the image doesn’t belong to the Kairos project, no image will be found.\nNote As described below, it is possible to reset to any desired image on first boot. That’s the reason only one Kairos flavor is published in Google Cloud (Ubuntu 24.04). This allows us to save costs and time by not pushing unnecessary artifacts. gcloud --project \u003cyour_project_here\u003e compute instances create kairos-vm-test \\ --image=projects/palette-kairos/global/images/kairos-ubuntu-24-04-core-amd64-generic-. \\ --image-project=palette-kairos \\ --zone=europe-central2-c \\ --metadata-from-file=user-data=\u003cpath_to_your_cloud_config\u003e \\ --boot-disk-size=40G Connect to the instance:\ngcloud compute connect-to-serial-port kairos-vm-test (disconnect with \u003cEnter\u003e~.)\nBy passing a file to --metadata-from-file=user-data=\u003cyour_file_here\u003e you can pass a cloud config to Kairos. You should at least specify a user and a password (or SSH key) if you need to SSH to the instance (Check the Getting started page for some examples).\nWhen the instance boots for the first time, it boots into “auto-reset mode” by default. This means, that Kairos will “install” itself on the first boot and then reboot. You can specify a different image to be installed using a block like the following in the cloud config:\nreset: source: \"oci:quay.io/kairos/opensuse:leap-15.6-standard-amd64-generic-master-k3sv1.33.4-k3s1\" This will reset to the specified image on the first boot instead of the image booted. Once the instance is running, you can access it via SSH. Make sure reset has completed and the system has rebooted into “active” mode. The following command should report “active_boot”:\nkairos-agent state get boot (It it reports recovery_boot, the system is still in the installation process. Wait a few minutes and try again.)\n","categories":"","description":"Install Kairos on Google Cloud","excerpt":"Install Kairos on Google Cloud","ref":"/docs/installation/gce/","tags":"","title":"Installation on Google Cloud"},{"body":"For each stage in the cloud-init file, various modules are available that implement different functionality each. This page describes what each one does and how to use it.\nThe order in this document is also the order in which they are executed.\ndns A way to configure the /etc/resolv.conf file.\n#cloud-config stages: boot: - name: \"Setup dns\" dns: nameservers: - 8.8.8.8 - 1.1.1.1 search: - foo.bar options: - .. path: \"/etc/resolv.conf.bak\" downloads Download files to specified locations\n#cloud-config stages: boot: - downloads: - path: /tmp/out url: \"https://www....\" permissions: 0700 owner: 0 group: 0 timeout: 0 owner_string: \"root\" - path: /tmp/out url: \"https://www....\" permissions: 0700 owner: 0 group: 0 timeout: 0 owner_string: \"root\" git Pull git repositories, using golang native git (no need of git in the host).\n#cloud-config stages: boot: - git: url: \"git@gitlab.com:.....git\" path: \"/oem/cloud-config-files\" branch: \"main\" auth: insecure: true private_key: | -----BEGIN RSA PRIVATE KEY----- -----END RSA PRIVATE KEY----- ensure_entities A user or a group in the entity format to be configured in the system\n#cloud-config stages: boot: - name: \"Setup users\" ensure_entities: - path: /etc/passwd entity: | kind: \"user\" username: \"foo\" password: \"x\" uid: 0 gid: 0 info: \"Foo!\" homedir: \"/home/foo\" shell: \"/bin/bash\" directories A list of directories to be created on disk. Runs before files.\n#cloud-config stages: boot: - name: \"Setup folders\" directories: - path: \"/etc/foo\" permissions: 0600 owner: 0 group: 0 files A list of files to write to disk.\n#cloud-config stages: boot: - files: - path: /tmp/bar encoding: \"b64\" # \"base64\", \"gz\", \"gzip\", \"gz+base64\", \"gzip+base64\", \"gz+b64\", \"gzip+b64\" content: IyEvYmluL3NoCgplY2hvICJ0ZXN0Igo= permissions: 0777 owner: 1000 group: 100 # or # owner_string: \"user:group\", or \"user\" commands A list of arbitrary commands to run after file writes and directory creation.\n#cloud-config stages: boot: - name: \"Setup something\" commands: - echo 1 \u003e /bar delete_entities A user or a group in the entity format to be pruned from the system\n#cloud-config stages: boot: - name: \"Setup users\" delete_entities: - path: /etc/passwd entity: | kind: \"user\" username: \"foo\" password: \"x\" uid: 0 gid: 0 info: \"Foo!\" homedir: \"/home/foo\" shell: \"/bin/bash\" hostname A string representing the machine hostname. It sets it in the running system, updates /etc/hostname and adds the new hostname to /etc/hosts. Templates can be used to allow dynamic configuration. For example in mass-install scenario it could be needed (and easier) to specify hostnames for multiple machines from a single cloud-init config file.\n#cloud-config stages: boot: - name: \"Setup hostname\" hostname: \"node-{{ trunc 4 .MachineID }}\" sysctl Kernel configuration. It sets /proc/sys/\u003ckey\u003e accordingly, similarly to sysctl.\n#cloud-config stages: boot: - name: \"Setup exception trace\" systctl: debug.exception-trace: \"0\" users A map of users and user info to set. Passwords can also be encrypted.\nThe users parameter adds or modifies the specified list of users. Each user is an object which consists of the following fields. Each field is optional and of type string unless otherwise noted. In case the user already exists, only the password and ssh-authorized-keys are evaluated. The rest of the fields are ignored.\nname: Required. Login name of user gecos: GECOS comment of user passwd: Hash of the password to use for this user. Unencrypted strings are supported too. homedir: User’s home directory. Defaults to /home/name no-create-home: Boolean. Skip home directory creation. primary-group: Default group for the user. Defaults to a new group created named after the user. groups: Add user to these additional groups. Kairos creates an admin group by default which is also added to the sudoers file. Add a user to the admin group if you want them to have sudo access. no-user-group: Boolean. Skip default group creation. ssh-authorized-keys: List of public SSH keys to authorize for this user system: Create the user as a system user. No home directory will be created. no-log-init: Boolean. Skip initialization of lastlog and faillog databases. shell: User’s login shell. #cloud-config stages: boot: - name: \"Setup users\" users: bastion: passwd: \"strongpassword\" homedir: \"/home/foo authorized_keys A list of SSH authorized keys that should be added for each user. SSH keys can be obtained from GitHub user accounts by using the format github:${USERNAME}, similarly for GitLab with gitlab:${USERNAME}.\n#cloud-config stages: boot: - name: \"Setup exception trace\" authorized_keys: mudler: - \"github:mudler\" - \"ssh-rsa: ...\" modules A list of kernel modules to load.\n#cloud-config stages: boot: - name: \"Setup users\" modules: - nvidia timesyncd Sets the systemd-timesyncd daemon file (/etc/system/timesyncd.conf) file accordingly. The documentation for timesyncd and all the options can be found here.\n#cloud-config stages: boot: - name: \"Setup NTP\" systemctl: enable: - systemd-timesyncd timesyncd: NTP: \"0.pool.org foo.pool.org\" FallbackNTP: \"us.pool.ntp.org\" - name: \"Restart NTP service so it gets the new config\" commands: - systemctl restart systemd-timesyncd systemctl A list of systemd services to enable, disable, mask or start.\n#cloud-config stages: boot: - name: \"Setup users\" systemctl: enable: - systemd-timesyncd - cronie mask: - purge-kernels disable: - crond start: - cronie environment_file A string to specify where to set the environment file\n#cloud-config stages: boot: - name: \"Setup users\" environment_file: \"/home/user/.envrc\" environment: FOO: \"bar\" environment A map of variables to write in /etc/environment, or otherwise specified in environment_file\n#cloud-config stages: boot: - name: \"Setup users\" environment: FOO: \"bar\" systemd_firstboot Runs systemd-firstboot with the arguments specified.\n#cloud-config debug: true stages: boot: - name: \"Run systemd-firstboot\" systemd_firstboot: hostname: \"myhostname\" datasource Sets to fetch user data from the specified cloud providers. It populates provider specific data into /run/config folder and the custom user data is stored into the provided path.\n#cloud-config stages: boot: - name: \"Fetch cloud provider's user data\" datasource: providers: - \"aws\" - \"digitalocean\" path: \"/etc/cloud-data\" layout Sets additional partitions on disk free space, if any, and/or expands the last partition. All sizes are expressed in MiB only and default value of size: 0 means all available free space in disk. This plugin is useful to be used in oem images where the default partitions might not suit the actual disk geometry.\n#cloud-config stages: boot: - name: \"Repart disk\" layout: device: # It will partition a device including the given filesystem label # or partition label (filesystem label matches first) or the device # provided in 'path'. The label check has precedence over path when # both are provided. label: \"COS_RECOVERY\" path: \"/dev/sda\" # Only last partition can be expanded and it happens after all the other # partitions are created. size: 0 means all available free space expand_partition: size: 4096 add_partitions: - fsLabel: \"COS_STATE\" size: 8192 # No partition label is applied if omitted pLabel: \"state\" - fsLabel: \"COS_PERSISTENT\" # default filesystem is ext2 if omitted filesystem: \"ext4\" You can also set custom partitions within the kairos-install.pre.before stage. In the following example we will do a custom partition in disk /dev/vda.\nWarning You’re responsible to make sure the sizes of the partitions fit properly within the disk. Issues of space will be highlighted by the agent, but they will not fail the installation process unless you pass the --strict flag. Warning In the case of multiple devices, make sure you don’t choose auto to determine on which device to install but instead to point the installation to the device where you are creating the custom partitions. #cloud-config install: # Make sure the installer won't delete our custom partitions no-format: true stages: kairos-install.pre.before: - if: '[ -e /dev/vda ]' name: \"Create partitions\" commands: - | parted --script --machine -- /dev/vda mklabel msdos layout: device: path: /dev/vda expand_partition: size: 0 # All available space add_partitions: # all sizes bellow are in MB - fsLabel: COS_OEM size: 64 pLabel: oem - fsLabel: COS_RECOVERY size: 8500 pLabel: recovery - fsLabel: COS_STATE size: 18000 pLabel: state - fsLabel: COS_PERSISTENT pLabel: persistent size: 25000 filesystem: \"ext4\" ","categories":"","description":"Explore built-in modules for DNS, users, files, and services that help you customize Kairos via cloud-init during boot stages.","excerpt":"Explore built-in modules for DNS, users, files, and services that help …","ref":"/docs/reference/stage_modules/","tags":"","title":"Stage modules"},{"body":"SecureBoot support implementation Currently Kairos supports SecureBoot based on the upstream artifacts. We piggyback on the upstream artifacts to be properly signed in order to support SecureBoot.\nBefore this was supported, we shipped a single set of artifacts that were signed by one of the upstream distros. That meant that only that distro was supported under SecureBoot as we needed all artifacts in the chain to be signed with the same key. So that meant that secureboot was only supported on the same distro that we obtained the artifacts from.\nShim (signed by X) -\u003e grub (signed by X) -\u003e kernel (signed by X) = SecureBoot Shim (signed by X) -\u003e grub (signed by X) -\u003e kernel (signed by Y) = No SecureBoot Now, instead of using a single set of artifacts, we use the upstream artifacts directly, so we can support SecureBoot on all the distros that support it directly with the upstream signatures. This also allows us to pinpoint specific bugs of a given artifact with the upstream report.\nAny of the current supported flavors in Kairos can be used with SecureBoot out of the box in UEFI mode.\nCurrently Alpine is not supported as it does not provide signed artifacts by default and relies in users generating their own keys to sign those.\nArtifacts used on each distro shim grub kernel ","categories":"","description":"Learn how Kairos supports Secure Boot using signed artifacts to ensure system integrity across distributions.","excerpt":"Learn how Kairos supports Secure Boot using signed artifacts to ensure …","ref":"/docs/architecture/secureboot/","tags":"","title":"SecureBoot support"},{"body":"To automate Kairos installation, you can configure a specific portion of the installation configuration file. The configuration file can then be supplied in a few different ways, such as creating an additional ISO to mount, specifying a URL, or even creating an ISO from a container image with an embedded configuration file.\nHere’s an example of how you might customize the install block:\ninstall: # Device for automated installs device: \"/dev/sda\" # Reboot after installation reboot: true # Power off after installation poweroff: true # Set to true to enable automated installations auto: true # A list of bundles bundles: - quay.io/kairos/packages:k9s-utils-0.26.7 This block allows you to specify the device on which to install Kairos, whether to reboot or power off after installation, and which bundles to include.\nData source To supply your Kairos configuration file, you can create an ISO that contains both a user-data file (which contains your configuration) and a meta-data file.\nHere’s an example user-data configuration that is set up to automatically install Kairos onto /dev/sda and reboot after installation:\n#cloud-config install: device: \"/dev/sda\" reboot: true poweroff: false auto: true # Required, for automated installations kairos: network_token: .... # extra configuration The token p2p.network_token is a base64 encoded string which contains an edgevpn token. For more information, check out the architecture section.\nSave this file as cloud_init.yaml, then create an ISO with the following steps:\nCreate a new directory and navigate to it: $ mkdir -p build $ cd build Create empty meta-data and copy your config as user-data: $ touch meta-data $ cp -rfv cloud_init.yaml user-data Use mkisofs to create the ISO file: $ mkisofs -output ci.iso -volid cidata -joliet -rock user-data meta-data Once the ISO is created, you can attach it to your machine and boot up as usual, along with the Kairos ISO.\nWarning For security reasons, when Kairos is installed in trusted boot mode, datasources are not parsed after installation. This prevents someone from plugging a usb stick on an edge device, applying arbitrary configuration to the system post-installation. To force parsing of the datasources after installation, you can set add the kairos.pull_datasources option to the cmdline. This requires extending the cmdline when building the installation medium with AuroraBoot (read more).\nThis security feature is only enabled when the system boots in trusted boot mode and only after installation (they are parsed in “live” mode). On “plain” boot mode, datasources are always parsed.\nVia config URL Another way to supply your Kairos configuration file is to specify a URL as a boot argument during startup. To do this, add config_url=\u003cURL\u003e as a boot argument. This will allow the machine to download your configuration from the specified URL and perform the installation using the provided settings.\nAfter installation, the configuration will be available on the system at /oem/90_custom.yaml.\nIf you’re not sure where to host your configuration file, a common option is to upload it as a GitHub gist.\nISO remastering It is possible to create custom ISOs with an embedded cloud configuration. This allows the machine to automatically boot with a pre-specified configuration file, which will be installed on the system after provisioning is complete. See also AuroraBoot for documentation.\nLocally To create a custom ISO, you will need Docker installed on your machine.\nHere’s an example of how you might do this:\nWarning The image passed to the auroraboot image, needs to have one of the accepted schemes: docker, oci, file, dir or channel.\nIf you don’t pass one, we will make an attempt to read it as a web URL but depending on your URL this might throw an error.\nAuroraBoot Manually We can use AuroraBoot to handle the the ISO build process, for example:\n$ IMAGE=\u003cscheme://host[:port]/path[:tag]\u003e $ docker pull $IMAGE # Build the ISO $ docker run -v $PWD/cloud_init.yaml:/cloud_init.yaml \\ -v $PWD/build:/tmp/auroraboot \\ -v /var/run/docker.sock:/var/run/docker.sock \\ --rm -ti quay.io/kairos/auroraboot \\ --set container_image=docker://$IMAGE \\ --set \"disable_http_server=true\" \\ --set \"disable_netboot=true\" \\ --cloud-config /cloud_init.yaml \\ --set \"state_dir=/tmp/auroraboot\" # Artifacts are under build/ $ sudo ls -liah build/iso total 778M 34648528 drwx------ 2 root root 4.0K Feb 8 16:39 . 34648526 drwxr-xr-x 5 root root 4.0K Feb 8 16:38 .. 34648529 -rw-r--r-- 1 root root 253 Feb 8 16:38 config.yaml 34649370 -rw-r--r-- 1 root root 389M Feb 8 16:38 kairos.iso 34649371 -rw-r--r-- 1 root root 76 Feb 8 16:39 kairos.iso.sha256 $ IMAGE=\u003cscheme://host[:port]/path[:tag]\u003e $ mkdir -p files-iso/boot/grub2 # You can replace this step with your own grub config. This GRUB configuration is the boot menu of the ISO $ wget https://raw.githubusercontent.com/kairos-io/packages/main/packages/livecd/grub2/config/grub_live_bios.cfg -O files-iso/boot/grub2/grub.cfg # Copy the config file $ cp -rfv cloud_init.yaml files-iso/cloud_config.yaml # Pull the image locally $ docker pull $IMAGE # Optionally, modify the image here! # docker run --entrypoint /bin/bash --name changes -ti $IMAGE # docker commit changes $IMAGE # Build an ISO with $IMAGE $ docker run -v $PWD:/cOS -v /var/run/docker.sock:/var/run/docker.sock -i --rm quay.io/kairos/auroraboot:v0.13.0 --debug build-iso --name \"custom-iso\" --date=false --overlay-iso /cOS/files-iso --output /cOS/ $IMAGE Cloud config In the case of Auroraboot, make sure that the cloud config that you are mounting in the container (-v $PWD/cloud_init.yaml:/cloud_init.yaml) exists. Otherwise docker will create an empty directory to mount it on the container without any warnings and you will end up with an empty cloud config. This will create a new ISO with your specified cloud configuration embedded in it. You can then use this ISO to boot your machine and automatically install Kairos with your desired settings.\nYou can as well modify the image in this step and add additional packages before deployment. See customizing the system image.\nCheck out the AuroraBoot documentation and the examples for learn more on how to generate customized images for installation.\nKubernetes It is possible to create custom ISOs and derivatives using extended Kubernetes API resources with an embedded configuration file. This allows you to drive automated installations and customize the container image without breaking the concept of immutability.\nYou can read more about it here.\n","categories":"","description":"Install Kairos automatically, with zero touch provisioning","excerpt":"Install Kairos automatically, with zero touch provisioning","ref":"/docs/installation/automated/","tags":"","title":"Automated"},{"body":"Kairos supports the standard cloud-init syntax and its own extended syntax to allow to configure a system declaratively with a cloud-config centric approach.\nIf you are not familiar with the concepts of cloud-init, official cloud-init is a recommended read.\nConfiguration persistency Kairos is an Immutable OS and the only configuration that is persistent across reboots is the cloud-init configuration. Multiple cloud-init files can be present in the system and Kairos will read them and process them in sequence (lexicographic order) allowing to extend the configuration with additional pieces also after deployment, or to manage logical configuration pieces separately.\nIn Kairos the /oem directory keeps track of all the configuration of the system and stores the configuration files. Multiple files are allowed and they are all executed during the various system stages. /usr/local/cloud-config can be optionally used as well to store cloud config files in the persistent partition instead. /system/oem is instead reserved to default cloud-init files that are shipped by the base OS image.\nBy using the standard cloud-config syntax, a subset of the functionalities are available and the settings will be executed in the boot stage.\nConfiguration order When an action is done (install, upgrade, reset) several default directories in the system are read to obtain the configuration and are merged together. The directories and the order in which they are read and merged, is as shown below, from first to last. Notice that any values found in different directories will override existing ones in previous directories.\n/run/initramfs/live (Only available on LiveCD/Netboot) /usr/local/cloud-config /etc/kairos /etc/elemental (deprecated) /oem This means that you could ship an ISO with a bundled config (see Automated install or Auroraboot to see how) that adds a generic configuration that you want everywhere, and using userdata you can then overwrite the default config if needed per node/datacenter/deployment, as the useradata is read and stored into /oem it will be read later in the process and overwrite whatever you shipped on the defaults bundled with the ISO.\nIn order to see the final config, you can run on a running system kairos-agent config and that should show the final configuration after scanning all sources.\nNOTE: Other than configuration (for installation/upgrade/reset/etc), in the cloud config files, you can also define “stages” to be run during boot. When the same stage is defined in more than one cloud config files, all definitions will be respected. In other words, stages won’t be overwritten.\nBoot stages During boot the stages are emitted in an event-based pattern until a system completes its boot process\nThe events can be used in the cloud-config extended syntax to hook into the various stages, which can allow to hook inside the different stages of a node lifecycle.\nFor instance, to execute something before reset is sufficient to add the following to the config file used to bootstrap a node:\nname: \"Run something before reset\" stages: before-reset: - name: \"Setting\" commands: - | echo \"Run a command before reset the node!\" Below there is a detailed list of the stages available that can be used in the cloud-init configuration files:\nStage Description rootfs This is the earliest stage, running before switching root, just right after the root is mounted in /sysroot and before applying the immutable rootfs configuration. This stage is executed over initrd root, no chroot is applied. initramfs This is still an early stage, running before switching root. Here you can apply radical changes to the booting setup of Kairos. Despite this is executed before switching root this execution runs chrooted into the target root after the immutable rootfs is set up and ready. boot This stage is executed after initramfs has switched root, during the systemd bootup process. fs This stage is executed when fs is mounted and is guaranteed to have access to the state and persistent partitions ( COS_STATE and COS_PERSISTENT respectively). network This stage is executed when network is available reconcile This stage is executed 5m after boot and periodically each 60m. kairos-install.pre This stage is executed before installation of the OS starts kairos-uki-install.pre This stage is executed before installation of the OS starts. Only run under Trusted Boot kairos-install.after This stage is executed after installation of the OS ends kairos-uki-install.after This stage is executed after installation of the OS ends. Only run under Trusted Boot kairos-uki-reset.pre This stage is executed before reset. Only run under Trusted Boot kairos-uki-reset.after This stage is executed after reset. Only run under Trusted Boot kairos-uki-upgrade.pre This stage is executed before upgrade. Only run under Trusted Boot kairos-uki-upgrade.after This stage is executed after upgrade. Only run under Trusted Boot before-install This stage happens after partitioning but before the image OS is applied after-install-chroot This stage happens after installing active and grub inside chroot1 after-install This stage runs after active,passive and recovery images are installed and after disks have been encrypted before-reset This stage happens after partitions have been formatted and mounted but before the image has been reset after-reset This stage happens after partitions have been formatted and mounted and active and passive images reset after-reset-chroot This stage happens after active has been reset but before passive has been touched inside chroot1 before-upgrade This stage happens after mounting partitions with RW but before any image has been upgraded after-upgrade This stage happens after upgrade has been done after-upgrade-chroot This stage happens after the image has been upgraded inside chroot1 In case you’re using a standard image, with the Kairos provider, then these other stages are also available\nStage Description provider-kairos.bootstrap.before. The provider fires this stage before starting to bootstrap K3S. provider-kairos.bootstrap.after. The provider fires this stage after it finished bootstrapping K3S. System stages with after and before substages The system run stages that are not part of an action (install,upgrade,reset) all have sub-stages so users can override or modify system behaviour.\nThis applies to rootfs, initramfs, boot, fs, reconcile and network stages. All of those stages will also run a suffixed after and before substage that users can hook into to change different setting before the main stage is run.\nAs those stages are run as part of the system os during different phases and some default configs are shipped with a Kairos system, we add those stages on the fly so they are easily overridable or reverted if one would not want something that ships with KAiros.\nFor example if we detect that we are running on a VM, we try to enable the helper services that VM vendors provide but that may conflict with a user approach of having no superfluous services running. As that config is shipped as part of the base image, its not easy to remove it unless you build a new artifact.\nInstead we can revert that by having a config that disables it as soon as possible.\nWe know that the stage is run during boot stage as shown in the config file so we could write the following config:\nname: \"Disable QEMU tools\" stages: boot.after: - name: \"Disable QEMU\" if: | grep -iE \"qemu|kvm|Virtual Machine\" /sys/class/dmi/id/product_name \u0026\u0026 \\ ( [ -e \"/sbin/systemctl\" ] || [ -e \"/usr/bin/systemctl\" ] || [ -e \"/usr/sbin/systemctl\" ] || [ -e \"/usr/bin/systemctl\" ] ) commands: - systemctl stop qemu-guest-agent Notice how we set the stage to be boot.after. That will run immediately after the boot stage has run, so we dont have to know where it will run and play with trying to disable it in the same stage and run into race problems, we can just use that substage to make sure that our configs runs after the default system ones.\nAll the mentioned stages (rootfs, initramfs, boot, fs, reconcile and network) have STAGE.before and STAGE.after substages.\nStages during kairos-agent operations in detail Modules For each stage, a number of modules are available, that implement various useful functions. Read more about them in this page: Stage modules\nSentinels When a Kairos boots it creates sentinel files in order to allow to execute cloud-init steps programmaticaly.\n/run/cos/recovery_mode is being created when booting from the recovery partition /run/cos/live_mode is created when booting from the LiveCD To execute a block using the sentinel files you can specify: if: '[ -f \"/run/cos/...\" ]', for instance:\nDefault Kairos configs We have a set of default cloud-init configs that are shipped with the base image, and can be found in /system/oem/. These configs are executed during the various boot stages and can be overridden by user-provided configs in /oem or /usr/local/cloud-config.\nYou can check the default configs in the kairos-init repository.\nOverriding default configs You can override the default configs by creating a new file in /oem and adding your custom configuration. For example, if you want to override the default sysctl configuration provided by Kairos on the boot stage you would create a file under /oem with the following content:\nname: \"Override\" stages: boot: - name: \"modify sysctl settings\" sysctl: fs.inotify.max_user_instances: 123192 Now, we need to make sure that the file is run after the default sysctl configuration is applied, so we can use the boot.after stage to ensure that our custom configuration is applied after the default one:\nname: \"Override\" stages: boot.after: - name: \"modify sysctl settings\" sysctl: fs.inotify.max_user_instances: 123192 This would ensure that our custom sysctl setting is applied after the default sysctl configuration provided by Kairos.\nWhat about overriding stages that are in the same stage? Or making sure they run after our wanted step\nFor example, you have a default stage that runs in the boot.after stage, and you want to override it with your own configuration.\nIn this case, running them would not guarantee that your configuration is applied after the default one, as both configurations are in the same stage.\nFor this case yip provides an after directive that allows you to run your configuration after the default one, even if they are in the same stage.\nname: \"Override\" stages: boot: - name: \"modify sysctl settings\" sysctl: fs.inotify.max_user_instances: 123192 after: - name: \"FULL NAME OF THE STAGE TO OVERRIDE\" This would ensure that your custom sysctl setting is applied after the default sysctl configuration provided by Kairos, even if they are in the same stage and step, yip will move it into a different layer to assure that it is run after.\nFor a practical example:\n/oem/01_first.yaml\nstages: test: - name: \"First stage\" commands: - echo \"Hello\" /oem/02_second.yaml\nstages: test: - name: \"after stage\" after: - name: \"/oem/01_first.yaml.First stage\" commands: - echo \"Hello\" Notice the after directive in the second file, that allows running the command after the first stage has been executed, even if they are in the same stage. And fully guarantees that the second stage will run after the first one, even if they are in the same stage and layer.\nThe name of the stage to override is the full name of the stage, which is a combination of the file path and the stage name, in this case /oem/01_first.yaml.First stage.\nIf you are not sure of the name of the stage to override, you can run kairos-agent run-stage -a STAGE to see the final DAG of the stage, which will include the full name of the stage, and you can use that to override it. Notice that if the yaml file has a root name, that will be used instead of the file name.\nHere is a real example of the output of kairos-agent run-stage -a boot:\n1. \u003cinit\u003e (background: false) (weak: false) 2. \u003cStart agent.0\u003e (background: false) (weak: true) \u003cStart recovery on tty1.Recovery\u003e (background: false) (weak: true) \u003cStart installer on tty1..0\u003e (background: false) (weak: true) \u003cDefault config.Default sysctl settings\u003e (background: false) (weak: true) \u003cEnable QEMU tools.Enable QEMU.0\u003e (background: false) (weak: true) 3. \u003cEnable QEMU tools.Enable QEMU.1\u003e (background: false) (weak: true) \u003cStart installer on tty1..1\u003e (background: false) (weak: true) 4. \u003cEnable QEMU tools.Enable VBOX.2\u003e (background: false) (weak: true) 5. \u003cEnable QEMU tools.Enable VBOX.3\u003e (background: false) (weak: true) 2025-07-04T07:18:12Z DBG [2994] Generating op for stage '/oem/91_sysctl.yaml.first step' 2025-07-04T07:18:12Z DBG [2994] Generating op for stage '/oem/92_another.yaml.0' 1. \u003cinit\u003e (background: false) (weak: false) 2. \u003c/oem/91_sysctl.yaml.first step\u003e (background: false) (weak: true) 3. \u003c/oem/92_another.yaml.0\u003e (background: false) (weak: true) Start agent.0 -\u003e The file has a root name Start agent and the step doesn’t have a set name so it gets the step number name: \"Start agent\" stages: boot: - if: '[ ! -f \"/run/cos/recovery_mode\" ]' only_service_manager: \"systemd\" files: - path: /etc/systemd/system/kairos-agent.service ... Start recovery on tty1.Recovery -\u003e The file has a root name Start recovery on tty1 and the step has a name Recovery name: \"Start recovery on tty1\" stages: boot: - name: \"Recovery\" if: '[ -f \"/run/cos/recovery_mode\" ]' hostname: \"cos-recovery\" commands: ... /oem/91_sysctl.yaml.first step -\u003e The file has no root name and the step has a name first step, so it gets the file name plus step name. stages: boot: - name: first step /oem/92_another.yaml.0 -\u003e The file has no root name and the step has no name, so it gets the file name plus step number. stages: boot: - commands: Steps executed at the chroot stage are running inside the new OS as chroot, allowing to write persisting changes to the image, for example by downloading and installing additional software. ↩︎ ↩︎ ↩︎\n","categories":"","description":"","excerpt":"Kairos supports the standard cloud-init syntax and its own extended …","ref":"/docs/architecture/cloud-init/","tags":"","title":"Cloud init based"},{"body":"The kairosctl binary is provided as part of releases associated to each Kairos version. It can be used from an external machine to generate network tokens and pair nodes on first-boot.\ncurl -L https://github.com/kairos-io/provider-kairos/releases/download/v2.13.4/kairosctl-.v2.13.4-.linux-.amd64.tar.gz -o - | tar -xvzf - -C . # optionally, install the CLI locally mv kairosctl /usr/local/bin/kairosctl chmod +x /usr/local/bin/kairosctl ./kairosctl --help NAME: kairosctl - A new cli application USAGE: kairosctl [global options] command [command options] [arguments...] VERSION: 0.0.0 AUTHOR: Ettore Di Giacinto COMMANDS: register Registers and bootstraps a node bridge Connect to a kairos VPN network get-kubeconfig Return a deployment kubeconfig role Set or list node roles help, h Shows a list of commands or help for one command GLOBAL OPTIONS: --help, -h show help --version, -v print the version COPYRIGHT: Ettore Di Giacinto create-config Generates a new Kairos configuration file which can be used as cloud-init, with a new unique EdgeVPN network token:\n$ ./kairosctl create-config kairos: network_token: b3RwOgogIGRodDoKICAgIGludGVydmFsOiA5MjIzMzcyMDM2ODU0Nzc1ODA3CiAgICBrZXk6IEVCMzJJMlNXTjJCNFBHNEtCWTNBUVBBS0FWRTY0Q0VLVUlDTktTUFVWVU5BWTM0QklEQ0EKICAgIGxlbmd0aDogMzIKICBjcnlwdG86CiAgICBpbnRlcnZhbDogOTIyMzM3MjAzNjg1NDc3NTgwNwogICAga2V5OiBDMk1RRk5DWEFVRElPWjVHM1pZUUIzVEVHTzVXVEdQR1pZSEVQQkY3SFEyVUROUlZCTkxRCiAgICBsZW5ndGg6IDMyCnJvb206IGp6Q29kQVVOWUZSUklQU3JISmx4d1BVUnVxTGJQQnh4CnJlbmRlenZvdXM6IG5NckRCbllyVVBMdnFPV0Z2dWZvTktXek1adEJIRmpzCm1kbnM6IGpQUUhIbVZza2x6V29xbWNkeVlnbVhMSVFjTE1HUFN6Cm1heF9tZXNzYWdlX3NpemU6IDIwOTcxNTIwCg== offline: false reboot: false device: \"\" poweroff: false Now you can use this in your configuration file to create new Kairos nodes:\nkairos: network_token: b3RwOgogIGRodDoKICAgIGludGVydmFsOiA5MjIzMzcyMDM2ODU0Nzc1ODA3CiAgICBrZXk6IEVCMzJJMlNXTjJCNFBHNEtCWTNBUVBBS0FWRTY0Q0VLVUlDTktTUFVWVU5BWTM0QklEQ0EKICAgIGxlbmd0aDogMzIKICBjcnlwdG86CiAgICBpbnRlcnZhbDogOTIyMzM3MjAzNjg1NDc3NTgwNwogICAga2V5OiBDMk1RRk5DWEFVRElPWjVHM1pZUUIzVEVHTzVXVEdQR1pZSEVQQkY3SFEyVUROUlZCTkxRCiAgICBsZW5ndGg6IDMyCnJvb206IGp6Q29kQVVOWUZSUklQU3JISmx4d1BVUnVxTGJQQnh4CnJlbmRlenZvdXM6IG5NckRCbllyVVBMdnFPV0Z2dWZvTktXek1adEJIRmpzCm1kbnM6IGpQUUhIbVZza2x6V29xbWNkeVlnbVhMSVFjTE1HUFN6Cm1heF9tZXNzYWdlX3NpemU6IDIwOTcxNTIwCg== offline: false reboot: false device: \"\" poweroff: false stages: network: - name: \"Setup users\" authorized_keys: kairos: - github:yourhandle! generate-token Generates a new EdgeVPN network token which can be used in a configuration file:\n$ ./kairosctl generate-token b3RwOgogIGRodDoKICAgIGludGVydmFsOiA5MjIzMzcyMDM2ODU0Nzc1ODA3CiAgICBrZXk6IFhMMjRYUk1MTlFOQ1pJQTU0SVFLQ1laMk83SENQWEFBU1ZKN0tZSTQ3MzVaUkpKSktRSEEKICAgIGxlbmd0aDogMzIKICBjcnlwdG86CiAgICBpbnRlcnZhbDogOTIyMzM3MjAzNjg1NDc3NTgwNwogICAga2V5OiBMR1dMWFBTUllaU0ZERDdOT0pBNzdKV0ZWQjRHVkZBMjJIWlZPWU1VT0lNSFVYNFZXUURRCiAgICBsZW5ndGg6IDMyCnJvb206IFRtcUt5VnFHQ1ZZam9TRm9CTEVNRGVEdmJzelBkVEdoCnJlbmRlenZvdXM6IGttb3J4Q21sY2NjVVppWmdkSW5xTERvTGJtS3ZGdm9mCm1kbnM6IEZkWVdQc2R4aHdvWHZlb0VzSXNnVHRXbEJUbE9IVHJmCm1heF9tZXNzYWdlX3NpemU6IDIwOTcxNTIwCg== And now:\nkairos: network_token: b3RwOgogIGRodDoKICAgIGludGVydmFsOiA5MjIzMzcyMDM2ODU0Nzc1ODA3CiAgICBrZXk6IFhMMjRYUk1MTlFOQ1pJQTU0SVFLQ1laMk83SENQWEFBU1ZKN0tZSTQ3MzVaUkpKSktRSEEKICAgIGxlbmd0aDogMzIKICBjcnlwdG86CiAgICBpbnRlcnZhbDogOTIyMzM3MjAzNjg1NDc3NTgwNwogICAga2V5OiBMR1dMWFBTUllaU0ZERDdOT0pBNzdKV0ZWQjRHVkZBMjJIWlZPWU1VT0lNSFVYNFZXUURRCiAgICBsZW5ndGg6IDMyCnJvb206IFRtcUt5VnFHQ1ZZam9TRm9CTEVNRGVEdmJzelBkVEdoCnJlbmRlenZvdXM6IGttb3J4Q21sY2NjVVppWmdkSW5xTERvTGJtS3ZGdm9mCm1kbnM6IEZkWVdQc2R4aHdvWHZlb0VzSXNnVHRXbEJUbE9IVHJmCm1heF9tZXNzYWdlX3NpemU6IDIwOTcxNTIwCg== offline: false reboot: false device: \"\" poweroff: false stages: network: - name: \"Setup users\" authorized_keys: kairos: - github:yourhandle! register The register command can be used to register and drive installation of nodes via QR code with a cloud-init config file (with --config).\nNAME: register - USAGE: register [command options] [arguments...] OPTIONS: --config value --device value --reboot --poweroff When booting Kairos via ISO, the boot process ends up in displaying a QR code which can be parsed by kairosctl register from another machine.\nTaking a screenshot register by default takes a screenshot and tries to find a QR code in it:\nkairosctl register Providing a QR code image/screenshot manually It can be also be specified an image:\nkairosctl register \u003cfile.png\u003e After the pairing is done, the node will start installation with the provided options.\nA --device and a --config file are required in order to have a functional installation.\nbridge Connect to the nodes in the VPN P2P network by creating a tun device on the host.\nIt needs a --network-token($NETWORK_TOKEN) argument and exposes an API endpoint available at localhost:8080 to monitor the network status.\nvalidate The validate command can be used to validate a cloud config file.\nNAME: kairosctl validate - Validates a cloud config file USAGE: kairosctl validate [command options] [arguments...] DESCRIPTION: The validate command expects a configuration file as its only argument. Local files and URLs are accepted. OPTIONS: --help, -h show help ","categories":"","description":"Learn how to use kairosctl to register nodes, generate tokens, and manage VPN connections securely and efficiently.","excerpt":"Learn how to use kairosctl to register nodes, generate tokens, and …","ref":"/docs/reference/kairosctl/","tags":"","title":"kairosctl"},{"body":"Installation media Download the Kairos ISO of your choice. See the Getting Started guide for more information.\nWhen deploying on a bare metal server, directly flash the image into a USB stick. There are multiple ways to do this:\nFrom the CLI dd if=/path/to/iso of=/path/to/dev bs=4MB From the GUI For example using an application like balenaEtcher but can be any other application which allows you to write bootable USBs.\nWarning If you’re booting in UEFI mode, make sure that your storage device where you’re planning to install Kairos, is configured as ACHI and not RAID. ","categories":"","description":"Install Kairos on real hardware!\n","excerpt":"Install Kairos on real hardware!\n","ref":"/docs/installation/bare-metal/","tags":"","title":"Installing on Bare-Metal"},{"body":" Warning This page is a work in progress! The feature is experimental and API is likely going to be subject to changes, don’t rely on it yet! Note This guide provides detailed information about building Kairos images. For a complete guide on creating custom cloud images, including when and how to use these build methods, see Creating Custom Cloud Images. This documentation section describes how the Kairos Kubernetes Native API extensions can be used to build custom appliances or booting medium for Kairos.\nWhile it’s possible to just run Kairos from the artifacts provided by our release process, there are specific use-cases which needs extended customization, for example when additional kernel modules, or custom, user-defined logic that you might want to embed in the media used for installations.\nNote the same can be achieved by using advanced configuration and actually modify the images during installation phase by leveraging the chroot stages that takes place in the image - this is discouraged - as it goes in opposite with the “Single Image”, “No infrastructure drift” approach of Kairos. The idea here is to create a system from “scratch” and apply that on the nodes - not to run any specific logic on the node itself.\nTo achieve that, Kairos provides a set of Kubernetes Native Extensions that allow to programmatically generate Installable mediums, Cloud Images and Netboot artifacts. These provide on-demand customization and exploit Kubernetes patterns to automatically provision nodes using control-plane management clusters - however, the same toolset can be used to build appliances for local development and debugging.\nThe automated section already shows some examples of how to leverage the Kubernetes Native Extensions and use the Kairos images to build appliances, in this section we will cover and describe in detail how to leverage the CRDs and the Kairos factory to build custom appliances.\nPrerequisites When building locally, only docker is required to be installed on the system. To build with the Kubernetes Native extensions, a Kubernetes cluster is required and helm and kubectl installed locally. Note kind can be used as well. The Native extensions don’t require any special permission, and run completely unprivileged.\nKubernetes To build with Kubernetes we need to install the Kairos osbuilder controller.\nThe chart depends on cert-manager. You can install the latest version of cert-manager by running the following commands:\nkubectl apply -f https://github.com/jetstack/cert-manager/releases/latest/download/cert-manager.yaml kubectl wait --for=condition=Available deployment --timeout=2m -n cert-manager --all Install the Kubernetes charts with helm:\nhelm repo add kairos https://kairos-io.github.io/helm-charts helm repo update helm install kairos-crd kairos/kairos-crds helm install kairos-osbuilder kairos/osbuilder Among the things deployed by the helm chart, is also an nginx server which is used to serve the artifact files after they are built. See below for more.\nBuild an ISO To build an ISO, consider the following spec, which provides a hybrid bootable ISO (UEFI/MBR), with the core kairos image, adding helm:\nkind: Secret apiVersion: v1 metadata: name: cloud-config stringData: userdata: | #cloud-config users: - name: \"kairos\" passwd: \"kairos\" install: device: \"auto\" reboot: true poweroff: false auto: true # Required, for automated installations --- kind: OSArtifact apiVersion: build.kairos.io/v1alpha2 metadata: name: hello-kairos spec: imageName: \"quay.io/kairos/@flavor:@flavorRelease-standard-amd64-generic-master-k3sv1.33.4-k3s1\" iso: true bundles: # Bundles available at: https://packages.kairos.io/Kairos/ - quay.io/kairos/packages:helm-utils-3.10.1 cloudConfigRef: name: cloud-config key: userdata exporters: - template: spec: restartPolicy: Never containers: - name: upload image: quay.io/curl/curl command: - /bin/sh args: - -c - | for f in $(ls /artifacts) do curl -T /artifacts/$f http://osartifactbuilder-operator-osbuilder-nginx/upload/$f done volumeMounts: - name: artifacts mountPath: /artifacts Apply the manifest with kubectl apply.\nNote, the CRD allows to specify a custom Cloud config file, check out the full configuration reference.\nAs mentioned above, there is an nginx server that will serve the built artifacts as soon as they are ready. By default, it is exposed with a NodePort type of service. Use the following commands to get its URL:\nThe controller will create a pod that builds the ISO ( we can follow the process by tailing to the containers log ) and later makes it accessible to its own dedicated service (nodeport by default):\n$ PORT=$(kubectl get svc osartifactbuilder-operator-osbuilder-nginx -o json | jq '.spec.ports[0].nodePort') $ curl http://\u003cnode-ip\u003e:$PORT/hello-kairos.iso -o output.iso Netboot artifacts It is possible to use the CRD to prepare artifacts required for netbooting, by enabling netboot: true for instance:\nkind: OSArtifact metadata: name: hello-kairos spec: imageName: \"quay.io/kairos/@flavor:@flavorRelease-core-amd64-generic-master\" netboot: true netbootURL: ... bundles: ... cloudConfig: ... exporters: ... Build a Cloud Image Cloud images are images that automatically boots into recovery mode and can be used to deploy whatever image you want to the VM. Custom user-data from the Cloud provider is automatically retrieved, additionally the CRD allows to embed a custom cloudConfig so that we can use to make configuration permanent also for VM images running outside a cloud provider.\nA Cloud Image boots in QEMU and also in AWS, consider:\nkind: Secret apiVersion: v1 metadata: name: cloud-config stringData: userdata: | #cloud-config users: - name: \"kairos\" passwd: \"kairos\" name: \"Default deployment\" stages: boot: - name: \"Repart image\" layout: device: label: COS_RECOVERY add_partitions: - fsLabel: COS_STATE size: 16240 # At least 16gb pLabel: state - name: \"Repart image\" layout: device: label: COS_RECOVERY add_partitions: - fsLabel: COS_PERSISTENT pLabel: persistent size: 0 # all space - if: '[ -f \"/run/cos/recovery_mode\" ] \u0026\u0026 [ ! -e /usr/local/.deployed ]' name: \"Deploy kairos\" commands: - kairos-agent --debug reset --unattended - touch /usr/local/.deployed - reboot --- apiVersion: build.kairos.io/v1alpha1 kind: OSArtifact metadata: name: hello-kairos spec: imageName: \"quay.io/kairos/@flavor:@flavorRelease-core-amd64-generic-master\" cloudImage: true cloudConfigRef: name: cloud-config key: userdata Note: Since the image come with only the recovery system populated, we need to apply a cloud-config similar to this one which tells which container image we want to deploy. The first steps when the machine boots into is to actually create the partitions needed to boot the active and the passive images, and its populated during the first boot.\nAfter applying the spec, the controller will create a Kubernetes Job which runs the build process and then copy the produced hello-kairos.raw file to the nginx server (see above). Alternatively you may configure your own job to copy the content elsewhere. This file is an EFI bootable raw disk, bootable in QEMU and compatible with AWS which automatically provisions the node:\n$ PORT=$(kubectl get svc osartifactbuilder-operator-osbuilder-nginx -o json | jq '.spec.ports[0].nodePort') $ curl http://\u003cnode-ip\u003e:$PORT/hello-kairos.raw -o output.raw Note, in order to use the image with QEMU, we need to resize the disk at least to 32GB, this can be done with the CRD by setting diskSize: 32000 or by truncating the file after downloading:\ntruncate -s \"+$((32000*1024*1024))\" hello-kairos.raw This is not required if running the image in the Cloud as providers usually resize the disk during import or creation of new instances.\nTo run the image locally with QEMU we need qemu installed in the system, and we need to be able to run VMs with EFI, for example:\nqemu-system-x86_64 -m 2048 -bios /usr/share/qemu/ovmf-x86_64.bin -drive if=virtio,media=disk,file=output.raw Use the Image in AWS To consume the image, copy it into an s3 bucket:\naws s3 cp \u003ccos-raw-image\u003e s3://\u003cyour_s3_bucket\u003e Create a container.json file referring to it:\n{ \"Description\": \"Kairos custom image\", \"Format\": \"raw\", \"UserBucket\": { \"S3Bucket\": \"\u003cyour_s3_bucket\u003e\", \"S3Key\": \"\u003ccos-raw-image\u003e\" } } Import the image:\naws ec2 import-snapshot --description \"Kairos custom image\" --disk-container file://container.json Follow the procedure described in AWS docs to register an AMI from snapshot. Use all default settings except for the firmware, set to force to UEFI boot.\nSince release v3.3.0, Kairos release pipeline is pushing a public image to AWS, which you can use. Read how to deploy Kairos using an AMI (the released or a custom one), in the relevant page.\nUse the Image in OpenStack First get the generated image:\n$ PORT=$(kubectl get svc osartifactbuilder-operator-osbuilder-nginx -o json | jq '.spec.ports[0].nodePort') $ curl http://\u003cnode-ip\u003e:$PORT/hello-kairos.raw -o output.raw Import the image to Glance:\nosp image create hello-kairos-image --property hw_firmware_type='uefi' --file ./hello-kairos.raw Image could be used to create an OpenStack instance.\nSet the property to force to UEFI boot. If not kairos won’t be able to start and you could be prompted endlessly by :\nBooting from hard drive... Build a Cloud Image for Azure Similarly we can build images for Azure, consider:\napiVersion: build.kairos.io/v1alpha1 kind: OSArtifact metadata: name: hello-kairos spec: imageName: \"quay.io/kairos/@flavor:@flavorRelease-core-amd64-generic-master\" azureImage: true ... Will generate a compressed disk hello-kairos-azure.vhd ready to be used in Azure.\n$ PORT=$(kubectl get svc osartifactbuilder-operator-osbuilder-nginx -o json | jq '.spec.ports[0].nodePort') $ curl http://\u003cnode-ip\u003e:$PORT/hello-kairos-azure.vhd -o output.vhd How to use the image in Azure Upload the Azure Cloud VHD disk in .vhda format to your bucket:\naz storage copy --source \u003ccos-azure-image\u003e --destination https://\u003caccount\u003e.blob.core.windows.net/\u003ccontainer\u003e/\u003cdestination-azure-image\u003e Import the disk:\naz image create --resource-group \u003cresource-group\u003e --source https://\u003caccount\u003e.blob.core.windows.net/\u003ccontainer\u003e/\u003cdestination-azure-image\u003e --os-type linux --hyper-v-generation v2 --name \u003cimage-name\u003e Note: There is currently no way of altering the boot disk of an Azure VM via GUI, use the az to launch the VM with an expanded OS disk if needed\nBuild a Cloud Image for GCE Similarly we can build images for GCE, consider:\napiVersion: build.kairos.io/v1alpha1 kind: OSArtifact metadata: name: hello-kairos spec: imageName: \"quay.io/kairos/@flavor:@flavorRelease-core-amd64-generic-master\" gceImage: true ... Will generate a compressed disk hello-kairos.gce.raw.tar.gz ready to be used in GCE.\n$ PORT=$(kubectl get svc osartifactbuilder-operator-osbuilder-nginx -o json | jq '.spec.ports[0].nodePort') $ curl http://\u003cnode-ip\u003e:$PORT/hello-kairos.gce.raw.tar.gz -o output.gce.raw.tar.gz How to use the image in GCE To upload the image in GCE (compressed):\ngsutil cp \u003ccos-gce-image\u003e gs://\u003cyour_bucket\u003e/ Import the disk:\ngcloud compute images create \u003cnew_image_name\u003e --source-uri=\u003cyour_bucket\u003e/\u003ccos-gce-image\u003e --guest-os-features=UEFI_COMPATIBLE See here how to use a cloud-init with Google cloud.\n","categories":"","description":"Learn how to build Kairos images from scratch","excerpt":"Learn how to build Kairos images from scratch","ref":"/docs/advanced/build/","tags":"","title":"Build Kairos appliances"},{"body":" Objective This guide will walk you through the steps to build a new Kairos container image and add additional packages. You will then use the newly built image to upgrade a Kairos node manually. Prerequisites A single node Kairos cluster as the one deployed in the Getting Started guide. Access to Docker or a different container engine on your machine Do you prefer to watch a video? Containers As we saw in the previous section on how to configure Kairos, it is not possible to add packages on a running system because it is mounted as read-only. Instead we are going to use containers to extend our base Kairos image. Containers play a central role on Kairos lifecycle.\nStart by creating a file called Dockerfile with the following content\nFROM quay.io/kairos/ubuntu:24.04-standard-amd64-generic-v3.1.2-k3sv1.30.4-k3s1 RUN apt-get update \u0026\u0026 apt-get install -y ruby We base our own image on the Kairos image, then we proceed to install the ruby package. This would be done differently depending on the package manager of the flavor of your choosing.\nNow we build the image\ndocker build -t ttl.sh/mauros-v1.0.0:24h . I’m using ttl.sh as my repositoy becuase it is a temporary one, but you can use whichever you prefer. In this case the 24h tag means that this image will only be accessible for 24 hours. I also named my image in a way that is helpful for me to keep track of the different changes within my images, in this case I use semver but you can use whatever other means for tracking changes.\nAnd we need to push it out to a registry\ndocker push ttl.sh/mauros-v1.0.0:24h Upgrading a Kairos node manually There are different ways to upgrade a Kairos node, in this case we will follow a manual approach just to get you initiated on doing Day-2 operations to a Kairos cluster. Make sure to also check out the other ways to upgrade your Kairos system and clusters.\nSSH into the node Run an upgrade with the Kairos agent and add your image as the source kairos-agent upgrade --source oci:ttl.sh/mauros-v1.0.0:24h Reboot the system Conclusion Congrats! You have now taken your Kairos experience to another level. You can now create your own images and upgrade your nodes, soon you will be an expert.\nFrequently Asked Questions (FAQs) When should I add things on the Dockerfile and when to the cloud-config?\nThe answer to this will depend on your setup but here are two things to keep in mind. If you can install software via Kubernetes, then you don’t need to build your own images, for example for a Ruby on Rails application, you don’t need Ruby at the host system, since it will running in containers. If you cannot or don’t want to install on Kubernetes, then just keep this in mind. Changes to your cloud-config can be applied on a node and the effects will be present as soon as you reboot, giving you a faster feedback loop, while changes done at the Dockerfile level, will require that you build a new image and upgrade all your nodes.\nWhat if I don’t want to base my image on the Kairos released artifacts?\nNo problem, just build your image from scratch\nCan I easily rollback an upgrade?\nYes you can! Boot into the passive (fallback) system, and apply an upgrade with the previous image (or to any image you’d rather upgrade to). This will completely replace the active image with the one you specify, keeping your passive image intact.\nWhat’s next? Check other ways to upgrade Kairos\nUpgrade Guide Create your own Kairos Flavor\nBuild from Scratch Guide Got stuck?\nTroubleshooting Guide ","categories":"","description":"Learn how to customize Kairos with additional packages and apply upgrades using container-based image workflows.\n","excerpt":"Learn how to customize Kairos with additional packages and apply …","ref":"/getting-started/building-and-upgrading/","tags":"","title":"Build a Kairos Image and Upgrade Your Cluster"},{"body":" Warning This feature is in preview state and only available in Kairos v3.4.x releases and alphas. Please check the section “Known issues” at the bottom for more information. Signing keys for system extensions under Trusted Boot Sysexts need to be signed with the same key/cert as the ones used to sign the EFI files. As those are part of the system and available in the EFI firmware, we can extract the public part and verify the sysexts locally. Any of the PK, KEK or DB keys can be used to sign sysexts. This only affects Trusted Boot. Introduction System extensions are a way to extend the system with additional files and directories that are mounted at boot time. System extension images may – dynamically at runtime — extend the /usr/ directory hierarchies with additional files. This is particularly useful on immutable system images where a /usr/ hierarchy residing on a read-only file system shall be extended temporarily at runtime without making any persistent modifications. Or on a Trusted Boot system where the system is booted from a read-only EFI and cannot be extended easily without breaking the signature.\nThis feature works on both Trusted Boot and normal Kairos installations, the only difference is the signature verification of the system extension images. On Trusted Boot, the system extension images are verified against the public keys stored in the firmware. This is done to ensure that only trusted extensions are loaded into the system.\nFor more information on system extensions, please refer to the System extensions documentation.\nRequirement: Base image of the OS needs to have at least systemd 252 or newer ( for example ubuntu \u003e=23.10 or fedora \u003e=38 )\nBuilding system extensions manually To build a system extension, you need to create a directory with the files you want to add to the system. Then you can use the systemd-repart tool to create a system extension image which is signed and verity protected.\nThe directory with the sources needs to be structured in a way that the files are placed in the same path as they would be in the final system. For example, this is the dir tree for k3s:\n. └── v1.29.2+k3s1 └── usr ├── lib │ └── extension-release.d │ └── extension-release.k3s-v1.29.2+k3s1 └── local ├── bin │ ├── crictl -\u003e ./k3s │ ├── ctr -\u003e ./k3s │ ├── k3s │ └── kubectl -\u003e ./k3s └── lib └── systemd └── system ├── k3s-agent.service └── k3s.service Then you can use the systemd-repart tool to create the sysext image:\n$ systemd-repart -S -s SOURCE_DIR NAME.sysext.raw --private-key=PRIVATE_KEY --certificate=CERTIFICATE Warning Note that the extensions MUST have a /usr/lib/extension-release.d/extension-release.NAME file in which the NAME needs to match the sysext NAME (extension is ignored). This is an enforcement by systemd to ensure the sysext is correctly identified and some sanity checks are done with the info in that file. This will generate a signed+verity sysextension that can then be used by sysext to extend the system.\nSome extension examples are available under https://github.com/Itxaka/sysext-examples for k3s and sbctl.\nBuilding system extensions from a docker image with auroraboot Warning This feature is in preview state and only available in Auroraboot from version v0.3.0 You can also build a system extension from a docker image directly by using auroraboot and using a dockerfile to isolate the artifacts you want converted into a system extension.\nNotice that when converting a docker image into a system extension, the last layer is the only one converted (The last command in a given Dockerfile) so have that in mind. This is useful for packages that ONLY install things in /usr or manual installation under /usr.\nThe /usr/lib/extension-release.d/extension-release.NAME file necessary for identifying the system extension is automatically created by the command so in this case you should not worry about that file.\nFor example for a given Dockerfiles as such:\nFROM anchore/grype:latest AS grype FROM scratch COPY --from=grype /grype /usr/local/bin/grype Only the files added in the last step will be converted to a sysext, so the contents of the sysext would be the /usr/local/bin/grype binary only.\nOr for a even more manual one:\nFROM alpine:3.19 RUN apk add curl RUN curl -L https://github.com/Foxboron/sbctl/releases/download/0.15.4/sbctl-0.15.4-linux-amd64.tar.gz | tar xvzf - --strip-components=1 -C /usr/local/bin/ Again, only the files in the last step would be converted into a system extension, so we would get the contents of the extracted tar archive at the /usr/local/bin/ path.\nAfter building the chosen Dockerfile, we would just need to run osbuilder with the sysext command and the key and certificate, like we would do with systemd-repart. Notice that we are binding the local keys/ dir into the container /keys dir for ease of access to the given keys and the current dir under /build on the container so we set the --output=/build flag when calling auroraboot:\n$ docker run \\ -v \"$PWD\"/keys:/keys \\ -v \"$PWD\":/build/ \\ -v /var/run/docker.sock:/var/run/docker.sock \\ --rm \\ quay.io/kairos/auroraboot:v0.13.0 sysext --private-key=/keys/PRIVATE_KEY --certificate=/keys/CERTIFICATE --output=/build NAME CONTAINER_IMAGE The explanation of the docker command flags is as follows:\n-v \"$PWD\"/keys:/keys: We mount the current dir + keys dir into the container /keys path. So auroraboot has access to the keys to sign the sysext. -v \"$PWD\":/build/: Mount the current dir into the container /build path. So the generated sysext is available after the container is gone. -v /var/run/docker.sock:/var/run/docker.sock: We pass the docker sock into the container so it can access our locally built container images. So we avoid pushing them and pulling them from a remote registry. --rm: Once the container exit, remove it so we dont leave stuff lying around. The explanation of the auroraboot command flags is as follows:\nsysext: Subcommand to call, in this case we want to build a sysext --private-key: Private key to sign the system extension. --certificate: Certificate to sign the system extension. --output: Dir where we will output the system extension. Make sure that this matches the directory that passed to the docker command to be able to keep the generated system extension once the container exists and its removed. NAME: Output and internal name of the sysext. CONTAINER_IMAGE: Image from which we will extract the last layer and covert it to a system extension. Example of a successful run:\n$ docker run -v \"$PWD\":/build/ -v /tmp/keys/:/keys -v /var/run/docker.sock:/var/run/docker.sock --rm -ti quay.io/kairos/auroraboot:v0.13.0 sysext --private-key=/keys/db.key --certificate=/keys/db.pem --output /build grype sysext 2024-09-16T14:59:36Z INF Starting auroraboot version 2024-09-16T14:59:36Z INF 🚀 Start sysext creation 2024-09-16T14:59:36Z INF 💿 Getting image info 2024-09-16T14:59:36Z INF 📤 Extracting archives from image layer 2024-09-16T14:59:37Z INF 📦 Packing sysext into raw image 2024-09-16T14:59:37Z INF 🎉 Done sysext creation output=/build/grype.sysext.raw $ ls -ltra *.raw -rw-r--r-- 1 root root 64729088 sep 16 17:24 grype.sysext.raw Verifying the system extensions You can use systemd-dissect to verify the system extension, the ID, ARCHITECTURE and the partitions that are included in the system extension.\n$ sudo systemd-dissect sbctl-0.14.sysext.raw Name: sbctl-0.14.sysext.raw Size: 21.0M Sec. Size: 512 Arch.: x86-64 Image UUID: 351f0e17-35e5-42ff-bf09-8db65c756f7b sysext R.: ID=_any ARCHITECTURE=x86-64 Use As: ✗ bootable system for UEFI ✗ bootable system for container ✗ portable service ✗ initrd ✓ sysext for system ✓ sysext for portable service ✗ sysext for initrd ✗ confext for system ✗ confext for portable service ✗ confext for initrd RW DESIGNATOR PARTITION UUID PARTITION LABEL FSTYPE AR\u003e ro root 4afae1e5-c73c-2f5a-acdc-3655ed91d4e0 root-x86-64 erofs x8\u003e ro root-verity abea5f2f-214d-4d9f-83f8-ee69ca7614ba root-x86-64-verity DM_verity_hash x8\u003e ro root-verity-sig bdb3ee65-ed86-480c-a750-93015254f1a7 root-x86-64-verity-sig verity_hash_signature x8\u003e Managing System Extensions in Kairos 📂 Where Extensions Live All system extensions are stored in:\n/var/lib/kairos/extensions/ From there, they’re symlinked into directories based on the system’s boot profile:\nDirectory Behavior active/ Loaded when booting into the active profile passive/ Loaded during passive boot recovery/ Loaded in recovery mode common/ Always loaded, regardless of boot mode 💡 These directories contain only symlinks—the actual disk image is stored once. This ensures there’s no duplication or leftover state between boots.\n🛠️ CLI Usage Manage extensions using kairos-agent sysext commands.\n📝 Tip: For enable, disable and remove commands, the extension name supports regex matching. You don’t need to type the full filename. For example, to match k3sv1.32.1.k3s0.sysext.raw, you can simply use k3s.\nSubcommands 📥 download Downloads a system extension and stores it on the node.\nkairos-agent sysext download \u003cURI\u003e Supported URI formats:\nhttps:// – Download a raw disk image from a remote server http:// – Same as above, unencrypted file:// – Load a local disk image file oci:// – Download from an OCI-compatible container registry ⚠️ Important Notes:\nhttp(s) and file:// URIs must point directly to a raw disk image file. oci:// support is alpha-stage and may change. When using oci://, the disk image must be embedded inside the OCI image layer. ✅ enable Enable an extension for a specific boot profile:\nkairos-agent sysext enable --active my-extension Supported profile flags:\n--active --passive --recovery --common 🔄 Use --now for Immediate Activation kairos-agent sysext enable --active --now my-extension If the current boot mode matches, this also:\nCreates a link in /run/extensions/ Reloads systemd-sysext so the extension is active immediately 🚫 disable Remove the symlink from the specified profile:\nkairos-agent sysext disable --common my-extension Add --now to also unload the extension if it’s currently live:\nkairos-agent sysext disable --common --now my-extension 🧹 remove Deletes the extension completely—including all symlinks from any profile.\nkairos-agent sysext remove my-extension Use --now to deactivate it immediately as well:\nkairos-agent sysext remove --now my-extension ⚠️ This is a permanent wipe. The extension will no longer be available for any boot profile.\n📋 list Without flags: lists all installed extensions With a profile flag: lists extensions enabled for that boot profile kairos-agent sysext list kairos-agent sysext list --recovery 🧪 Example Workflow # Download a disk image over HTTPS kairos-agent sysext download https://example.org/extensions/k3sv1.32.1.raw # Enable for the active profile and activate it live kairos-agent sysext enable --active --now k3s # See what’s currently enabled for active kairos-agent sysext list --active # Fully remove it and clean up live state kairos-agent sysext remove --now k3s 🧼 Designed for Clean State Management No duplication: all symlinks point to a single image Reversible: simply unlink or remove to disable --now lets you test and roll out changes live All state reset at boot via ephemeral /run/extensions Boot workflow During boot, Immucore will identify under which boot state is running (active, passive, recovery) and will link the found extensions to the /run/extensions dir during initramfs. Then it will enable the systemd-sysext service so they are loaded correctly.\nUnder Trusted Boot, the extensions signature will be verified and if they dont match they will be ignored and a warning emitted under the logs at /run/immucore/.\nKnown issues Sysext images need to be named with the extension .sysext.raw to be identified correctly. This is a design choice to avoid conflicts with other files that could be present in the EFI partition and we don’t expect this to change in the future. Any folder that is mounted as a system extension will be mounted as read-only. So if your sysext is mounting /usr/local/bin to add binaries, it will be mounted as read-only. Other sysexts can be added and they will be merged correctly, but the final dir will be read-only. This is a limitation of the current systemd version (lower than 256) and will be addressed in future releases. Only /usr can be extended. This is a design choice and might change in the future to allow other directories to be extended. System extensions provided binaries are only available after the initramfs stage. Currently only signed+verity sysexts are supported under Trusted Boot (UKI). For non-uki Kairos, the signature is not enforced yet. Sysexts need to be signed with the same key/cert as the ones used to sign the EFI files. As those are part of the system and available in the EFI firmware, we can extract the public part and verify the sysexts locally. Any of the PK, KEK or DB keys can be used to sign sysexts. This is planned to be expanded in the future to allow signing them with a different key/cert and provide the public keys as part of the install configuration so they can be verified. Sysexts are mounted by the name order by trying to parse the name as a version and comparing it to others. This is done directly by systemd so be aware of the naming of your extensions and try to keep them in a versioned format. And example from systemd source code is provided as a guide: * (older) 122.1 * ^ 123~rc1-1 * | 123 * | 123-a * | 123-a.1 * | 123-1 * | 123-1.1 * | 123^post1 * | 123.a-1 * | 123.1-1 * v 123a-1 * (newer) 124-1 ","categories":"","description":"","excerpt":" Warning This feature is in preview state and only available in Kairos …","ref":"/docs/advanced/sys-extensions/","tags":"","title":"Extending the system with systemd extensions"},{"body":"By default, Kairos ISOs are configured to automatically get an IP from the network interface. However, depending on the base system you have chosen, there are different way to configure networking. This section collects information on setting network configuration depending on the base that is being chosen (openSUSE, Alpine, Ubuntu).\nThere are different network managers depending on the distro:\nconnman is available on Alpine-based distribution. By default is enabled on Kairos Alpine flavored variants. systemd-based flavors are all using systemd-networkd Static IP To get a static IP, you can additionally define the following in your configuration file, depending on the network-manager being used:\nconnman systemd-networkd stages: initramfs: - files: - path: /var/lib/connman/default.config permissions: 0644 content: | [service_eth0] Type = ethernet IPv4 = 192.168.122.170/255.255.255.0/192.168.122.1 IPv6 = off Nameservers = 1.1.1.1 stages: initramfs: - files: - path: /etc/systemd/network/01-man.network permissions: 0644 content: | [Match] Name=ens18 [Network] Address=10.1.1.1/16 Gateway=10.1.0.1 DNS=10.1.0.1 Bonding Bonding setup with Ubuntu can be configured via systemd-networkd (Ubuntu based images) and wicked (openSUSE based images), consider the following examples:\nsystemd-networkd connman #cloud-config name: \"My Deployment\" stages: boot: - name: \"Setup network\" commands: - systemctl restart systemd-networkd initramfs: # Drop network config file - name: \"Setup hostname\" hostname: \"hostname\" - name: \"Setup network files\" files: - path: /etc/systemd/network/10-bond0.network content: | [Match] Name=bond0 [Network] DHCP=yes permissions: 0644 owner: 0 group: 0 - path: /etc/systemd/network/10-bond0.netdev content: | [NetDev] Name=bond0 Kind=bond [Bond] Mode=802.3ad permissions: 0644 owner: 0 group: 0 - path: /etc/systemd/network/15-enp.network content: | [Match] Name=enp* [Network] Bond=bond0 permissions: 0644 owner: 0 group: 0 - path: /etc/systemd/network/05-bond0.link content: | [Match] Driver=bonding Name=bond0 [Link] MACAddress=11:22:33:44:55:66 permissions: 0644 owner: 0 group: 0 network: - name: \"Setup user ssh-keys\" authorized_keys: kairos: - \"ssh-rsa AAA...\" - \"ssh-rsa AAA...\" # k3s settings k3s-agent: enabled: true env: K3S_TOKEN: \"KubeSecret\" K3S_URL: https://hostname:6443 stages: boot: - name: \"Setup network\" commands: - modprobe bonding mode=4 miimon=100 - ifenslave bond0 eno1 - ifenslave bond0 eno2 - ifenslave bond0 eno3 - ifenslave bond0 eno4 - ifconfig bond0 up hw ether 11:22:33:44:55:66 - ifup bond0 - sleep 5 - rc-service connman restart initramfs: - name: \"Setup network files\" files: - path: /var/lib/connman/default.config content: | [service_eth] Type = ethernet IPv4 = off IPv6 = off [service_bond0] Type = ethernet DeviceName = bond0 IPv4 = dhcp MAC = 11:22:33:44:55:66 permissions: 0644 owner: 0 group: 0 References https://kerlilow.me/blog/setting-up-systemd-networkd-with-bonding/ ","categories":"","description":"","excerpt":"By default, Kairos ISOs are configured to automatically get an IP from …","ref":"/docs/advanced/networking/","tags":"","title":"Networking"},{"body":"After your system has been running for a while, you will need to upgrade it to a different version. Check out the following sections to learn the different ways to upgrade Kairos.\n","categories":"","description":"Learn how to keep Kairos systems up-to-date using manual methods, Kubernetes-based upgrades, and trusted boot options.\n","excerpt":"Learn how to keep Kairos systems up-to-date using manual methods, …","ref":"/docs/upgrade/","tags":"","title":"Upgrade"},{"body":"Using /opt with System Extensions By default, Kairos does not include /opt as a system extension (sysext) overlay hierarchy. This is because in normal runtime, /opt is writable and bind-mounted to the persistent partition, allowing users and applications to freely write data that persists across reboots.\nHowever, when a system extension is loaded that includes a /opt hierarchy, the behavior of that directory changes: it becomes read-only, overridden by the overlay from the system extension image. This is a consequence of how systemd-sysext currently operates and reflects a known upstream limitation.\nWhy /opt Might Be Needed If your use case includes deploying system extensions that provide optional software, plugins, or third-party tools installed under /opt, you might need to explicitly enable /opt as a supported hierarchy for sysext overlays.\nThis can be done by configuring the environment variable SYSTEMD_SYSEXT_HIERARCHIES to include /opt, and ensuring that the systemd-sysext service uses this setting both at runtime and in the initramfs phase.\n⚠️ Once /opt is included in the sysext overlay, the directory becomes read-only as soon as any system extension mounts a /opt subtree. This can break applications or scripts expecting to write to /opt.\nAs of systemd 255, there is no way to mark overlay hierarchies as mutable. However, upstream efforts are underway to address this in systemd 256 and beyond, allowing finer control over the mutability of sysext mount points.\nEnabling /opt in System Extensions: Cloud-Init Configuration To allow system extensions to provide content under /opt, you can modify the systemd-sysext configuration using Kairos cloud config.\nBelow is an example cloud-init YAML you can embed in your OEM configuration or custom image. It ensures that /opt is registered as a sysext hierarchy both in normal and Trusted Boot installations. In here we are overriding the existing files to include the /opt directory in the SYSTEMD_SYSEXT_HIERARCHIES environment variable that Kairos sets by default.\nname: \"sysext using /opt\" stages: initramfs.after: - name: \"systemd-sysext uki config\" if: '[ -e \"/run/cos/uki_boot_mode\" ] \u0026\u0026 [ ! -e \"/run/cos/recovery_mode\" ] \u0026\u0026 [ ! -e \"/run/cos/autoreset_mode\" ]' files: - path: /etc/systemd/system/systemd-sysext.service.d/kairos-uki.conf permissions: 0644 owner: 0 group: 0 content: | [Service] TimeoutStartSec=10 ExecStart= ExecStart=systemd-sysext refresh --image-policy=\"root=verity+signed+absent:usr=verity+signed+absent\" ExecReload= ExecReload=systemd-sysext refresh --image-policy=\"root=verity+signed+absent:usr=verity+signed+absent\" Environment=\"SYSTEMD_SYSEXT_HIERARCHIES=/usr/local/bin:/usr/local/sbin:/usr/local/include:/usr/local/lib:/usr/local/share:/usr/local/src:/usr/bin:/usr/share:/usr/lib:/usr/include:/usr/src:/usr/sbin:/opt\" [Unit] JobRunningTimeoutSec=5 - name: \"systemd-sysext config\" if: '[ ! -e \"/run/cos/uki_boot_mode\" ] \u0026\u0026 [ ! -e \"/run/cos/recovery_mode\" ] \u0026\u0026 [ ! -e \"/run/cos/autoreset_mode\" ]' files: - path: /etc/systemd/system/systemd-sysext.service.d/kairos.conf permissions: 0644 owner: 0 group: 0 content: | [Service] TimeoutStartSec=10 ExecStart= ExecStart=systemd-sysext refresh --image-policy=\"root=verity+absent:usr=verity+absent\" ExecReload= ExecReload=systemd-sysext refresh --image-policy=\"root=verity+absent:usr=verity+absent\" Environment=\"SYSTEMD_SYSEXT_HIERARCHIES=/usr/local/bin:/usr/local/sbin:/usr/local/include:/usr/local/lib:/usr/local/share:/usr/local/src:/usr/bin:/usr/share:/usr/lib:/usr/include:/usr/src:/usr/sbin:/opt\" [Unit] JobRunningTimeoutSec=5 - name: \"systemd-sysext set hierarchy system-wide\" if: '[ ! -e \"/run/cos/recovery_mode\" ] \u0026\u0026 [ ! -e \"/run/cos/autoreset_mode\" ]' files: - path: /etc/profile.d/systemd-sysext.sh permissions: 0644 owner: 0 group: 0 content: | export SYSTEMD_SYSEXT_HIERARCHIES=\"/usr/local/bin:/usr/local/sbin:/usr/local/include:/usr/local/lib:/usr/local/share:/usr/local/src:/usr/bin:/usr/share:/usr/lib:/usr/include:/usr/src:/usr/sbin:/opt\" - name: \"systemd-sysext initramfs settings\" if: '[ -e \"/sbin/systemctl\" ] || [ -e \"/usr/bin/systemctl\" ] || [ -e \"/usr/sbin/systemctl\" ] || [ -e \"/bin/systemctl\" ]' systemctl: enable: - systemd-sysext ","categories":"","description":"","excerpt":"Using /opt with System Extensions By default, Kairos does not include …","ref":"/docs/advanced/adding_opt_to_system_extensions/","tags":"","title":"Using /opt with System Extensions"},{"body":"The Kairos operator is the recommended way to manage upgrades and operations on Kairos nodes in a Kubernetes cluster. It provides a more integrated and Kairos-specific approach compared to the system-upgrade-controller which was used in the past.\nOverview The Kairos operator provides two custom resources:\nNodeOp: For generic operations on Kubernetes nodes (Kairos or not). It allows mounting the host’s root filesystem to perform operations or run scripts.\nNodeOpUpgrade: A Kairos-specific custom resource for upgrading Kairos nodes. It automatically creates a NodeOp with the appropriate upgrade script and configuration.\nDeploying the operator To deploy the operator, you can use kubectl (provided that the git command is available):\n# Using GitHub URL kubectl apply -k https://github.com/kairos-io/kairos-operator/config/default # Or using local directory (if you have the operator checked out) kubectl apply -k config/default When the operator starts, it will automatically detect Kairos nodes and label them with kairos.io/managed: true. This label can be used to target Kairos nodes specifically in hybrid clusters.\nRemoving the operator # Using GitHub URL kubectl delete -k https://github.com/kairos-io/kairos-operator/config/default # Or using local directory kubectl delete -k config/default Installing via Bundle You can also install the Kairos Operator using a bundle by adding the following configuration to your cloud-config file:\nbundles: - targets: - run://quay.io/kairos/community-bundles:kairos-operator_latest This will automatically deploy the operator during the node initialization process.\nRemoving the Bundle Installation To remove the operator installed via bundle, you need to delete the kairos-operator.yaml file from the appropriate location:\nk0s: /var/lib/k0s/manifests/kairos-operator/ k3s: /var/lib/rancher/k3s/server/manifests/ Basic NodeOp example Here’s a simple example of a NodeOp resource:\napiVersion: operator.kairos.io/v1alpha1 kind: NodeOp metadata: name: example-nodeop namespace: default spec: # NodeSelector to target specific nodes (optional) nodeSelector: matchLabels: kairos.io/managed: \"true\" # The container image to run on each node image: busybox:latest # The command to execute in the container command: - sh - -c - | echo \"Running on node $(hostname)\" ls -la /host/etc/kairos-release cat /host/etc/kairos-release # Path where the node's root filesystem will be mounted (defaults to /host) hostMountPath: /host # Whether to cordon the node before running the operation cordon: true # Drain options for pod eviction drainOptions: enabled: true force: false gracePeriodSeconds: 30 ignoreDaemonSets: true deleteEmptyDirData: false timeoutSeconds: 300 # Whether to reboot the node after successful operation rebootOnSuccess: true # Number of retries before marking the job failed backoffLimit: 3 # Maximum number of nodes that can run the operation simultaneously # 0 means run on all nodes at once concurrency: 1 # Whether to stop creating new jobs when a job fails # Useful for canary deployments stopOnFailure: true What’s next? Upgrading Kairos with the operator Trusted Boot upgrades Manual upgrades ","categories":"","description":"Install the Kairos operator for managing upgrades and operations","excerpt":"Install the Kairos operator for managing upgrades and operations","ref":"/docs/upgrade/kairos-operator/","tags":"","title":"Installing Kairos Operator"},{"body":" Deprecated The system-upgrade-controller approach is deprecated. We recommend using the Kairos operator instead, which provides a more integrated and Kairos-specific way to manage upgrades and operations. To upgrade Kairos with Kubernetes using the legacy approach, it is necessary to have system-upgrade-controller deployed on the target cluster.\nThe upstream documentation on how to install the system-upgrade-controller, is this command:\nkubectl apply -k github.com/rancher/system-upgrade-controller This command requires the git command to be available in order to clone the remote repository. Kairos images, generally, don’t include git. You will need to run this command from a machine which has git available and access to the cluster with a valid KUBECONFIG file.\nAlternatively, from withing the Kairos node, you can deploy the following Job which will clone the system-upgrade controller repository to the /home/kairos directory:\napiVersion: batch/v1 kind: Job metadata: name: git spec: template: spec: restartPolicy: Never containers: - name: git image: alpine/git command: [\"git\", \"clone\", \"--branch\", \"v0.14.1\", \"https://github.com/rancher/system-upgrade-controller\", \"/homedir/system-upgrade-controller\"] volumeMounts: - name: homedir mountPath: /homedir volumes: - name: homedir hostPath: path: /home/kairos type: Directory (make sure you checkout the desired branch/release)\nThen, from the /home/kairos directory, you can run this command to deploy the system-upgrade-controller:\nkubectl apply -k system-upgrade-controller ","categories":"","description":"Install the system-upgrade-controller (deprecated - use Kairos operator instead)","excerpt":"Install the system-upgrade-controller (deprecated - use Kairos …","ref":"/docs/upgrade/system-upgrade-controller/","tags":"","title":"Installing system-upgrade-controller (Deprecated)"},{"body":"When developing or troubleshooting Kairos, it can be useful to share a local cluster with another peer. This section illustrates how to use Entangle to achieve that. We call this setup debugging-station.\nConfiguration Note This section describes the configuration step by step. If you are in a hurry, you can skip this section and directly go to Deploy with AuroraBoot. When deploying a new cluster, we can use Bundles to install the entangle and cert-manager chart automatically. We specify the bundles in the cloud config file as shown below:\nbundles: - targets: - run://quay.io/kairos/community-bundles:cert-manager_latest - run://quay.io/kairos/community-bundles:kairos_latest We also need to enable entangle by setting kairos.entangle.enable: true.\nNext, we generate a new token that we will use to connect to the cluster later.\ndocker run -ti --rm quay.io/mudler/edgevpn -b -g In order for entangle to use the token, we can define a Entanglement to expose ssh in the mesh network like the following:\napiVersion: v1 kind: Secret metadata: name: ssh-entanglement namespace: kube-system type: Opaque stringData: network_token: ___GENERATED TOKEN HERE___ --- apiVersion: entangle.kairos.io/v1alpha1 kind: Entanglement metadata: name: ssh-entanglement namespace: kube-system spec: serviceUUID: \"ssh\" secretRef: \"ssh-entanglement\" host: \"127.0.0.1\" port: \"22\" hostNetwork: true Note If you have already a kubernetes cluster, you can install the Entangle chart and just apply the manifest. This entanglement will expose the port 22 in the node over the mesh network with the ssh service UUID so we can later connect to it. Replace ___GENERATED TOKEN HERE___ with the token you previously generated with the docker command (check out the documentation for advanced usage).\nIn order to deploy the Entanglement automatically, we can add it to the k3s manifests folder in the cloud config file:\nwrite_files: - path: /var/lib/rancher/k3s/server/manifests/expose-ssh.yaml permissions: \"0644\" owner: \"root\" content: | apiVersion: v1 kind: Secret metadata: name: ssh-entanglement namespace: kube-system type: Opaque stringData: network_token: ___GENERATED TOKEN HERE___ --- apiVersion: entangle.kairos.io/v1alpha1 kind: Entanglement metadata: name: ssh-entanglement namespace: kube-system spec: serviceUUID: \"ssh\" secretRef: \"ssh-entanglement\" host: \"127.0.0.1\" port: \"22\" hostNetwork: true Here’s an example of a complete cloud configuration file which automatically install a Kairos node in the bigger disk, and exposes ssh with entangle:\n#cloud-config install: device: \"auto\" auto: true reboot: true hostname: debugging-station-{{ trunc 4 .MachineID }} users: - name: kairos passwd: kairos groups: - admin ssh_authorized_keys: - github:mudler k3s: enabled: true # Specify the bundle to use bundles: - targets: - run://quay.io/kairos/community-bundles:cert-manager_latest - run://quay.io/kairos/community-bundles:kairos_latest kairos: entangle: enable: true write_files: - path: /var/lib/rancher/k3s/server/manifests/expose-ssh.yaml permissions: \"0644\" owner: \"root\" content: | apiVersion: v1 kind: Secret metadata: name: ssh-entanglement namespace: kube-system type: Opaque stringData: network_token: ___GENERATED TOKEN HERE___ --- apiVersion: entangle.kairos.io/v1alpha1 kind: Entanglement metadata: name: ssh-entanglement namespace: kube-system spec: serviceUUID: \"ssh\" secretRef: \"ssh-entanglement\" host: \"127.0.0.1\" port: \"22\" hostNetwork: true In this file, you can specify various settings for your debugging station. For example, the hostname field sets the name of the machine, and the users field creates a new user with the name “kairos” and a pre-defined password and SSH key. The k3s field enables the installation of the k3s Kubernetes distribution.\nDeploy with AuroraBoot To automatically boot and install the debugging station, we can use Auroraboot. The following example shows how to use the cloud config above with it:\ncat \u003c\u003cEOF | docker run --rm -i --net host quay.io/kairos/auroraboot \\ --cloud-config - \\ --set \"container_image=quay.io/kairos/@flavor:@flavorRelease-standard-amd64-generic-master-k3sv1.33.4-k3s1\" #cloud-config install: device: \"auto\" auto: true reboot: true hostname: debugging-station-{{ trunc 4 .MachineID }} users: - name: kairos passwd: kairos groups: - admin ssh_authorized_keys: - github:mudler k3s: enabled: true # Specify the bundle to use bundles: - targets: - run://quay.io/kairos/community-bundles:cert-manager_latest - run://quay.io/kairos/community-bundles:kairos_latest kairos: entangle: enable: true write_files: - path: /var/lib/rancher/k3s/server/manifests/expose-ssh.yaml permissions: \"0644\" owner: \"root\" content: | apiVersion: v1 kind: Secret metadata: name: ssh-entanglement namespace: kube-system type: Opaque stringData: network_token: ___GENERATED TOKEN HERE___ --- apiVersion: entangle.kairos.io/v1alpha1 kind: Entanglement metadata: name: ssh-entanglement namespace: kube-system spec: serviceUUID: \"ssh\" secretRef: \"ssh-entanglement\" host: \"127.0.0.1\" port: \"22\" hostNetwork: true EOF Connecting to the cluster To connect to the cluster, we first need to open the tunnel in one terminal and then ssh from another one.\nIn one terminal, run the following command (it will run in the foreground):\n# Run in a terminal (it is foreground) export EDGEVPNTOKEN=\"___GENERATED TOKEN HERE___\" docker run -e \"EDGEVPNTOKEN=$EDGEVPNTOKEN\" --net host quay.io/mudler/edgevpn service-connect ssh 127.0.0.1:2222 In another terminal, run the following command to ssh to the box:\n# Run in another terminal ssh kairos@127.0.0.1 -p 2222 Note: it might take few attempts to establish a connection\n","categories":"","description":"Debugging station","excerpt":"Debugging station","ref":"/docs/development/debugging-station/","tags":"","title":"Debugging station"},{"body":"We like to define Kairos as a meta-Linux Distribution, as its goal is to convert other distros to an immutable layout with Kubernetes Native components.\nKairos Kairos is a software stack is composed of the following:\nA core OS image release for each flavor in ISO, qcow2, and other similar formats (see the list of supported distributions) provided for user convenience A release with K3s embedded (optional). An agent installed into the nodes to manage the node lifecycle. Every component is extensible and modular such as it can be customized and replaced in the stack and built off either locally or with Kubernetes.\nRequirements In order to convert a Linux Distribution to Kairos, the distribution must meet the following requirements:\nTrusted Boot Images Use a recent enough version of systemd (256+) as init system Use systemd-boot as bootloader Secure Boot Only Images Either use systemd or openrc init system Use grub as bootloader All If the system is meant to be used with EFI, the kernel needs to have enabled the CONFIG_EFI_STUB option ( see: https://docs.kernel.org/admin-guide/efi-stub.html) To build Kairos from scratch, see the documentation section.\nInternal components The Kairos artifacts are composed by a base OS (an upstream Linux distribution, like Ubuntu, Alpine, …) and a set of components that are installed on top of it. The components are:\nInternal:\nkairos is the main repository, building the kairos-agent and containing the image definitions which runs on our CI pipelines. immucore is the immutability management interface. kairos-agent manages the installation, reset, and upgrade of the Kairos nodes. system packages contains additional packages, cross-distro kcrypt is the component responsible for encryption and decryption of data at rest kcrypt-challenger is the kairos plugin that works with the TPM chip to unlock LUKS partitions Optional/External:\nK3s as a Kubernetes distribution edgevpn (optional) as fabric for the distributed network, node coordination and bootstrap. Provides also embedded DNS capabilities for the cluster. Internally uses libp2p for the P2P mesh capabilities. nohang A sophisticated low memory handler for Linux. entangle a CRD to interconnect Kubernetes clusters entangle-proxy a CRD to control interconnected clusters osbuilder is used to build bootable artifacts from container images AuroraBoot is the Kairos Node bootstrapper ","categories":"","description":"","excerpt":"We like to define Kairos as a meta-Linux Distribution, as its goal is …","ref":"/docs/architecture/meta/","tags":"","title":"Meta-Distribution"},{"body":" Warning Despite the Flavor you may have selected to look into the docs. The Nvidia AGX Orin only works with Ubuntu 22.04 This page describes how to install Kairos on Nvidia AGX Orin in the eMMC.\nPrerequisites Nvidia AGX Orin An USB type-C cable A Linux host used to flash the Nvidia AGX Orin board Jetson linux SDK download You can find debugging information here: https://developer.ridgerun.com/wiki/index.php/NVIDIA_Jetson_Orin/In_Board/Getting_in_Board/Serial_Console\nFlashing We are going to write the partitions in the eMMC. In order to do this we will use the Nvidia SDK configured with a custom partitioning layout.\nThe partitions are:\nOEM for storing cloud config files (/oem) COS_STATE for storing the active/passive images to boot the system EFI for storing the efi shell and grub to boot the system RECOVERY - to store the recovery system PERSISTENT - this is an optional partition to store the persistent data of the system. you can either write this in the eMMC or, for instance, to an external storage. It is enough to create a partition and label it as COS_PERSISTENT. There can be only one partition with such label, the first that matches wins. Prepare the SDK The Jetson Linux SDK is used to perform the flashing process.\nDownload the Jetson Linux SDK:\nwget https://developer.nvidia.com/downloads/embedded/l4t/r36_release_v4.3/release/Jetson_Linux_r36.4.3_aarch64.tbz2 -O tegra.bz2 tar xvf tegra.bz2 Now, we are going to prepare the rootfs and the bootloader. The Jetson Linux SDK requires the rootfs to generate the system.img file and continue the flashing process however, we will not use the image generated by the SDK as we will use a different set of images (see below). Here we also disable extlinux as Kairos uses GRUB:\ncd Linux_for_Tegra # Drop extlinux echo \"\" \u003e ./bootloader/extlinux.conf # This is needed so the SDK doesn't complain of missing files (not really used in the flash process) IMAGE=quay.io/kairos/ubuntu:22.04-core-arm64-nvidia-jetson-agx-orin-master docker run -ti --rm -v $PWD/rootfs:/rootfs quay.io/luet/base util unpack \"$IMAGE\" /rootfs # workaround needed (SDK writes to the symlink) rm rootfs/boot/initrd # Extlinux is required by the SDK - so we fake it in our root (it will not be there eventually) mkdir -p rootfs/boot/extlinux/ echo \"\" \u003e rootfs/boot/extlinux/extlinux.conf Prepare the images You can find Kairos core ubuntu images based on Ubuntu 22.04 here: https://quay.io/repository/kairos/ubuntu (search for nvidia in the tags)\nBuild partition images from a container image Build partition images from a directory If you are customizing the image, or either modifying the default partition sizes you can build the images by running:\nIMAGE=quay.io/kairos/ubuntu:22.04-core-arm64-nvidia-jetson-agx-orin-master docker run --privileged --platform=linux/arm64 \\ -e container_image=$IMAGE \\ -e STATE_SIZE=\"25500\" \\ -e RECOVERY_SIZE=\"21000\" \\ -e DEFAULT_ACTIVE_SIZE=\"7000\" \\ -v $PWD/bootloader:/bootloader --entrypoint /prepare_nvidia_orin_images.sh -ti --rm quay.io/kairos/auroraboot:v0.13.0 If you have instead the rootfs as a directory, you can create the required partitions with:\nROOTFS=/rootfs/path docker run --privileged --platform=linux/arm64 \\ -e directory=/rootfs \\ -e STATE_SIZE=\"25500\" \\ -e RECOVERY_SIZE=\"21000\" \\ -e DEFAULT_ACTIVE_SIZE=\"7000\" \\ -v $ROOTFS:/rootfs \\ -v $PWD/bootloader:/bootloader --entrypoint /prepare_nvidia_orin_images.sh -ti --rm quay.io/kairos/auroraboot:v0.13.0 After running any of the commands above, the generated images files required for flashing will be inside the bootloader directory (bootloader/efi.img, bootloader/recovery_partition.img, bootloader/state_partition.img, bootloader/oem.img, bootloader/persistent.img ).\nNote The persistent image is optional, as you can store the system persistent data rather in an SD card or an NVME disk. The default persistent.img is of 2GB size. To create a persistent image manually of the size you prefer instead you can run:\n# Create a 2GB filesystem for COS_PERSISTENT volume truncate -s $((2048*1024*1024)) bootloader/persistent.img mkfs.ext2 -L \"COS_PERSISTENT\" bootloader/persistent.img Note that the size of the partitions you modify should be duly reported in the partition layout (see below).\nEdit the parition layout We are going now to modify the partition layout in bootloader/generic/cfg/flash_t234_qspi_sdmmc.xml which corresponds to the partitioning of the AGX Orin board. An example config file can be found in here. Note that the file might change across Nvidia jetson releases, so if flashing fails, use this file as baseline.\nwget 'https://kairos.io/examples/images/flash_t234_qspi_sdmmc.xml' -O ./bootloader/generic/cfg/flash_t234_qspi_sdmmc.xml If you are editing the partition sizes and generating the images manually, use the example config file as a baseline and edit the size accordingly to the corresponding partitions (find the respective filename and compare the file size, see the notes below).\nNote on editing the parition layout manually If you want to use the original file, identify the sdmmc_user section ( e.g. \u003cdevice type=\"sdmmc_user\" instance=\"3\" sector_size=\"512\" num_sectors=\"INT_NUM_SECTORS\" \u003e ), inside there is an “APP” partition ( \u003cpartition name=\"APP\" id=\"1\" type=\"data\"\u003e ), remove it , and add the following instead:\n\u003cpartition name=\"COS_RECOVERY\" type=\"data\"\u003e \u003callocation_policy\u003e sequential \u003c/allocation_policy\u003e \u003cfilesystem_type\u003e basic \u003c/filesystem_type\u003e \u003csize\u003e 10485760000 \u003c/size\u003e \u003callocation_attribute\u003e 0x8 \u003c/allocation_attribute\u003e \u003cfilename\u003e recovery_partition.img \u003c/filename\u003e \u003cdescription\u003e \u003c/description\u003e \u003c/partition\u003e \u003cpartition name=\"COS_STATE\" type=\"data\"\u003e \u003callocation_policy\u003e sequential \u003c/allocation_policy\u003e \u003cfilesystem_type\u003e basic \u003c/filesystem_type\u003e \u003csize\u003e 14680064000 \u003c/size\u003e \u003callocation_attribute\u003e 0x8 \u003c/allocation_attribute\u003e \u003cfilename\u003e state_partition.img \u003c/filename\u003e \u003cdescription\u003e \u003c/description\u003e \u003c/partition\u003e \u003cpartition name=\"COS_OEM\" type=\"data\"\u003e \u003callocation_policy\u003e sequential \u003c/allocation_policy\u003e \u003cfilesystem_type\u003e basic \u003c/filesystem_type\u003e \u003csize\u003e 67108864 \u003c/size\u003e \u003callocation_attribute\u003e 0x8 \u003c/allocation_attribute\u003e \u003cfilename\u003e oem.img \u003c/filename\u003e \u003cdescription\u003e \u003c/description\u003e \u003c/partition\u003e \u003c!-- Optional. COS_PERSISTENT can be provided by an NVME or via SD card --\u003e \u003cpartition name=\"COS_PERSISTENT\" type=\"data\"\u003e \u003callocation_policy\u003e sequential \u003c/allocation_policy\u003e \u003cfilesystem_type\u003e basic \u003c/filesystem_type\u003e \u003csize\u003e 2147483648 \u003c/size\u003e \u003callocation_attribute\u003e 0x8 \u003c/allocation_attribute\u003e \u003cfilename\u003e persistent.img \u003c/filename\u003e \u003cdescription\u003e \u003c/description\u003e \u003c/partition\u003e Be mindful also to change the esp partition or add it if required:\n\u003cpartition name=\"esp\" type=\"data\"\u003e \u003callocation_policy\u003e sequential \u003c/allocation_policy\u003e \u003cfilesystem_type\u003e basic \u003c/filesystem_type\u003e \u003csize\u003e 20971520 \u003c/size\u003e \u003cfile_system_attribute\u003e 0 \u003c/file_system_attribute\u003e \u003cpartition_type_guid\u003e C12A7328-F81F-11D2-BA4B-00A0C93EC93B \u003c/partition_type_guid\u003e \u003callocation_attribute\u003e 0x8 \u003c/allocation_attribute\u003e \u003cpercent_reserved\u003e 0 \u003c/percent_reserved\u003e \u003cfilename\u003e efi.img \u003c/filename\u003e \u003cdescription\u003e **Required.** Contains a redundant copy of CBoot. \u003c/description\u003e \u003c/partition\u003e You can also remove the other partitions under sdmmc_user as not effectively used by Kairos during boot.\nNote The COS_PERSISTENT partition is optional. You can also use an SD card, or an nvme drive instead. The only requirement is to have the partition labeled as COS_PERSISTENT. Note If modifiying the parition sizes, you need to replace the size inside the \u003csize\u003e\u003c/size\u003e tags of each partition in the XML:\nstat -c %s bootloader/efi.img stat -c %s bootloader/recovery_partition.img stat -c %s bootloader/state_partition.img stat -c %s bootloader/oem.img stat -c %s bootloader/persistent.img Flash To flash the images to the Orin board\nPut the board in recovery mode Run: sudo ./tools/l4t_flash_prerequisites.sh # Install missing dependencies and fix file permissions sudo ./flash.sh jetson-agx-orin-devkit internal Booting The Orin board now should boot. If you are connected over the serial you can login with: kairos/kairos, similarly if you have plugged it to the network you should be able to SSH in as well.\nNotes USB Timeout error It is possible that during flashing on certain kernel versions to see an error message:\n[ 0.3623 ] tegrarcm_v2 --new_session --chip 0x23 --uid --download bct_br br_bct_BR.bct --download mb1 mb1_t234_prod_aligned_sigheader.bin.encrypt --download psc_bl1 psc_bl1_t234_prod_aligned_sigheader.bin.encrypt --download bct_mb1 mb1_bct_MB1_sigheader.bct.encrypt [ 0.3630 ] BR_CID: 0x80012344705DD25D1C00000019028240 [ 0.3932 ] Sending bct_br [ 0.4409 ] ERROR: might be timeout in USB write. [ 5.5325 ] See also the relevant Nvidia discussions in the forum:\nhttps://forums.developer.nvidia.com/t/usb-timeout-when-flashing-agx-orin/235600 https://forums.developer.nvidia.com/t/cannot-flash-jetson-os-image-to-jetson-agx-orin-devkit-via-sdk-manager/219489/2 The solution here is trying with a different kernel version, as suggested in the Nvidia threads.\nDefault configuration To customize the default cloud config of the board, generate the images mounting the cloud config you want in the images in /defaults.yaml:\nIMAGE=quay.io/kairos/ubuntu:22.04-core-arm64-nvidia-jetson-agx-orin-master CLOUD_CONFIG=/cloud/config.yaml docker run -v $CLOUD_CONFIG:/defaults.yaml --privileged \\ -e container_image=$IMAGE \\ -e STATE_SIZE=\"25500\" \\ -e RECOVERY_SIZE=\"21000\" \\ -e DEFAULT_ACTIVE_SIZE=\"7000\" \\ -v $PWD/bootloader:/bootloader --entrypoint /prepare_nvidia_orin_images.sh -ti --rm quay.io/kairos/auroraboot:v0.13.0 Debugging Use the micro USB as debug serial port with minicom to debug any booting issues.\nsudo minicom -D /dev/ttyACM0 -8 -b 115200 Flashing port In order to flash the Nvidia AGX Orin you will need to use the USB Type-C ports. The Micro USB port is reserved only for debugging over the serial console.\n","categories":"","description":"Install Kairos on Nvidia AGX Orin","excerpt":"Install Kairos on Nvidia AGX Orin","ref":"/docs/installation/nvidia_agx_orin/","tags":"","title":"Nvidia AGX Orin"},{"body":" Info Kairos supports Raspberry Pi model 3 and 4 with 64bit architecture. Warning Model 5 is currently not supported because of how we use U-boot to boot the device. There’s currently some work from the people from SUSE, see https://github.com/openSUSE/u-boot/pull/29 for more information. If you are not familiar with the process, it is suggested to follow the quickstart first to see how Kairos works.\nPrerequisites An SD card which size is at least 16 GB Etcher or dd A Linux host where to flash the device Install using AuroraBoot Create build directory and add cloud-config.yaml, then run:\ndocker run --rm --privileged -v /var/run/docker.sock:/var/run/docker.sock \\ --platform linux/arm64 -v $PWD/build/:/output \\ quay.io/kairos/auroraboot:latest \\ --debug --set \"disable_http_server=true\" --set \"disable_netboot=true\" \\ --set \"state_dir=/output\" --set \"disk.raw=true\" \\ --cloud-config /output/cloud-config.yaml \\ --set \"container_image=quay.io/kairos/@flavor:@flavorRelease-standard-arm64-rpi4-master-k3sv1.33.4-k3s1\" Once complete, use Etcher or dd to flash the image to an SD card:\nsudo dd if=build/kairos-@flavor-@flavorRelease-standard-arm64-rpi4-master-k3sv1.33.4+k3s1.raw of=\u003cdevice\u003e oflag=sync status=progress bs=10M Install using images Download Extract the img file from a container image as described in this page\nFlash the image Plug the SD card to your system. To flash the image, you can either use Etcher or dd:\nsudo dd if=kairos-@flavor-@flavorRelease-standard-arm64-rpi4-master-k3sv1.33.4+k3s1.raw of=\u003cdevice\u003e oflag=sync status=progress bs=10MB Once the image is flashed, there is no need to carry out any other installation steps, it can be booted right away. However you may want to add a cloud-config at this point - see below.\nBoot Use the SD Card to boot the device. During the first boot, the system will enter recovery mode and create the COS_STATE and COS_PERSISTENT volumes. After creating the volumes, the system will automatically reboot. This process may take a few minutes depending on the size and speed of the storage. The default username/password for the system is kairos/kairos. To configure your access or disable password change the /oem/90_custom.yaml accordingly.\nConfigure your node To configure the device beforehand, be sure to have the SD plugged in your host. We need to overwrite the default configuration file with the desired cloud-config in the COS_OEM partition:\n$ OEM=$(blkid -L COS_OEM) $ mkdir /tmp/oem $ sudo mount $OEM /tmp/oem $ sudo cp cloud-config.yaml /tmp/oem/90_custom.yaml $ sudo umount /tmp/oem You can push additional cloud config files. For a full reference check out the docs and also configuration after-installation\n","categories":"","description":"Install Kairos on RaspberryPi","excerpt":"Install Kairos on RaspberryPi","ref":"/docs/installation/raspberry/","tags":"","title":"RaspberryPi"},{"body":"Kairos has a recovery mechanism built-in which can be leveraged to restore the system to a known point. At installation time, the recovery partition is created from the installation medium and can be used to restore the system from scratch, leaving configuration intact and cleaning any persistent data accumulated by usage in the host (e.g. Kubernetes images, persistent volumes, etc. ).\nThe reset action will regenerate the bootloader configuration and the images in the state partition (labeled COS_STATE) by using the recovery image generated at install time, cleaning up the host.\nThe configuration files in /oem are kept intact, the node on the next reboot after a reset will perform the same boot sequence (again) of a first-boot installation.\nHow to Note By following the steps below you will reset entirely a node and the persistent data will be lost. This includes every user-data stored on the machine. The reset action can be accessed via the Boot menu, remotely, triggered via Kubernetes or manually. In each scenario the machine will reboot into reset mode, perform the cleanup, and reboot automatically afterwards.\nFrom the boot menu It is possible to reset the state of a node by either booting into the “Reset” mode into the boot menu, which automatically will reset the node:\nRemotely, via command line On a Kairos booted system, logged as root:\nKairos v3.0.0 and upwards Kairos before v3.0.0 To directly select the entry:\n$ kairos-agent bootentry --select statereset Or to get a list of available boot entries and select one interactively:\n$ kairos-agent bootentry $ grub2-editenv /oem/grubenv set next_entry=statereset $ reboot From Kubernetes The Kairos operator can be used to apply a NodeOp to the nodes to use Kubernetes to schedule the reset on the nodes itself, similarly on how upgrades are applied.\nConsider the following example which resets a machine by changing the config file used during installation:\nKairos v3.0.0 and upwards Kairos before v3.0.0 apiVersion: operator.kairos.io/v1alpha1 kind: NodeOp metadata: name: reset-and-reconfig namespace: default spec: # NodeSelector to target specific nodes nodeSelector: matchLabels: kairos.io/managed: \"true\" # The container image to use image: quay.io/kairos/@flavor # Custom command to execute command: - sh - -c - | set -e # Create new configuration cat \u003e /host/oem/90_custom.yaml \u003c\u003c 'EOF' #cloud-config hostname: testcluster-{{ trunc 4 .MachineID }} k3s: enabled: true users: - name: kairos passwd: kairos groups: - admin ssh_authorized_keys: - github:mudler EOF # Set next boot to reset state grub2-editenv /host/oem/grubenv set next_entry=statereset sync # Note: The reboot is handled automatically by the operator when rebootOnSuccess: true # Path where the node's root filesystem will be mounted hostMountPath: /host # Whether to cordon the node before running the operation cordon: true # Drain options for pod eviction drainOptions: enabled: true force: false gracePeriodSeconds: 30 ignoreDaemonSets: true deleteEmptyDirData: false timeoutSeconds: 300 # Whether to reboot the node after successful operation rebootOnSuccess: true # Maximum number of nodes that can run the operation simultaneously concurrency: 2 # Whether to stop creating new jobs when a job fails stopOnFailure: true --- apiVersion: v1 kind: Secret metadata: name: custom-script namespace: system-upgrade type: Opaque stringData: config.yaml: | #cloud-config hostname: testcluster-{{ trunc 4 .MachineID }} k3s: enabled: true users: - name: kairos passwd: kairos groups: - admin ssh_authorized_keys: - github:mudler add-config-file.sh: | #!/bin/sh set -e if diff /host/run/system-upgrade/secrets/custom-script/config.yaml /host/oem/90_custom.yaml \u003e/dev/null; then echo config present exit 0 fi # we can't cp, that's a symlink! cat /host/run/system-upgrade/secrets/custom-script/config.yaml \u003e /host/oem/90_custom.yaml kairos-agent bootentry --select statereset sync mount --rbind /host/dev /dev mount --rbind /host/run /run nsenter -i -m -t 1 -- reboot exit 1 --- apiVersion: upgrade.cattle.io/v1 kind: Plan metadata: name: reset-and-reconfig namespace: system-upgrade spec: concurrency: 2 # This is the version (tag) of the image. version: \"@flavorRelease-standard-amd64-generic-master-k3sv1.33.4-k3s1\" nodeSelector: matchExpressions: - { key: kubernetes.io/hostname, operator: Exists } serviceAccountName: system-upgrade cordon: false upgrade: # Here goes the image which is tied to the flavor being used. # Currently can pick between opensuse and alpine image: quay.io/kairos/@flavor command: - \"/bin/bash\" - \"-c\" args: - bash /host/run/system-upgrade/secrets/custom-script/add-config-file.sh secrets: - name: custom-script path: /host/run/system-upgrade/secrets/custom-script Manual reset It is possible to trigger the reset manually by logging into the recovery from the boot menu and running kairos reset from the console.\nCleaning up state directories An alternative way and manual of resetting your system is possible by deleting the state paths. You can achieve this by deleting the contents of the /usr/local directory. It’s recommended that you do this while in recovery mode with all services turned off.\nPlease note that within /usr/local, there are two important folders to keep in mind. The first is /usr/local/.kairos, which contains sentinel files that will trigger a complete deployment from scratch when deleted. However, your data will be preserved. The second folder is /usr/local/.state, which contains the bind-mounted data for the system. By deleting these two folders, you can achieve a pristine environment while leaving all other contents of /usr/local untouched.\n","categories":"","description":"Discover how to reset a Kairos node using boot options, Kubernetes integration, or recovery tools while preserving config data.","excerpt":"Discover how to reset a Kairos node using boot options, Kubernetes …","ref":"/docs/reference/reset/","tags":"","title":"Reset a node"},{"body":"","categories":"","description":"Deep dive into Kairos architecture. Here you will learn in detail how the main features of Kairos have been designed.\n","excerpt":"Deep dive into Kairos architecture. Here you will learn in detail how …","ref":"/docs/architecture/","tags":"","title":"Architecture"},{"body":"Boot Assessment in Kairos: Introduction and Extensions Kairos provides a robust mechanism for assessing the success or failure of boot entries through integration with systemd-boot. This document is divided into two parts:\nKairos Default Boot Assessment Strategy: Explains how boot assessment is managed in a standard Kairos installation. Extending the Default Boot Assessment: Shows how to customize and extend Kairos boot assessment by integrating additional systemd services and adding automatic reboot mechanisms. Part 1: Kairos Default Boot Assessment Strategy Kairos uses systemd-boot to manage boot entries and determine their health based on runtime behavior. The current boot assessment strategy in Kairos works as follows:\nBoot Entry Marking:\nIf the system successfully reaches multi-user.target, the boot entry is marked as good. If it does not, retries are consumed for the boot entry. Failure Handling:\nKernel panics, initramfs failures, or any unexpected reboots reduce the retry count for the current boot entry. No Kairos services explicitly auto-reboot on failure. Retry Exhaustion and Fallbacks:\nEach boot entry is given a limited number of retries (3 by default). If retries are exhausted: The passive (fallback) entry is booted next. If the passive entry also fails, the system boots into recovery mode. If recovery fails, the system attempts autorecovery. Current boot fallback behaviour is not set in stone yet and prone to changes in the future. This default behavior ensures resilience and an automatic progression to recovery states, but it can be further extended to incorporate custom services and automatic reboot logic.\nPart 2: Extending the Default Boot Assessment with Your Own Services While the default Kairos behavior is sufficient for many use cases, you can extend the boot assessment mechanism to include custom services and additional robustness features, such as:\nIntegrating custom systemd services into the boot assessment process. Adding automatic reboot behavior for services that fail. All the commands shown in this tutorial are meant to be run on a Kairos node. Step 1: Configuring a Service to Trigger boot-complete.target To ensure a service’s failure impacts the boot assessment, modify its service file to interact with boot-complete.target:\nEdit the Service File:\nOverride the service configuration using:\nsudo systemctl edit \u003cservice-name\u003e Add Dependencies and Order to the Service File:\nAppend the following to the override file:\n[Unit] # Ensure this unit starts after default system targets After=default.target graphical.target multi-user.target # Ensure this unit completes before boot-complete.target Before=boot-complete.target [Install] # Make this service a hard dependency of boot-complete.target RequiredBy=boot-complete.target Reload Systemd and Enable the Service::\nsudo systemctl daemon-reload sudo systemctl enable \u003cservice-name\u003e Explanation:\nThe service runs after critical system targets (e.g., default.target) to ensure the system is operational. The service must complete successfully to allow boot-complete.target to be reached. If the service fails, the boot entry is not marked as good. Step 2: Adding Automatic Reboot to a Service To configure a service to automatically reboot the system upon failure:\nEdit the Service File:\nOverride the service configuration using:\nsudo systemctl edit \u003cservice-name\u003e Add the Reboot Action:\nIn the [Unit] section, add:\n[Unit] FailureAction=reboot Reload Systemd:\nsudo systemctl daemon-reload Explanation:\nOn failure, FailureAction=reboot instructs systemd to reboot the system. This causes the boot entry to retry until success or retries are exhausted. Step 3: Combining Both Approaches While the above configurations are independent, combining them can create a robust system:\nTrigger boot-complete.target: Configure services as described in Step 1 to impact boot assessment.\nEnable Automatic Reboot: Add FailureAction=reboot to relevant services as described in Step 2.\nBehavior:\nOn a service failure, the system reboots (FailureAction=reboot). During the retry, if boot-complete.target is not reached, the boot entry is not marked as good, and retries continue. If retries are exhausted, the system attempts the next available boot entry. Using cloud configs to automate the process As usual you can use https://kairos.io/docs/architecture/cloud-init/ with the different https://kairos.io/docs/reference/stage_modules/) to automate this process. Here is an example of how to use cloud-init to enable boot assessment and configure services to participate in the boot assessment process:\n#cloud-config name: Enable Boot assessment for \u003cservice-name\u003e stages: initramfs: - name: Configure service to trigger boot-complete.target files: - path: /etc/systemd/system/\u003cservice-name\u003e.service.d/override.conf permissions: 0644 owner: 0 group: 0 content: | [Unit] # Ensure this unit starts after default system targets After=default.target graphical.target multi-user.target # Ensure this unit completes before boot-complete.target Before=boot-complete.target # If auto reboot on service failure is wanted FailureAction=reboot [Install] # Make this service a hard dependency of boot-complete.target RequiredBy=boot-complete.target - name: Enable service systemctl: enabled: - \u003cservice-name\u003e Notes We expect the number of checks for a system to be marked “good” to keep growing as we add more checks to the boot assessment process. Services are started on both passive and active boot entries. So if a service is failing on active, and the failure is not due to the OS, it will also fail on passive. This can lead to the system rebooting on passive boot entries as well as active and end in the system booting to recovery. We recommend using this feature with caution, as it can lead to a boot loop if not configured correctly. Ideally, as the upgrade is done against the active images, we would recommend having 2 service overrides, one for the active and one for the passive, to avoid the system rebooting on passive boot entries and having a safe fallback to the active boot entry. This can be achieved by using and IF stanza when using cloud-init to check for the system state (marked by the files /run/cos/active_mode and /run/cos/passive_mode) so the service that auto reboots can be started only on the active boot entry. The follow up example uses https://kairos.io/docs/architecture/cloud-init/ to generate 2 different service overrides during initramfs, one for the active and one for the passive boot entry. Only when selecting the active entry will the service auto restart:\n#cloud-config name: Enable Boot assessment for \u003cservice-name\u003e stages: initramfs: - name: Configure service to trigger boot-complete.target on active if: '[ ! -f \"/run/cos/active_mode\" ]' files: - path: /etc/systemd/system/\u003cservice-name\u003e.service.d/override.conf permissions: 0644 owner: 0 group: 0 content: | [Unit] # Ensure this unit starts after default system targets After=default.target graphical.target multi-user.target # Ensure this unit completes before boot-complete.target Before=boot-complete.target # If auto reboot on service failure is wanted FailureAction=reboot [Install] # Make this service a hard dependency of boot-complete.target RequiredBy=boot-complete.target - name: Configure service to trigger boot-complete.target on passive if: '[ ! -f \"/run/cos/passive_mode\" ]' files: - path: /etc/systemd/system/\u003cservice-name\u003e.service.d/override.conf permissions: 0644 owner: 0 group: 0 content: | [Unit] # Ensure this unit starts after default system targets After=default.target graphical.target multi-user.target # Ensure this unit completes before boot-complete.target Before=boot-complete.target [Install] # Make this service a hard dependency of boot-complete.target RequiredBy=boot-complete.target - name: Enable service systemctl: enabled: - \u003cservice-name\u003e For more information about cloud-init in Kairos, see the Cloud-Init Architecture guide. For more information about stage modules, see the Stage Modules Reference.\n","categories":"","description":"This section describe examples on how to enable Automatic Boot Assessment with Trusted Boot in your own services.","excerpt":"This section describe examples on how to enable Automatic Boot …","ref":"/docs/examples/boot_assessment_trusted_boot/","tags":"","title":"Enabling Automatic Boot Assessment with Trusted Boot"},{"body":" Ongoing Project The Kairos factory is an ongoing project. Things might change, and we are working on improving the documentation and the process. If you encounter any issues, please feel free to open up issues and help us improve the Documentation!\nFor further info check out #1914\nKairos is not just an OS, it’s also a way to turn an existing OS into a Kairos-ready image. This process is called “Kairosification” and it’s done by the Kairos Factory.\nFor the newcomer or someone who simply needs an immutable OS with k3s and edgeVPN, the Kairos OS is the way to go. As long as this components work, you don’t need to worry about the changes in the underlying OS. However, if you need to ensure certain packages are present or remain stable in your system, you can use the Kairos Factory to convert your base image into a Kairos-ready image. This is particularly useful if you have special firmware requirements, or if you want to have your own release cadence.\nRequirements In order to run the Kairos Factory, you will need docker installed on your system. You can find the installation instructions here. The Kairos Factory Process The Kairos factory is a single step process applied on a container image. All you need to do is run kairos-init in your Dockerfile. Optionally, you can use auroraboot.md to generate artifacts (isos, raw images, etc..) based on the generated OCI artifact.\nWhat is Kairos-init? kairos-init is a tool designed to facilitate the initialization and customization of Kairos-based images. The primary purpose of kairos-init is to convert an existing base image into a Kairos-ready image.\nkairos-init should be available to plug into your existing dockerfile and would allow you to only use docker to generate valid Kairos compatible artifacts\nPlatforms Note that as we are using standard docker tools, the platform to build for is provided by docker, either by the default value or by the --platform setting when building images. The platform to build the Kairos OCI artifacts is based on that, so running it under an arm64 platform will build and arm64 Kairos artifact.\nSee the docker multi-platform docs for more info.\nHow to use Create a single Dockerfile:\nFROM quay.io/kairos/kairos-init:v0.5.20 AS kairos-init FROM ubuntu:24.04 ARG VERSION=1.0.0 RUN --mount=type=bind,from=kairos-init,src=/kairos-init,dst=/kairos-init /kairos-init --version \"${VERSION}\" The only required argument for kairos-init is the version, which will be set under the /etc/kairos-release values to track the artifacts version so you can upgrade to those and track changes.\nThen you can just build it like any other Dockerfile ever:\n$ docker build -t ubuntu-kairos:24.04 . [+] Building 69.9s (10/10) FINISHED docker:default =\u003e [internal] load build definition from Dockerfile 0.0s =\u003e =\u003e transferring dockerfile: 240B 0.0s =\u003e [internal] load metadata for docker.io/library/ubuntu:24.04 0.0s =\u003e [internal] load metadata for quay.io/kairos/kairos-init:v0.5.0 0.4s =\u003e [internal] load .dockerignore 0.0s =\u003e =\u003e transferring context: 2B 0.0s =\u003e CACHED [kairos-init 1/1] FROM quay.io/kairos/kairos-init:v0.5.0@sha256:8d6a0000b6dfcf905eceeb 0.0s =\u003e [stage-1 1/4] FROM docker.io/library/ubuntu:24.04 0.0s =\u003e CACHED [stage-1 2/4] COPY --from=kairos-init /kairos-init /kairos-init 0.0s =\u003e [stage-1 3/4] RUN /kairos-init --version \"1.0.0\" 66.7s =\u003e [stage-1 4/4] RUN rm /kairos-init 0.1s =\u003e exporting to image 2.6s =\u003e =\u003e exporting layers 2.6s =\u003e =\u003e writing image sha256:78d8ba90a19bfa472438f207002a0ba2178917ab4c5190c3b2146f1964bac6dc 0.0s =\u003e =\u003e naming to docker.io/library/ubuntu-kairos:24.04 0.0s That will give you a nice local image tagged ubuntu-kairos:24.04 that can be feed to auroraboot.md to generate an ISO, Trusted Boot artifacts or Cloud Images.\nFor example:\n$ docker run --rm -v /var/run/docker.sock:/var/run/docker.sock -v $PWD/build/:/output \\ quay.io/kairos/auroraboot:v0.13.0 build-iso --output /output/ docker:ubuntu-kairos:24.04 2025-02-27T13:33:42Z INF Copying ubuntu-kairos:24.04 source to /output/temp-rootfs 2025-02-27T13:33:47Z INF Finished copying ubuntu-kairos:24.04 into /output/temp-rootfs 2025-02-27T13:33:47Z INF Preparing squashfs root... 2025-02-27T13:33:47Z INF Copying /output/temp-rootfs source to /tmp/auroraboot-iso220411365/rootfs 2025-02-27T13:33:47Z INF Starting rsync... 2025-02-27T13:33:49Z INF Finished syncing 2025-02-27T13:33:49Z INF Finished copying /output/temp-rootfs into /tmp/auroraboot-iso220411365/rootfs 2025-02-27T13:33:49Z INF Preparing ISO image root tree... 2025-02-27T13:33:49Z INF Copying /tmp/geniso2408076146 source to /tmp/auroraboot-iso220411365/iso 2025-02-27T13:33:49Z INF Starting rsync... 2025-02-27T13:33:49Z INF Finished syncing 2025-02-27T13:33:49Z INF Finished copying /tmp/geniso2408076146 into /tmp/auroraboot-iso220411365/iso 2025-02-27T13:33:49Z INF Creating EFI image... 2025-02-27T13:33:49Z INF Detected Flavor: ubuntu 2025-02-27T13:33:49Z INF Ubuntu based ISO detected, copying grub.cfg to /EFI/ubuntu/grub.cfg 2025-02-27T13:33:49Z INF Creating file system image /tmp/auroraboot-iso220411365/iso/boot/uefi.img with size 4Mb 2025-02-27T13:33:49Z INF Creating squashfs... 2025-02-27T13:33:53Z INF Creating ISO image... $ ls build config.yaml kairos.iso kairos.iso.sha256 netboot temp-rootfs Bind mount the binary instead of copying? As you will see over the examples, we do not copy the kairos-init binary into the image, but rather we bind mount it from the kairos-init image. This is in order to save space due to how docker works with layers and caching. If you want to copy the binary instead, you can use the COPY --from=kairos-init /kairos-init /kairos-init command instead of the RUN --mount=type=bind,from=kairos-init,src=/kairos-init,dst=/kairos-init /kairos-init command, the output will be the same but the end image will be a bit larger. Why the Version Matters When using kairos-init, the –version argument you set isn’t just cosmetic — it defines the version metadata for the image you’re building. This version is embedded into /etc/kairos-release inside the image, and it becomes critical for:\nUpgrade management: Kairos upgrade tooling checks versions to decide when and how to upgrade systems safely.\nTracking changes: It helps users, automation, and debugging processes know exactly what version of a system they are running.\nCompatibility validation: Different components, like trusted boot artifacts or upgrade servers, rely on accurate versioning to operate properly.\nImportant! Kairos Factory prepares base artifacts. It’s the responsibility of the derivative project or user (you!) to define and manage the versioning of your images. The only requirement is that versions must follow Semantic Versioning (semver.org) conventions to ensure upgrades and compatibility checks work predictably. Different users may adopt different strategies:\nA project building nightly or weekly Kairos images might automatically bump the patch or minor version each time, pulling in the latest OS package updates and security fixes.\nAnother team might maintain stable, long-lived releases, only issuing a new version every six months after extensive testing, validation, and certification.\nBoth are perfectly valid. What matters is that you track and manage your own version history, ensuring each new artifact has a clear and correct version that reflects its expected upgrade and compatibility behavior.\nIf you don’t set a meaningful version when running kairos-init, you risk confusing upgrade flows, making troubleshooting harder, and potentially breaking compatibility guarantees for users and automated systems.\nKairos releases Kairos releases its own artifacts with our own cadence, as we are also consumers of kairos-init. We use the same recommendations as above for our own “vanilla” Kairos releases. Configuration kairos-init can generate both core and standard images, and standard images can be bundled with either k3s or k0s and any version of the software that you want.\nIt can also prepare OCI artifacts for Trusted Boot which are slimmer than the usual ones, as they have size limitations plus we dont want to ship things like grub or dracut in them as they are useless.\nHere is a list of flags, explanation and what are the possible and default values\nFlag Explanation Possible values Default value -v Set version of the artifact that we are building Any None (REQUIRED) -l Sets the log level info,warn,debug,trace info -s Sets the stage to run install, init, all all -m Sets the model generic, rpi3, rpi4 generic -k Sets the Kubernetes provider k3s,k0s None –k8sversion Set the Kubernetes version to use for the given provider Any valid provider version Latest -t Sets Trusted Boot on true,false false –fips Use FIPS 140-2 compliance packages for images bool false -x Enable the loading of stage extensions bool false –skip-steps Skip the given steps during the image build Steps or Stages None You can provide a generic Dockerfile that gets all this values and passes them down into kairos-init like we do under Kairos:\nARG BASE_IMAGE=ubuntu:20.04 ARG KAIROS_INIT=v0.6.0-RC1 FROM quay.io/kairos/kairos-init:${KAIROS_INIT} AS kairos-init FROM ${BASE_IMAGE} AS base-kairos ARG MODEL=generic ARG TRUSTED_BOOT=false ARG KUBERNETES_DISTRO ARG KUBERNETES_VERSION ARG VERSION RUN --mount=type=bind,from=kairos-init,src=/kairos-init,dst=/kairos-init \\ if [ -n \"${KUBERNETES_DISTRO}\" ]; then \\ K8S_FLAG=\"-p ${KUBERNETES_DISTRO}\"; \\ if [ \"${KUBERNETES_DISTRO}\" = \"k0s\" ] \u0026\u0026 [ -n \"${KUBERNETES_VERSION}\" ]; then \\ K8S_VERSION_FLAG=\"--provider-k0s-version \\\"${KUBERNETES_VERSION}\\\"\"; \\ elif [ \"${KUBERNETES_DISTRO}\" = \"k3s\" ] \u0026\u0026 [ -n \"${KUBERNETES_VERSION}\" ]; then \\ K8S_VERSION_FLAG=\"--provider-k3s-version \\\"${KUBERNETES_VERSION}\\\"\"; \\ else \\ K8S_VERSION_FLAG=\"\"; \\ fi; \\ else \\ K8S_FLAG=\"\"; \\ K8S_VERSION_FLAG=\"\"; \\ fi; \\ eval /kairos-init -l debug -s install -m \\\"${MODEL}\\\" -t \\\"${TRUSTED_BOOT}\\\" ${K8S_FLAG} ${K8S_VERSION_FLAG} --version \\\"${VERSION}\\\" \u0026\u0026 \\ eval /kairos-init -l debug -s init -m \\\"${MODEL}\\\" -t \\\"${TRUSTED_BOOT}\\\" ${K8S_FLAG} ${K8S_VERSION_FLAG} --version \\\"${VERSION}\\\" K8s versions When selecting a k8s provider, the produced image will contain the latest published version of that provider and the Kairos provider for kubernetes. If you want to override the version installed see the flag --k8sversion Phases kairos-init is divided in 2 phases, one its the install phase which install all needed packages and binaries and the other is the init phase, which gets the system ready. This are the main parts of each phase:\nInstall:\nInstall required packages via system package manager Install the kernel Install the Kairos binaries, like the agent or immucore Install the Kairos configurations, like oem yip configs, initrd configs, etc.. Install the Kairos’ provider, including a k8s distribution and tools if requested Init:\nFill the /etc/kairos-release data (needed for upgrades, grub booting, kernel cmdline, etc…) Remove Old kernel links and duplicates Get and link kernel to /boot/vmlinuz Remove all existing initrds Build a new initrd and link it to /boot/initrd Enable/disable needed services Run some workarounds (grub vs grub2 naming for example) Run system cleanup to avoid leftovers As you can see, both of these stages runs separately so you can hijack this in the middle and add or remove things.\nFor example, it’s possible to add extra modules to be added to the initrd, or a specific kernel instead of the default latest one. Extra services can be either added or made required, etc..\nThe separation it’s also very useful for caching, as once the install phase has been cached by docker we can modify the following steps to fix any issues before the init phase is run, without removing the cache. This is particularly useful under cross platform builds where speed can take a big impact depending on your setup.\nSkipping steps and stages We recognize that one size does not fit all, and sometimes you may want to skip certain steps or stages during the image build process. To accommodate this, kairos-init provides the --skip-steps flag.\nThis is useful if you want to customize the image yourself and find that some steps collide with your customization. You can choose between install and init to skip those full stages or go into specific steps.\nRun kairos-init steps-info to see the available steps and their descriptions. You can pass more than one step, separated by comma, to skip multiple steps, for example: --skip-steps installPackages,kernel.\nThis is an example output of the stages and steps available. We recommend you run kairos-init steps-info to see the updated list of steps and stages available, as this is subject to change:\nFROM quay.io/kairos/kairos-init:v0.5.20 AS kairos-init FROM ubuntu:24.04 RUN --mount=type=bind,from=kairos-init,src=/kairos-init,dst=/kairos-init /kairos-init steps-info 2025-07-07T09:53:05Z INF [7] Starting kairos-init version 59628ad 2025-07-07T09:53:05Z INF [7] Step name \u0026 Description 2025-07-07T09:53:05Z INF [7] -------------------------------------------------------- 2025-07-07T09:53:05Z INF [7] \"branding\": applies the branding for the system 2025-07-07T09:53:05Z INF [7] \"cleanup\": cleans up the system of unneeded packages and files 2025-07-07T09:53:05Z INF [7] \"cloudconfigs\": installs the cloud-configs for the system 2025-07-07T09:53:05Z INF [7] \"grub\": configures the grub bootloader 2025-07-07T09:53:05Z INF [7] \"init\": The full init stage, which includes kairosRelease, kubernetes, initrd, services, workarounds and cleanup steps 2025-07-07T09:53:05Z INF [7] \"initramfsConfigs\": configures the initramfs for the system 2025-07-07T09:53:05Z INF [7] \"initrd\": generates the initrd 2025-07-07T09:53:05Z INF [7] \"install\": The full install stage, which includes installPackages, kubernetes, cloudconfigs, branding, grub, services, kairosBinaries, providerBinaries, initramfsConfigs and miscellaneous steps 2025-07-07T09:53:05Z INF [7] \"installKernel\": installs the kernel packages 2025-07-07T09:53:05Z INF [7] \"installPackages\": installs the base system packages 2025-07-07T09:53:05Z INF [7] \"kairosBinaries\": installs the kairos binaries 2025-07-07T09:53:05Z INF [7] \"kairosRelease\": creates and fills the /etc/kairos-release file 2025-07-07T09:53:05Z INF [7] \"kernel\": installs the kernel 2025-07-07T09:53:05Z INF [7] \"kubernetes\": installs the kubernetes provider 2025-07-07T09:53:05Z INF [7] \"miscellaneous\": applies miscellaneous configurations 2025-07-07T09:53:05Z INF [7] \"providerBinaries\": installs the kairos provider binaries for k8s 2025-07-07T09:53:05Z INF [7] \"services\": creates and enables required services 2025-07-07T09:53:05Z INF [7] \"workarounds\": applies workarounds for known issues Extending stages with custom actions This allows to load stage extensions from a dir in the filesystem to expand the default stages with custom logic.\nYou can enable this feature by using the -x flag\nThe structure is as follows:\nWe got a base dir which is /etc/kairos-init/stage-extensions (this is the default, but you can override it using the KAIROS_INIT_STAGE_EXTENSIONS_DIR env var)\nYou can drop your custom yip files and as usual, they will be loaded and executed in lexicographic order.\nSo for example, if we have:\n/etc/kairos-init/stage-extensions/10-foo.yaml /etc/kairos-init/stage-extensions/20-bar.yaml /etc/kairos-init/stage-extensions/30-baz.yaml The files will be loaded in the following order:\n10-foo.yaml 20-bar.yaml 30-baz.yaml The files are loaded using the yip library, so you can use all the features of yip to expand the stages.\nThe current stages available are:\nbefore-install: Good for adding extra repos and such. install: Good for installing packages and such. after-install: Do some cleanup of packages, add extra packages, add different kernels and remove the kairos default one, etc. before-init: Good for adding some dracut modules for example to be added to the initramfs. init: Anything that configures the system, security hardening for example. after-init: Good for rebuilding the initramfs, or adding a different initramfs like a kdump one, add grub configs or branding, etc. So for example, if we were to add an extra repo for zfs and install the package we could do the following:\n/etc/kairos-init/stage-extensions/10-zfs.yaml\nstages: after-install: - files: - path: /etc/apt/sources.list.d/zfs.list permissions: 0644 owner: 0 group: 0 content: | deb http://deb.debian.org/debian bookworm-backports main contrib deb-src http://deb.debian.org/debian bookworm-backports main contrib - packages: install: - \"zfs-dkms\" - \"zfsutils-linux\" refresh: true This would run the before-install and install stages as normal, but then on the after-install stage it would add the zfs repo and install the zfs packages.\nValidation You can validate the image you built using the kairos-init validate command inside the image. This will check if the image is valid and if it has all the necessary components to run Kairos.\nBuilding RHEL images Before running kairos-init, you need to register the system with the subscription manager and attach a subscription to it. You can do this by modifying the Dockerfile to register the system before running kairos-init:\nFROM quay.io/kairos/kairos-init:v0.5.20 AS kairos-init FROM redhat/ubi9 RUN subscription-manager register --username \u003cyour-username\u003e --password \u003cyour-password\u003e RUN --mount=type=bind,from=kairos-init,src=/kairos-init,dst=/kairos-init /kairos-init Examples All the examples are using the kairos default dockerfile as the base Dockerfile and its reproduced below:\nARG BASE_IMAGE=ubuntu:20.04 ARG KAIROS_INIT=v0.6.0-RC1 FROM quay.io/kairos/kairos-init:${KAIROS_INIT} AS kairos-init FROM ${BASE_IMAGE} AS base-kairos ARG MODEL=generic ARG TRUSTED_BOOT=false ARG KUBERNETES_DISTRO ARG KUBERNETES_VERSION ARG VERSION RUN --mount=type=bind,from=kairos-init,src=/kairos-init,dst=/kairos-init \\ if [ -n \"${KUBERNETES_DISTRO}\" ]; then \\ K8S_FLAG=\"-p ${KUBERNETES_DISTRO}\"; \\ if [ \"${KUBERNETES_DISTRO}\" = \"k0s\" ] \u0026\u0026 [ -n \"${KUBERNETES_VERSION}\" ]; then \\ K8S_VERSION_FLAG=\"--provider-k0s-version \\\"${KUBERNETES_VERSION}\\\"\"; \\ elif [ \"${KUBERNETES_DISTRO}\" = \"k3s\" ] \u0026\u0026 [ -n \"${KUBERNETES_VERSION}\" ]; then \\ K8S_VERSION_FLAG=\"--provider-k3s-version \\\"${KUBERNETES_VERSION}\\\"\"; \\ else \\ K8S_VERSION_FLAG=\"\"; \\ fi; \\ else \\ K8S_FLAG=\"\"; \\ K8S_VERSION_FLAG=\"\"; \\ fi; \\ eval /kairos-init -l debug -s install -m \\\"${MODEL}\\\" -t \\\"${TRUSTED_BOOT}\\\" ${K8S_FLAG} ${K8S_VERSION_FLAG} --version \\\"${VERSION}\\\" \u0026\u0026 \\ eval /kairos-init -l debug -s init -m \\\"${MODEL}\\\" -t \\\"${TRUSTED_BOOT}\\\" ${K8S_FLAG} ${K8S_VERSION_FLAG} --version \\\"${VERSION}\\\" You can see more examples in the Kairos repo.\nBuild rpi4 k3s oci artifacts (arm64 platform) Based on Alpine 3.19:\n$ docker build --platform=linux/arm64 -t alpine-rpi:3.19 --build-arg MODEL=rpi4 --build-arg BASE_IMAGE=alpine:3.19 --build-arg VERSION=v1.0.0 . Based on Ubuntu 22.04:\n$ docker build --platform=linux/arm64 -t ubuntu-rpi:22.04 --build-arg MODEL=rpi4 --build-arg BASE_IMAGE=ubuntu:22.04 --build-arg VERSION=v1.0.0 . Add a specific kernel to ubuntu 24.04 (amd64 platform) FROM quay.io/kairos/kairos-init:v0.5.20 AS kairos-init FROM ubuntu:24.04 RUN --mount=type=bind,from=kairos-init,src=/kairos-init,dst=/kairos-init /kairos-init -l debug -s install --version \"v0.0.1\" --skip-steps installKernel # Install Smaller virtual kernel, useful for testing things in VMS RUN apt-get install -y linux-image-virtual # Run the init phase as normal, it will find the new kernel and link it + generate initrd RUN --mount=type=bind,from=kairos-init,src=/kairos-init,dst=/kairos-init /kairos-init -l debug -s init --version \"v0.0.1\" $ docker build -t ubuntu-kairos-virtual:24.04 . [+] Building 106.1s (13/13) FINISHED docker:default =\u003e [internal] load build definition from Dockerfile 0.0s =\u003e =\u003e transferring dockerfile: 629B 0.0s =\u003e [internal] load metadata for docker.io/library/ubuntu:24.04 0.0s =\u003e [internal] load metadata for quay.io/kairos/kairos-init:v0.5.0 0.2s =\u003e [internal] load .dockerignore 0.0s =\u003e =\u003e transferring context: 2B 0.0s =\u003e CACHED [kairos-init 1/1] FROM quay.io/kairos/kairos-init:v0.5.0@sha256:8d6a0000b6dfcf905eceeb 0.0s =\u003e [stage-1 1/7] FROM docker.io/library/ubuntu:24.04 0.0s =\u003e CACHED [stage-1 2/7] COPY --from=kairos-init /kairos-init /kairos-init 0.0s =\u003e [stage-1 3/7] RUN /kairos-init -l debug -s install --version \"v0.0.1\" 74.8s =\u003e [stage-1 4/7] RUN apt-get remove -y linux-base linux-image-generic-hwe-24.04 \u0026\u0026 apt-get autor 2.3s =\u003e [stage-1 5/7] RUN apt-get install -y linux-image-virtual 8.3s =\u003e [stage-1 6/7] RUN /kairos-init -l debug -s init --version \"v0.0.1\" 17.9s =\u003e [stage-1 7/7] RUN rm /kairos-init 0.1s =\u003e exporting to image 2.6s =\u003e =\u003e exporting layers 2.6s =\u003e =\u003e writing image sha256:94a792dad87629860094820860d68e8d0587bd758d537835d9c5ae7c476af71c 0.0s =\u003e =\u003e naming to docker.io/library/ubuntu-kairos-virtual:24.04 Build Trusted Boot images (core and standard) Core:\n$ docker build -t ubuntu-kairos-trusted-core:24.04 --build-arg BASE_IMAGE=ubuntu:24.04 --build-arg TRUSTED_BOOT=true --build-arg VERSION=v1.0.0 . [+] Building 64.0s (12/12) FINISHED docker:default =\u003e [internal] load build definition from Dockerfile 0.0s =\u003e =\u003e transferring dockerfile: 717B 0.0s =\u003e [internal] load metadata for docker.io/library/ubuntu:24.04 0.0s =\u003e [internal] load metadata for quay.io/kairos/kairos-init:v0.5.0 0.6s =\u003e [internal] load .dockerignore 0.0s =\u003e =\u003e transferring context: 2B 0.0s =\u003e CACHED [kairos-init 1/1] FROM quay.io/kairos/kairos-init:v0.5.0@sha256:8d6a0000b6dfcf905eceeb 0.0s =\u003e [base-kairos 1/6] FROM docker.io/library/ubuntu:24.04 0.0s =\u003e CACHED [base-kairos 2/6] COPY --from=kairos-init /kairos-init /kairos-init 0.0s =\u003e [base-kairos 3/6] RUN /kairos-init -l debug -s install -m \"generic\" -t \"true\" -k \"${KUBERNET 58.3s =\u003e [base-kairos 4/6] RUN /kairos-init -l debug -s init -m \"generic\" -t \"true\" -k \"${KUBERNETES_D 2.5s =\u003e [base-kairos 5/6] RUN /kairos-init validate -t \"true\" 0.2s =\u003e [base-kairos 6/6] RUN rm /kairos-init 0.1s =\u003e exporting to image 2.3s =\u003e =\u003e exporting layers 2.3s =\u003e =\u003e writing image sha256:5ca83ab1eaa4b16210e243f6f9b30e7721e2be0d55b47ae4bd178939e5a44d0f 0.0s =\u003e =\u003e naming to docker.io/library/ubuntu-kairos-trusted-core:24.04 0.0s Standard, default latest k3s:\n$ docker build -t ubuntu-kairos-trusted-standard:24.04 --build-arg BASE_IMAGE=ubuntu:24.04 --build-arg TRUSTED_BOOT=true --build-arg VERSION=v1.0.0 --build-arg KUBERNETES_DISTRO=k3s . [+] Building 51.4s (12/12) FINISHED docker:default =\u003e [internal] load build definition from Dockerfile 0.0s =\u003e =\u003e transferring dockerfile: 717B 0.0s =\u003e [internal] load metadata for docker.io/library/ubuntu:24.04 0.0s =\u003e [internal] load metadata for quay.io/kairos/kairos-init:v0.5.0 0.4s =\u003e [internal] load .dockerignore 0.0s =\u003e =\u003e transferring context: 2B 0.0s =\u003e CACHED [kairos-init 1/1] FROM quay.io/kairos/kairos-init:v0.5.0@sha256:8d6a0000b6dfcf905eceeb 0.0s =\u003e [base-kairos 1/6] FROM docker.io/library/ubuntu:24.04 0.0s =\u003e CACHED [base-kairos 2/6] COPY --from=kairos-init /kairos-init /kairos-init 0.0s =\u003e [base-kairos 3/6] RUN /kairos-init -l debug -s install -m \"generic\" -t \"true\" -k \"k3s\" --k8s 45.0s =\u003e [base-kairos 4/6] RUN /kairos-init -l debug -s init -m \"generic\" -t \"true\" -k \"k3s\" --k8svers 3.1s =\u003e [base-kairos 5/6] RUN /kairos-init validate -t \"true\" 0.2s =\u003e [base-kairos 6/6] RUN rm /kairos-init 0.1s =\u003e exporting to image 2.5s =\u003e =\u003e exporting layers 2.5s =\u003e =\u003e writing image sha256:019237bde8a0a8b5cccf328c86aa2e4090525dadc4037ab6397272454dfd5e55 0.0s =\u003e =\u003e naming to docker.io/library/ubuntu-kairos-trusted-standard:24.04 0.0s Now lets build a Trusted Boot ISO from the standard image: Note that for Trusted Boot builds we need to pass the keys dir, please refer to Trusted Boot docs for more info about this.\n$ docker run --rm -v /var/run/docker.sock:/var/run/docker.sock \\ -v $PWD/e2e/assets/keys:/keys \\ -v $PWD/build/:/output \\ quay.io/kairos/auroraboot:v0.13.0 build-uki --output-dir /output/ -k /keys --output-type iso \\ docker:ubuntu-kairos-trusted-standard:24.04 2025-02-27T14:54:41Z INF Extracting image to a temporary directory 2025-02-27T14:54:41Z INF Copying ubuntu-kairos-trusted-standard:24.04 source to /tmp/auroraboot-build-uki-1771870410 2025-02-27T14:54:44Z INF Finished copying ubuntu-kairos-trusted-standard:24.04 into /tmp/auroraboot-build-uki-1771870410 2025-02-27T14:54:44Z INF Creating additional directories in the rootfs 2025-02-27T14:54:44Z INF Copying kernel 2025-02-27T14:54:44Z INF Creating an initramfs file `2025-02-27T14:55:09Z INF Running ukify for cmdline: Kairos: console=ttyS0 console=tty1 net.ifnames=1 rd.immucore.oemlabel=COS_OEM rd.immucore.oemtimeout=2 rd.immucore.uki selinux=0 panic=5 rd.shell=0 systemd.crash_reboot=yes install-mode 2025-02-27T14:55:09Z INF Generating: norole.efi 2025-02-27T14:55:12Z INF Creating kairos and loader conf files 2025-02-27T14:55:12Z INF Calculating the size of the img file 2025-02-27T14:55:12Z INF Creating the img file with size: 434Mb 2025-02-27T14:55:13Z INF Created image: /tmp/auroraboot-iso-dir-805355778/efiboot.img 2025-02-27T14:55:13Z INF Creating directories in the img file 2025-02-27T14:55:13Z INF Copying files in the img file 2025-02-27T14:55:13Z INF Adding files from /tmp/auroraboot-build-uki-1771870410 to iso 2025-02-27T14:55:13Z INF Copying /tmp/auroraboot-build-uki-1771870410 source to /tmp/auroraboot-iso-dir-805355778 2025-02-27T14:55:13Z INF Starting rsync... 2025-02-27T14:55:14Z INF Finished syncing 2025-02-27T14:55:14Z INF Finished copying /tmp/auroraboot-build-uki-1771870410 into /tmp/auroraboot-iso-dir-805355778 2025-02-27T14:55:14Z INF Creating the iso files with xorriso 2025-02-27T14:55:15Z INF Done building iso at: /output/ GitHub Actions Integration For users who prefer to automate their Kairos image builds using GitHub Actions, the Kairos Factory Action provides a reusable workflow that simplifies the build process.\nWhat is the Kairos Factory Action? The Kairos Factory Action is a GitHub Actions reusable workflow that automates the entire Kairos image building process. It handles:\nMulti-platform builds: Support for amd64 and arm64 architectures Multiple base images: Ubuntu, OpenSUSE, and other distributions Kubernetes integration: Built-in support for K3s and K0s distributions Artifact generation: Create ISO and RAW disk images Security scanning: Integrated Grype and Trivy vulnerability scanning Digital signing: Cosign integration for artifact signing Trusted boot: Support for UKI/USI (Unified Kernel/System Image) builds Registry publishing: Push to any container registry Basic Usage Example jobs: build: uses: kairos-io/kairos-factory-action/.github/workflows/reusable-factory.yaml@main with: version: \"v1.0.0\" base_image: \"ubuntu:24.04\" model: \"generic\" kubernetes_distro: \"k3s\" iso: true summary_artifacts: true This example builds a Kairos image based on Ubuntu 24.04 with K3s, generates an ISO artifact, and provides a rich build summary with artifact links.\nKey Features Flexible versioning: Automatic git-based versioning or manual semver Security-focused: Optional vulnerability scanning and digital signing Custom naming: Flexible tag and artifact naming formats Cloud config support: Integration with cloud-init configurations GitHub integration: Rich build summaries and optional release creation For complete documentation, configuration options, and advanced examples, visit the Kairos Factory Action repository.\nWeb UI The Kairos Factory is also available as a web UI, which is currently under development but you can already preview since version 0.6.0 of AuroraBoot.\nTo use the web UI, you need to run the AuroraBoot with the web command:\ndocker run --rm -v /var/run/docker.sock:/var/run/docker.sock \\ --privileged \\ -v $PWD/build/:/output \\ -p 8080:8080 \\ quay.io/kairos/auroraboot:v0.13.0 web If the process is successful, you will see the following output:\n____ __ / __/___/ / ___ / _// __/ _ \\/ _ \\ /___/\\__/_//_/\\___/ v4.13.3 High performance, minimalist Go web framework https://echo.labstack.com ____________________________________O/_______ O\\ ⇨ http server started on [::]:8080 From there you can access the web UI by visiting http://localhost:8080 in your browser.\nFactory API The Kairos Factory exposes a REST API that allows you to programmatically interact with the factory. The API documentation is available through a ReDoc page that is served alongside the web UI.\nTo access the API documentation, simply visit http://localhost:8080/redoc.html in your browser after starting the web server. The ReDoc page provides detailed information about:\nAvailable API endpoints Request/response formats Example requests and responses This documentation is automatically generated from the OpenAPI/Swagger specifications and is kept up-to-date with the latest API changes.\n","categories":"","description":"Converting any base image into a Kairos ready image","excerpt":"Converting any base image into a Kairos ready image","ref":"/docs/reference/kairos-factory/","tags":"","title":"The Kairos Factory"},{"body":"Introduction As more organizations seek to take advantage of the benefits of Kubernetes for their edge applications, the difficulties of managing large-scale clusters become apparent. Managing, configuring, and coordinating multiple clusters can be a complex and time-consuming process. We need solutions that offer zero-touch configuration and self-coordination.\nTo address these challenges, Kairos provides an easy and robust solution for deploying Kubernetes workloads at the edge. By utilizing peer-to-peer (p2p) technology, Kairos can automatically coordinate and create Kubernetes clusters without requiring a control management interface. This frees users up to concentrate on running and scaling their applications instead of spending time on cluster management.\nIn this document, we will examine the advantages of using Kairos to deploy Kubernetes clusters at the edge, and how p2p technology facilitates self-coordination for a zero-touch configuration experience. We will also explore how Kairos’ highly adaptable and container-based approach, combined with an immutable OS and meta-distribution, makes it an excellent choice for edge deployments.\nNote You can also watch our Kairos and libp2p video in the Media Section Overview: P2P for self-coordination Kairos creates self-coordinated, fully meshed clusters at the edge by using a combination of P2P technology, VPN, and Kubernetes.\nThis design is made up of several components:\nThe Kairos base OS with support for different distribution flavors and k3s combinations (see our support matrix here). A Virtual private network interface (EdgeVPN which leverages libp2p). K3s/CNI configured to work with the VPN interface. A shared ledger accessible to all nodes in the p2p private network. By using libp2p as the transport layer, Kairos can abstract connections between the nodes and use it as a coordination mechanism. The shared ledger serves as a cache to store additional data, such as node tokens to join nodes to the cluster or the cluster topology, and is accessible to all nodes in the P2P private network. The VPN interface is automatically configured and self-coordinated, requiring zero-configuration and no user intervention.\nMoreover, any application at the OS level can use P2P functionalities by using Virtual IPs within the VPN. The user only needs to provide a generated shared token containing OTP seeds for rendezvous points used during connection bootstrapping between the peers. It’s worth noting that the VPN is optional, and the shared ledger can be used to coordinate and set up other forms of networking between the cluster nodes, such as KubeVIP. (See this example here)\nImplementation Peer-to-peer (P2P) networking is used to coordinate and bootstrap nodes. When this functionality is enabled, there is a distributed ledger accessible over the nodes that can be programmatically accessed and used to store metadata.\nKairos can automatically set up a VPN between nodes using a shared secret. This enables the nodes to automatically coordinate, discover, configure, and establish a network overlay spanning across multiple regions. EdgeVPN is used for this purpose.\nThe private network is bootstrapped in three phases, with discovery driven by a distributed hash table (DHT) and multicast DNS (mDNS), which can be selectively disabled or enabled. The three phases are:\nDiscovery Gossip network Full connectivity During the discovery phase, which can occur via mDNS (for LAN) or DHT (for WAN), nodes discover each other by broadcasting their presence to the network.\nIn the second phase, rendezvous points are rotated by OTP (one-time password). A shared token containing OTP seeds is used to generate these rendezvous points, which serve as a secure way to bootstrap connections between nodes. This is essential for establishing a secure and self-coordinated P2P network.\nIn the third phase, a gossip network is formed among nodes, which shares shared ledger blocks symmetrically encrypted with AES. The key used to encrypt these blocks is rotated via OTP. This ensures that the shared ledger is secure and that each node has access to the most up-to-date version of the shared configuration. The ledger is used to store arbitrary metadata from the nodes of the network. On each update, a new block is created with the new information and propagated via gossip.\nOptionally, full connectivity can be established by bringing up a TUN interface, which routes packets via the libp2p network. This enables any application at the OS level to leverage P2P functionalities by using VirtualIPs accessible within the VPN.\nThe coordination process in Kairos is designed to be resilient and self-coordinated, with no need for complex network configurations or control management interfaces. By using this approach, Kairos simplifies the process of deploying and managing Kubernetes clusters at the edge, making it easy for users to focus on running and scaling their applications.\nWhy Peer-to-Peer? Kairos has chosen Peer-to-Peer as an internal component to enable automatic coordination of Kairos nodes. To understand why EdgeVPN has been selected, see the comparison table below, which compares EdgeVPN with other popular VPN solutions:\nWireguard OpenVPN EdgeVPN Memory Space Kernel-module Userspace Userspace Protocol UDP UDP, TCP TCP, UDP/quick, UDP, ws, everything supported by libp2p P2P Yes Yes Yes Fully meshed No No Yes Management Server (SPOF) Yes Yes No Self-coordinated No No Yes Key factors, such as self-coordination and the ability to share metadata between nodes, have led to the selection of EdgeVPN. However, there are tradeoffs and considerations to note in the current architecture, such as:\nRouting all traffic to a VPN can introduce additional latency Gossip protocols can be chatty, especially if using DHT, creating VPNs that span across regions EdgeVPN is in user-space, which can be slower compared to kernel-space solutions such as Wireguard For highly trafficked environments, there will be an increase in CPU usage due to the additional encryption layers introduced by EdgeVPN Nonetheless, these tradeoffs can be overcome, and new features can be added due to EdgeVPN’s design. For example:\nThere is no need for any server to handle traffic (no SPOF), and no additional configuration is necessary The p2p layer is decentralized and can span across different networks by using DHT and a bootstrap server Self-coordination simplifies the provisioning experience Internal cluster traffic can also be offloaded to other mechanisms if network performance is a prerequisite For instance, with KubeVIP, new nodes can join the network and become cluster members even after the cluster provisioning phase, making EdgeVPN a scalable solution. Why a VPN ? A VPN allows for the configuration of a Kubernetes cluster without depending on the underlying network configuration. This design model is popular in certain use cases at the edge where fixed IPs are not a viable solution. We can summarize the implications as follows:\nK8s Without VPN K8s With VPN IP management Needs to have static IP assigned by DHCP or manually configured (can be automated) Automatically coordinated Virtual IPs for nodes. Or manually assign them Network Configuration etcd needs to be configured with IPs assigned by your network/fixed Automatically assigned, fixed VirtualIPs for etcd. Networking Cluster IPs, and networking is handled by CNIs natively (no layers) Kubernetes Network services will have Cluster IPs sitting below the VPN. Every internal kubernetes communication goes through VPN. The additional e2e encrypted network layer might add additional latency, 0-1ms in LAN. The use of a VPN for a Kubernetes cluster has significant implications. With a VPN, IP management is automatic and does not require static IP addresses assigned by DHCP or manually configured. Nodes can be assigned virtual IPs that are automatically coordinated or manually assigned, which eliminates the need for manual configuration of IP addresses. Additionally, EdgeVPN implements distributed DHCP, so there are no Single point of Failures.\nAdditionally, network configuration is simplified with a VPN. Without a VPN, etcd needs to be configured with IPs assigned by your network or fixed. With a VPN, virtual IPs are automatically assigned for etcd.\nIn terms of networking, a Kubernetes cluster without a VPN handles cluster IPs and networking natively without additional layers. However, with a VPN, Kubernetes network services will have Cluster IPs below the VPN. This means that all internal Kubernetes communication goes through the VPN. While the additional end-to-end encrypted network layer might add some latency, it is observed typically to be only 0-1ms in LAN. However, due to the Encryption layers, the CPU usage might be high if used for high-demanding traffic.\nIt’s also worth noting that while a VPN provides a unified network environment, it may not be necessary or appropriate for all use cases. Users can choose to opt-out of using the VPN and leverage only the coordination aspect, for example, with KubeVIP. Ultimately, the decision to use a VPN should be based on the specific needs and requirements of your Kubernetes cluster, and as such you can just use the co-ordination aspect and leverage for instance KubeVIP.\nPacket flow The Virtual Private Network used is EdgeVPN, which leverages libp2p for the transport layer.\nTo explain how the packet flow works between two nodes, Node A and Node B, refer to the diagram below:\nWhile participating actively on a network, each node keeps the shared ledger up-to-date with information about itself and how to be reached by advertizing its own IP and the libp2p identity, allowing nodes to discover each other and how to route packets.\nAssuming that we want to establish an SSH connection from Node A to Node B through the VPN network, which exposes the sshd service, the process is as follows:\nNode A (10.1.0.1) uses ssh to dial the VirtualIP of the Node B (10.1.0.2) in the network. EdgeVPN reads the frame from the TUN interface. If EdgeVPN finds a match in the ledger between the VirtualIP and an associated Identity, it opens a p2p stream to Node B using the libp2p Identity. Node B receives the incoming p2p stream from EdgeVPN. Node B performs a lookup in the shared ledger. If a match is found, Node B routes the packet back to the TUN interface, up to the application level. Controller A set of Kubernetes Native Extensions (Entangle) provides peer-to-peer functionalities also to existing clusters by allowing to bridge connection with the same design architecture described above.\nIt can be used to:\nBridge services between clusters Bridge external connections to cluster Setup EdgeVPN as a daemonset between cluster nodes See also the Entangle documentation to learn more about it.\nBenefits The use of p2p technology to enable self-coordination of Kubernetes clusters in Kairos offers a number of benefits:\nSimplified deployment: Deploying Kubernetes clusters at the edge is greatly simplified. Users don’t need to specify any network settings or use a control management interface to set up and manage their clusters. Easy customization: Kairos offers a highly customizable approach to deploying Kubernetes clusters at the edge. Users can choose from a range of meta distributions, including openSUSE, Ubuntu, Alpine and many others, and customize the configuration of their clusters as needed. Automatic coordination: With Kairos, the coordination of Kubernetes clusters is completely automated. The p2p network is used as a coordination mechanism for the nodes, allowing them to communicate and coordinate with each other without the need for any external management interface. This means that users can set up and manage their Kubernetes clusters at the edge with minimal effort, freeing up their time to focus on other tasks. Secure and replicated: The use of rendezvous points and a shared ledger, encrypted with AES and rotated via OTP, ensures that the p2p network is secure and resilient. This is especially important when deploying Kubernetes clusters at the edge, where network conditions can be unpredictable. Resilient: Kairos ensures that the cluster remains resilient, even in the face of network disruptions or failures. By using VirtualIPs, nodes can communicate with each other without the need for static IPs, and the cluster’s etcd database remains unaffected by any disruptions. Scalable: Kairos is designed to be highly scalable. With the use of p2p technology, users can easily add or remove nodes from the cluster, without the need for any external management interface. By leveraging p2p technology, Kairos makes it easy for users to deploy and manage their clusters without the need for complex network configurations or external management interfaces. The cluster remains secure, resilient, and scalable, ensuring that it can handle the challenges of deploying Kubernetes at the edge. Conclusions In conclusion, Kairos offers an innovative approach to deploying and managing Kubernetes clusters at the edge. By leveraging peer-to-peer technology, Kairos eliminates the need for a control management interface and enables self-coordination of clusters. This makes it easier to deploy and manage Kubernetes clusters at the edge, saving users time and effort.\nThe use of libp2p, shared ledger, and OTP for bootstrapping and coordination thanks to EdgeVPN make the solution secure and resilient. Additionally, the use of VirtualIPs and the option to establish a TUN interface ensures that the solution is flexible and can be adapted to a variety of network configurations without requiring exotic configurations.\nWith Kairos, users can boost large-scale Kubernetes adoption at the edge, achieve zero-touch configuration, and have their cluster’s lifecycle completely managed, all while enjoying the benefits of self-coordination and zero network configuration. This allows users to focus on running and scaling their applications, rather than worrying about the complexities of managing their Kubernetes clusters.\n","categories":"","description":"How Kairos leverage Peer-to-peer (P2P) to self-coordinate clusters at the edge.","excerpt":"How Kairos leverage Peer-to-peer (P2P) to self-coordinate clusters at …","ref":"/docs/architecture/network/","tags":"","title":"P2P Network"},{"body":"Most hardware these days, supports booting an operating system from the network. The technology behind this is called Preboot Execution Environment. Kairos releases include artifacts to allow booting from the network. In general, the following files are needed:\nThe initrd image: It’s the system that loads first. It’s responsible to load the kernel. The kernel: This is the kernel of the operating system that will boot. The squashfs: The filesystem of the operating system that will boot. Booting using these files can happen in multiple ways:\nEither with direct support from the machine BIOS plus network configuration (DHCP server etc). Software based network booting. This works with a special ISO, built with ipxe project. Kairos releases include pre-built ISOs for netbooting (named like *.ipxe.iso.ipxe). Use AuroraBoot Generic hardware based netbooting is out of scope for this document. Below we give instructions on how to use the Kairos release artifacts to netboot and how to use AuroraBoot to boot from network.\nBoot with pre-built ISOs The ipxe ISOs from the Kairos release artifacts, were built with a ipxe script that points directly to the kernel, initrd and squashfs artifacts of the same release on GitHub.\nE.g.:\n#!ipxe set url https://github.com/kairos-io/kairos/releases/download/master set kernel kairos-@flavor-@flavorRelease-standard-amd64-generic-master-k3sv1.33.4+k3s1-kernel set initrd kairos-@flavor-@flavorRelease-standard-amd64-generic-master-k3sv1.33.4+k3s1-initrd set rootfs kairos-@flavor-@flavorRelease-standard-amd64-generic-master-k3sv1.33.4+k3s1.squashfs # Configure interface ifconf # set config https://example.com/machine-config # set cmdline extra.values=1 kernel ${url}/${kernel} initrd=${initrd} rd.neednet=1 ip=dhcp rd.cos.disable root=live:${url}/${rootfs} netboot install-mode config_url=${config} console=tty1 console=ttyS0 ${cmdline} initrd ${url}/${initrd} boot Booting the ISO will automatically download and boot those artifacts. E.g. using qemu:\n#!/bin/bash qemu-img create -f qcow2 disk.img 40g qemu-system-x86_64 \\ -m 4096 \\ -smp cores=2 \\ -nographic \\ -drive if=virtio,media=disk,file=disk.img \\ -drive if=ide,media=cdrom,file=${1:-kairos.iso} Use AuroraBoot AuroraBoot is a Kairos convinience tool that can be used to quickly deploy Kairos from Network with zero-touch configuration, for instance:\ndocker run --rm -ti --net host quay.io/kairos/auroraboot \\ --set \"container_image=quay.io/kairos/@flavor:@flavorRelease-standard-amd64-generic-master-k3sv1.33.4-k3s1\" # Optionally: # --cloud-config .... Will netboot the quay.io/kairos/@flavor:@flavorRelease-standard-amd64-generic-master-k3sv1.33.4-k3s1 image. You can find more details in the AuroraBoot documentation section.\nNotes on booting from network Another way to boot with the release artifacts is using pixiecore. pixiecore acts as a server which offers net boot files over the network and it’s automatically discovered on a network where a DHCP server is running and is compatible with the pixiecore architecture.\nAssuming the current directory has the kernel, initrd and squashfs artifacts, pixiecore server can be started with docker like this:\n#!/bin/bash wget \"https://github.com/kairos-io/kairos/releases/download/master/kairos-@flavor-@flavorRelease-standard-amd64-generic-master-k3sv1.33.4+k3s1-kernel\" wget \"https://github.com/kairos-io/kairos/releases/download/master/kairos-@flavor-@flavorRelease-standard-amd64-generic-master-k3sv1.33.4+k3s1-initrd\" wget \"https://github.com/kairos-io/kairos/releases/download/master/kairos-@flavor-@flavorRelease-standard-amd64-generic-master-k3sv1.33.4+k3s1.squashfs\" cat \u003c\u003c EOF \u003e config.yaml #cloud-config hostname: \"hostname.domain.tld\" users: - name: \"kairos\" passwd: \"kairos\" EOF # This will start the pixiecore server. # Any machine that depends on DHCP to netboot will be send the specified files and the cmd boot line. docker run \\ -d --name pixiecore --net=host -v $PWD:/files quay.io/pixiecore/pixiecore \\ boot /files/kairos-@flavor-@flavorRelease-standard-amd64-generic-master-k3sv1.33.4+k3s1-kernel /files/kairos-@flavor-@flavorRelease-standard-amd64-generic-master-k3sv1.33.4+k3s1-initrd --cmdline=\"rd.neednet=1 ip=dhcp rd.cos.disable root=live:{{ ID \\\"/files/kairos-@flavor-@flavorRelease-standard-amd64-generic-master-k3sv1.33.4+k3s1.squashfs\\\" }} netboot install-mode config_url={{ ID \\\"/files/config.yaml\\\" }} console=tty1 console=ttyS0 console=tty0\" If your machine doesn’t support netbooting, you can use our generic image, which is built using an ipxe script from the pixiecore project. The ISO will wait for a DHCP proxy response from pixiecore.\nIf pixiecore is successfully reached, you should see an output similar to this in the pixiecore docker container:\n$ docker logs pixiecore [DHCP] Offering to boot 08:00:27:e5:22:8c [DHCP] Offering to boot 08:00:27:e5:22:8c [HTTP] Sending ipxe boot script to 192.168.1.49:4371 [HTTP] Sent file \"kernel\" to 192.168.1.49:4371 [HTTP] Sent file \"initrd-0\" to 192.168.1.49:4371 ","categories":"","description":"Install Kairos from network","excerpt":"Install Kairos from network","ref":"/docs/installation/netboot/","tags":"","title":"Network booting"},{"body":" Warning Despite the Flavor you may have selected to look into the docs. The Nvidia AGX Orin only works with Ubuntu 20.04 Note Please note that the following page contains only development reference. At the time of writing, we have tried porting Kairos to Jetson Nano eMMC without success. This is due to the old kernel supported (4.9), not properly working with EFISTUB and U-boot (you can see the issue here). However, the steps outlined should be a good reference to port Kairos to those architecture when a new kernel version is available. We have tested, and have successfully booted a Jetson Nano with the 5.15 kernel, however, due to the lack of driver support, eMMC partitions are not properly recognized. This page is a development reference in order to boot Kairos in Nvidia Jetson devices. Nvidia Jetson images by default ship extlinux as bootloader, without EFI boot. This guide explains how to get instead u-boot to chainload to grub2, which can be used to boot and load Kairos.\nNote that currently there are no official Kairos core images for Jetson images, this page will refer to Jetson Nano eMMC version as the current reference, but the steps should be similar, as outline how to use the Nvidia SDK to flash the OS onboard in the eMMC of the device.\nThe steps involved are:\nPrepare the Kernel (if you have one, compatible with EFISTUB, you can skip this part) Flash u-boot (If the U-boot version support booting efi shells, you might skip this part too) Prepare the Kairos partitions Flash the image to the board Prerequisites You need the Nvidia SDK and few other dependencies in the system. Note that for the Jetson Nano you can’t use the latest SDK version as it is not anymore supporting it. The latest version available with support for Jetson Nano is r32.7.3:\n# Build dependencies apt update \u0026\u0026 apt install -y git-core build-essential bc wget xxd kmod flex libelf-dev bison libssl-dev mkdir build build_dir=$PWD/build cd build # Get Jetson SDK compatible with Jetson NANO wget https://developer.nvidia.com/downloads/remetpack-463r32releasev73t210jetson-210linur3273aarch64tbz2 -O Jetson-210_Linux_R32.7.3_aarch64.tbz2 tar xvf Jetson-210_Linux_R32.7.3_aarch64.tbz2 Prepare the Kernel The only requirement of the kernel in order to this to work is that has to have CONFIG_EFI_STUB and CONFIG_EFI enabled.\nThe default kernel with the Nvidia Jetson Nano is 4.9 and it turns out to not have those enabled.\nBuild from official Nvidia sources If your kernel is not compiled to boot as EFI stub you can refer to the steps below to compile the official Nvidia kernel with EFISTUB:\ncd build wget https://developer.nvidia.com/downloads/remack-sdksjetpack-463r32releasev73sourcest210publicsourcestbz2 -O public_sources.tbz2 wget https://developer.nvidia.com/embedded/dlc/l4t-gcc-7-3-1-toolchain-64-bit tar xvf https://developer.nvidia.com/embedded/dlc/l4t-gcc-7-3-1-toolchain-64-bit # gcc-linaro-7.3.1-2018.05-x86_64_aarch64-linux-gnu/.... export CROSS_COMPILE_AARCH64_PATH=$PWD/gcc-linaro-7.3.1-2018.05-x86_64_aarch64-linux-gnu/ cd Linux_for_Tegra/source/public tar xvf kernel_src.bz2 mkdir kernel_out echo \"CONFIG_EFI_STUB=y\" \u003e\u003e ./kernel/kernel-4.9/arch/arm64/configs/tegra_defconfig echo \"CONFIG_EFI=y\" \u003e\u003e ./kernel/kernel-4.9/arch/arm64/configs/tegra_defconfig # https://forums.developer.nvidia.com/t/kernel-build-script-nvbuild-sh-with-output-dir-option-not-working/173087 sed -i '86s/.*/ O_OPT=(O=\"${KERNEL_OUT_DIR}\")/' nvbuild.sh ## See workaround for DTB errors in Troubleshooting (edit Kconfig.include..) ./nvbuild.sh -o $PWD/kernel_out Note that, with the Jetson NANO, the kernel will fail to boot allocating the memory during the EFI stub boot phase.\nBuild from official linux kernel Seems the kernel 5.15 boots fine on the Jetson Nano, however, it fails to load eMMC drivers to detect eMMC partitions. A configuration reference can be found here.\nbuild_dir=$PWD/build cd build # Clone the kernel git clone --branch v5.15 --depth 1 https://github.com/torvalds/linux.git kernel-4.9 wget https://developer.nvidia.com/downloads/remack-sdksjetpack-463r32releasev73sourcest210publicsourcestbz2 -O public_sources.tbz2 tar xvf public_sources.tbz2 wget https://developer.nvidia.com/embedded/dlc/l4t-gcc-7-3-1-toolchain-64-bit tar xvf l4t-gcc-7-3-1-toolchain-64-bit # Replace the kernel in the SDK pushd Linux_for_Tegra/source/public \u0026\u0026 tar xvf kernel_src.tbz2 \u0026\u0026 rm -rf kernel/kernel-4.9 \u0026\u0026 mv $build_dir/kernel-4.9 ./kernel/ \u0026\u0026 popd # Use the tegra config, patch nvbuild.sh mkdir kernel_out \u0026\u0026 \\ wget https://raw.githubusercontent.com/kairos-io/packages/main/packages/kernels/linux-tegra/config -O ./kernel/kernel-4.9/arch/arm64/configs/defconfig \u0026\u0026 \\ wget https://raw.githubusercontent.com/kairos-io/packages/main/packages/kernels/linux-tegra/nvbuild.sh -O nvbuild.sh \u0026\u0026 chmod +x nvbuild.sh # gcc 12 patches pushd Linux_for_Tegra/source/public/kernel/kernel-4.9 \u0026\u0026 curl -L https://raw.githubusercontent.com/kairos-io/packages/main/packages/kernels/linux-tegra/patch.patch | patch -p1 \u0026\u0026 popd # Build the kernel pushd Linux_for_Tegra/source/public \u0026\u0026 \\ CROSS_COMPILE_AARCH64_PATH=$build_dir/gcc-linaro-7.3.1-2018.05-x86_64_aarch64-linux-gnu/ ./nvbuild.sh -o $PWD/kernel_out Prepare container image (Kairos) Now we need a container image with the OS image. The image need to contain the kernel and the initramfs generated with dracut.\nFor instance, given that the kernel is available at /boot/Image, and the modules at /lib/modules:\nFROM .... RUN ln -sf Image /boot/vmlinuz RUN kernel=$(ls /lib/modules | head -n1) \u0026\u0026 \\ dracut -f \"/boot/initrd-${kernel}\" \"${kernel}\" \u0026\u0026 \\ ln -sf \"initrd-${kernel}\" /boot/initrd \u0026\u0026 \\ depmod -a \"${kernel}\" Flashing In order to flash to the eMMC we need the Nvidia SDK.\nmkdir work cd work wget https://developer.nvidia.com/downloads/remetpack-463r32releasev73t210jetson-210linur3273aarch64tbz2 tar xvf Jetson-210_Linux_R32.7.3_aarch64.tbz2 Replace U-boot (optional) If the version of u-boot is old and doesn’t support EFI booting, you can replace the u-boot binary like so:\nwget http://download.opensuse.org/ports/aarch64/tumbleweed/repo/oss/aarch64/u-boot-p3450-0000-2023.01-2.1.aarch64.rpm mkdir u-boot cd u-boot rpm2cpio ../u-boot-p3450-0000-2023.01-2.1.aarch64.rpm | cpio -idmv cd .. cd Linux_for_Tegra # \"p3450-0000\" Depends on your board cp -rfv ../u-boot/boot/u-boot.bin bootloader/t210ref/p3450-0000/u-boot.bin Disable Extlinux We need to disable extlinux, in order for u-boot to scan for EFI shells:\n# Drop extlinux echo \"\" \u003e ./bootloader/extlinux.conf Prepare Partitions We need to prepare the partitions from the container image we want to boot, in order to achieve this, we can use osbuilder, which will prepare the img files ready to be flashed for the SDK:\ncd Linux_for_Tegra docker run --privileged -e container_image=$IMAGE -v $PWD/bootloader:/bootloader --entrypoint /prepare_nvidia_orin_images.sh -ti --rm quay.io/kairos/auroraboot:v0.13.0 This command should create efi.img, oem.img, persistent.img, recovery_partition.img, state_partition.img in the bootloader directory\nConfigure the SDK In order to flash the partitions to the eMMC of the board, we need to configure the SDK to write the partitions to the board via its configuration files.\nFor the Jetson Nano, the configuration file for the partitions is located at bootloader/t210ref/cfg/flash_l4t_t210_emmc_p3448.xml, where we replace the partition name=APP with:\n\u003cpartition name=\"esp\" type=\"data\"\u003e \u003callocation_policy\u003e sequential \u003c/allocation_policy\u003e \u003cfilesystem_type\u003e basic \u003c/filesystem_type\u003e \u003csize\u003e 20971520 \u003c/size\u003e \u003cfile_system_attribute\u003e 0 \u003c/file_system_attribute\u003e \u003cpartition_type_guid\u003e C12A7328-F81F-11D2-BA4B-00A0C93EC93B \u003c/partition_type_guid\u003e \u003callocation_attribute\u003e 0x8 \u003c/allocation_attribute\u003e \u003cpercent_reserved\u003e 0 \u003c/percent_reserved\u003e \u003cfilename\u003e efi.img \u003c/filename\u003e \u003cdescription\u003e **Required.** Contains a redundant copy of CBoot. \u003c/description\u003e \u003c/partition\u003e \u003cpartition name=\"COS_RECOVERY\" type=\"data\"\u003e \u003callocation_policy\u003e sequential \u003c/allocation_policy\u003e \u003cfilesystem_type\u003e basic \u003c/filesystem_type\u003e \u003csize\u003e 2298478592 \u003c/size\u003e \u003callocation_attribute\u003e 0x8 \u003c/allocation_attribute\u003e \u003cfilename\u003e recovery_partition.img \u003c/filename\u003e \u003cdescription\u003e \u003c/description\u003e \u003c/partition\u003e \u003cpartition name=\"COS_STATE\" type=\"data\"\u003e \u003callocation_policy\u003e sequential \u003c/allocation_policy\u003e \u003cfilesystem_type\u003e basic \u003c/filesystem_type\u003e \u003csize\u003e 5234491392 \u003c/size\u003e \u003callocation_attribute\u003e 0x8 \u003c/allocation_attribute\u003e \u003cfilename\u003e state_partition.img \u003c/filename\u003e \u003cdescription\u003e \u003c/description\u003e \u003c/partition\u003e \u003cpartition name=\"COS_OEM\" type=\"data\"\u003e \u003callocation_policy\u003e sequential \u003c/allocation_policy\u003e \u003cfilesystem_type\u003e basic \u003c/filesystem_type\u003e \u003csize\u003e 67108864 \u003c/size\u003e \u003callocation_attribute\u003e 0x8 \u003c/allocation_attribute\u003e \u003cfilename\u003e oem.img \u003c/filename\u003e \u003cdescription\u003e \u003c/description\u003e \u003c/partition\u003e \u003cpartition name=\"COS_PERSISTENT\" type=\"data\"\u003e \u003callocation_policy\u003e sequential \u003c/allocation_policy\u003e \u003cfilesystem_type\u003e basic \u003c/filesystem_type\u003e \u003csize\u003e 2147483648 \u003c/size\u003e \u003callocation_attribute\u003e 0x8 \u003c/allocation_attribute\u003e \u003cfilename\u003e persistent.img \u003c/filename\u003e \u003cdescription\u003e \u003c/description\u003e \u003c/partition\u003e Note: The order matters here. We want to replace the default “APP” partition with our set of partitions.\nIf you didn’t changed the default size of the images you should be fine, however, you should check the \u003csize\u003e\u003c/size\u003e of each of the blocks if corresponds to the files generated from your container image:\nstat -c %s bootloader/efi.img stat -c %s bootloader/recovery_partition.img stat -c %s bootloader/state_partition.img stat -c %s bootloader/oem.img stat -c %s bootloader/persistent.img Flash Turn the board in recovery mode, depending on the model this process might differ:\nTurn off the board Jump the FCC REC pin to ground Plug the USB cable Power on the board If you see the board ready to be flashed, you should see the following:\n$ lsusb Bus 003 Device 092: ID 0955:7f21 NVIDIA Corp. APX To flash the configuration to the board, run:\n./flash.sh -r jetson-nano-devkit-emmc mmcblk0p1 Troubleshooting notes You can use picom to see the serial console:\npicocom -b 115200 /dev/ttyUSB0 References https://docs.nvidia.com/jetson/archives/r35.1/DeveloperGuide/text/SD/SoftwarePackagesAndTheUpdateMechanism.html#update-with-partition-layout-changes https://docs.nvidia.com/jetson/archives/r34.1/DeveloperGuide/text/SD/Kernel/KernelCustomization.html?highlight=kernel https://en.opensuse.org/HCL:Jetson_Nano#Update_Firmware https://nullr0ute.com/2020/11/installing-fedora-on-the-nvidia-jetson-nano/ https://forums.developer.nvidia.com/t/support-nano-on-openwrt/219168/7 ","categories":"","description":"This page contains a reference on how to run Kairos on Nvidia Jetson ARM","excerpt":"This page contains a reference on how to run Kairos on Nvidia Jetson …","ref":"/docs/development/nvidia/","tags":"","title":"Booting Kairos on Nvidia Jetson ARM"},{"body":"Kairos offers several pre-built images for user convenience based on popular Linux distributions such as openSUSE, Alpine Linux, and Ubuntu. The Kairos core team does its best to test these images, but those that are based on systemd (e.g. openSUSE, Ubuntu) are more thoroughly tested due to their homogenous settings. Support for other non-systemd based flavors (e.g. Alpine) may be limited due to team bandwidth. However, as Kairos is an open source community-driven project, we welcome any contributions, bug reports, and bug fixes. Check out our Contribution guidelines for more information.\nIn addition, tighter integration with systemd allows for several features that are only available with it, such as live layering.\nThese images are pushed to quay.io and are available for installation and upgrading. The installable mediums included in the releases are generated using the methods described in the automated installation reference, and the images can be used for upgrades as well.\nImage flavors Kairos release processes generates images based on official container images from popular Linux distributions. If you don’t see your preferred distribution, check if we are already planning support for it or create a new issue.\nNote You can also download ISOs and other artifacts from the releases page. Below is a list of the container repositories for each flavor:\nFlavor repository Alpine quay.io/kairos/alpine Debian quay.io/kairos/debian Fedora quay.io/kairos/fedora openSUSE quay.io/kairos/opensuse Ubuntu quay.io/kairos/ubuntu Rocky Linux quay.io/kairos/rockylinux The various images are available with different tags in the form of:\nquay.io/kairos/\u003cflavor\u003e:\u003cflavor_release\u003e-\u003cvariant\u003e-\u003carch\u003e-\u003cdevice\u003e-\u003cversion\u003e For example: quay.io/kairos/@flavor:@flavorRelease-standard-amd64-generic-master-k3sv1.33.4-k3s1. More about Kairos naming conventions here.\nNotes:\nThe Core images do not include any Kubernetes engine and can be used as a base for customizations. The Standard images include k3s and the kairos provider, which enables Kubernetes deployments and optionally enables p2p. The -img repositories contain an img file which can be directly written to an SD card or USB drive for use with ARM devices. Note The pipelines do not publish raw artifacts for the arm architecture because the files are too large for GitHub Actions (they exceed the artifact size limit). These artifacts can be extracted from the published docker images using the following command:\ndocker run -ti --rm -v $PWD:/image gcr.io/go-containerregistry/crane export \"quay.io/kairos/@flavor:@flavorRelease-core-arm64-rpi4-master-img\" - | tar -xvf - The artifacts can be found in the build directory.\nBuilding core and standard generic images Unfortunately we don’t have the resources and capacity to build every possible artifact in our matrix. Thankfully, you can still build those images manually on your local machine, all you need is git and docker. Here’s an example how to build an Almalinux ARM RPI4 container image.\ngit clone https://github.com/kairos-io/kairos.git cd kairos docker build --platform linux/arm64 --build-arg BASE_IMG=almalinux:9 --build-arg MODEL=rpi4 --build-arg VERSION=1.0.0 -f images/Dockerfile -t mycustomimage:1.0.0 . Note See the kairos-factory.md page for more info. Versioning policy Kairos follows Semantic Versioning and our releases signal changes to Kairos components, rather than changes to the underlying OS and package versions. Flavors are pinned to specific upstream OS branches (e.g. opensuse to leap 15.4) and major version bumps will be reflected through new flavors in our build matrix or through specific releases to follow upstream with regard to minor version bumps (e.g. leap 15.3 and leap 15.4).\nHere are some key points to note:\nWe only support the latest release branch with patch releases. Patch releases (e.g. 1.1.x) follow a weekly release cadence, unless there are exceptions for highly impactful bugs in Kairos itself or at the OS layer (e.g. high-severity CVEs). Minor releases follow a monthly cadence and are expected to bring enhancements through planned releases. Major releases signal new advanced features or significant changes to the codebase. In-place upgrades from old to new major release branches are not always guaranteed, but we strive for compatibility across versions. Note In order to give users more control over the chosen base image (e.g. openSUSE, Ubuntu, etc.) and reduce reliance on our CI infrastructure, we are actively working on streamlining the creation of Kairos-based distributions directly from upstream base images. You can track the development progress here.\nIf you need to further customize images, including changes to the base image, package updates, and CVE hotfixes, check out the customization docs.\nRelease changelog Our changelog is published as part of the release process and contains all the changes, highlights, and release notes that are relevant to the release. We strongly recommend checking the changelog for each release before upgrading or building a customized version of Kairos.\nRelease changelogs are available for Kairos core and for each component. Below is a list of the components that are part of a Kairos release and their respective release pages with changelogs.\nProject Release page Kairos core and standard (k3s support) https://github.com/kairos-io/kairos/releases Kairos’ provider https://github.com/kairos-io/provider-kairos/releases Immucore https://github.com/kairos-io/Immucore/releases AuroraBoot https://github.com/kairos-io/AuroraBoot/releases OSBuilder https://github.com/kairos-io/osbuilder/releases Service Billing Of Materials (SBOM) SBOM lists are regularly pushed via the CI as part of the Github releases assets. For instance,\nhttps://github.com/kairos-io/kairos/releases/download/master/kairos-@flavor-@flavorRelease-core-amd64-generic-master-sbom.spdx.json is the SBOM for the core @flavor image.\nImage verification Images signatures are pushed regularly for tagged releases. To verify images with cosign (install guide) for example, you can use the following command:\ncosign verify-attestation \\ --type spdx quay.io/kairos/@flavor:@flavorRelease-core-amd64-generic-master \\ --certificate-identity \"https://github.com/kairos-io/kairos/.github/workflows/release.yaml@refs/tags/master\" \\ --certificate-oidc-issuer \"https://token.actions.githubusercontent.com\" To see how to verify image attestation during upgrades with kyverno, see the documentation page.\n","categories":"","description":"","excerpt":"Kairos offers several pre-built images for user convenience based on …","ref":"/docs/reference/image_matrix/","tags":"","title":"Image support matrix"},{"body":"","categories":"","description":"Learn how to configure advanced Kairos features such as system extensions, partitions, bundles, and confidential computing.\n","excerpt":"Learn how to configure advanced Kairos features such as system …","ref":"/docs/advanced/","tags":"","title":"Advanced"},{"body":"Please see the Kairos factory section to see how build images from scratch.\nYou can also see the Kairos Examples in the Github Repo for some examples.\n","categories":"","description":"This article shows how to bring your own image with Kairos, and build a Kairos derivative from scratch using base container images from popular distributions such as Ubuntu, Fedora, openSUSE, etc.","excerpt":"This article shows how to bring your own image with Kairos, and build …","ref":"/docs/reference/build-from-scratch/","tags":"","title":"Build Kairos from scratch"},{"body":"This page provides a reference guide on how to build raw images for Kairos using QEMU. It covers the process of using a default cloud configuration and a script to generate bootable images. The cloud configuration can be customized to suit different use cases. This mechanism can be used to create golden images, or simply be an alternative to use tools like Packer.\nNote This method differs from the ones documented in the Auroraboot section: this method is suitable if you need to create appliances that have to run a full-installation. AuroraBoot will create instead images pre-installed which will skip the usual Kairos installation process in runtime For more information about AuroraBoot, see the AuroraBoot Reference.\nRequirements The following tools are required in the system:\nqemu A Kairos ISO (or a custom built one) Default Cloud Configuration The following is the default cloud-config file used for Kairos. You can modify this file as needed to fit your environment:\n#cloud-config hostname: kairos-{{ trunc 4 .MachineID }} # Automated install block install: # Device for automated installs device: \"auto\" # Reboot after installation reboot: false # Power off after installation poweroff: true # Set to true to enable automated installations auto: true ## Login users: - name: \"kairos\" passwd: kairos groups: - admin #lock_passwd: true #ssh_authorized_keys: #- github:mudler stages: boot: - name: \"Repart image\" layout: device: label: COS_PERSISTENT expand_partition: size: 0 # all space commands: # grow filesystem if not used 100% - | [[ \"$(echo \"$(df -h | grep COS_PERSISTENT)\" | awk '{print $5}' | tr -d '%')\" -ne 100 ]] \u0026\u0026 resize2fs /dev/disk/by-label/COS_PERSISTENT Note install.poweroff is set to true to power off the machine after installation and install.auto is set to true to enable automated installations. Both of these settings are needed to function properly. Script to Build Raw Images Use the following Bash script to generate raw bootable images using QEMU. This script takes a cloud-init YAML file as an argument and creates a raw disk image for Kairos.\n#!/bin/bash # Generates raw bootable images with qemu set -ex CLOUD_INIT=${1:-cloud_init.yaml} QEMU=${QEMU:-qemu-system-x86_64} ISO=${2:-iso.iso} mkdir -p build pushd build touch meta-data cp -rfv $CLOUD_INIT user-data mkisofs -output ci.iso -volid cidata -joliet -rock user-data meta-data truncate -s \"+$((20000*1024*1024))\" disk.raw ${QEMU} -m 8096 -smp cores=2 \\ -nographic -cpu host \\ -serial mon:stdio \\ -rtc base=utc,clock=rt \\ -chardev socket,path=qga.sock,server,nowait,id=qga0 \\ -device virtio-serial \\ -device virtserialport,chardev=qga0,name=org.qemu.guest_agent.0 \\ -drive if=virtio,media=disk,file=disk.raw \\ -drive format=raw,media=cdrom,readonly=on,file=$ISO \\ -drive format=raw,media=cdrom,readonly=on,file=ci.iso \\ -boot d \\ -enable-kvm Script Breakdown Cloud-init Setup: Copies the cloud-init configuration to the build directory. Metadata: Creates an empty meta-data file as required by cloud-init. ISO Creation: Creates a cloud-init ISO image (ci.iso) with mkisofs. Disk Image: Generates a raw disk image (disk.raw) with a size of 20 GB. QEMU Command: Uses QEMU to boot the Kairos installer with: 8 GB memory (-m 8096) 2 CPU cores (-smp cores=2) KVM acceleration (-enable-kvm) Attaching the raw disk and ISO images as drives. Customizing the Cloud Configuration and Building the Image To customize your installation:\nModify the cloud-config YAML as needed. Pass the modified configuration as an argument to the script (optionally, pass the Kairos ISO as the second argument): ./build_image.sh my_custom_cloud_config.yaml This script will generate a raw disk image with the specified cloud configuration.\n","categories":"","description":"This article shows how to bring your own image with Kairos, and build a Kairos derivative from scratch using base container images from popular distributions such as Ubuntu, Fedora, openSUSE, etc.","excerpt":"This article shows how to bring your own image with Kairos, and build …","ref":"/docs/reference/build_raw_images_with_qemu/","tags":"","title":"Build Raw images with QEMU"},{"body":"Whether you need to add custom logic, install extra packages, or make other modifications to your system, bundles simplify the process. They can be applied after installation or before bootstrapping a node.\nBundles are container images containing only files (and not full OS) that can be used to install new software or extend the cloud-init syntax. You can find community-supported bundles in the community-bundles repository.\nConsuming Bundles To use a bundle in your Kairos configuration, you will need to specify the type of bundle and the target image in your cloud-config file.\nThere are two points in time when the bundles may be installed:\nRight after installation, before booting to the installed system. On first boot, after booting to the installed system. If you are booting a “standard” image (the one with Kubernetes), the bundle is installed before Kubernetes starts. The first type of installation can be used to create systemd-sysext extensions. You can read more here.\nThe second type of installation can be used to make changes to the installed system. E.g. create kubernetes resource files in /var/lib/rancher/k3s/server/manifests/ like longhorn community bundle does.\nTo apply a bundle on the first boot (and before Kubernetes starts), you can include it in your config like this:\n#cloud-config bundles: - targets: - run://\u003cimage\u003e Replace \u003cimage\u003e with the URL or path to the bundle image. The prefix (e.g. run://) indicates the type of bundle being used.\nTo install a bundle after installation instead (for bundles that are created to be used like that), use the following:\n#cloud-config install: bundles: - targets: - run://\u003cimage\u003e Bundles have access to the Kairos cloud-config during their installation. This allows the user to add new blocks of configuration to configure the bundles.\nFor example, this is how metalLB community bundle can be configured:\n#cloud-config hostname: kairoslab-{{ trunc 4 .MachineID }} users: - name: kairos ssh_authorized_keys: # Add your github user here! - github:mudler k3s: enable: true args: - --disable=servicelb # Specify the bundle to use bundles: - targets: - run://quay.io/kairos/community-bundles:metallb_latest # Specify metallb settings metallb: version: 0.13.7 address_pool: 192.168.1.10-192.168.1.20 Warning For both types of bundle installation (after installation, on first boot), the installation only happens once. Changing the bundle’s configuration block after Kairos installation is complete, will not have any effect. If you want the installation to stop if a bundle installation fails, you can add set the following option to true in your Kairos config:\nfail_on_bundles_errors: true If you want to install a bundle after installation has finished, you can use the kairos-agent to perform a manual installation. E.g.:\nkairos-agent install-bundle run://quay.io/kairos/community-bundles:cert-manager_latest Bundle types Bundles can carry also binaries that can be overlayed in the rootfs while building images.\nKairos supports three types of bundles:\nContainer: This type is a bare container that simply contains files that need to be copied to the system. It is useful for copying over configuration files, scripts, or any other static content that you want to include on your system (prefixed with container: or docker:).\nRun: This type is also a bare container, but it comes with a script that can be run during the installation phase to add custom logic. This is useful for performing any necessary setup or configuration tasks that need to be done before the cluster is fully deployed (prefixed with run:).\nPackage: This type is a luet package that will be installed in the system. It requires you to specify a luet repository in order to work. Luet packages are a powerful way to manage dependencies and install software on your system (prefixed with luet:).\nYou can also specify local_file: true in the bundles configuration. In that case the bundle’s URL is translated as an absolute path to an image tarball on the filesystem. This feature can be used in airgap situations, where you can pre-add bundles to the image before deployment.\nFor example:\ninstall: bundles: - targets: - container:///home/kairos/mybundle.tar local_file: true The format of the bundle tarball is the one you get when you docker save myorg/myimage or skopeo copy docker-daemon:myorg/myimage docker-archive:myimage.tar\nIt’s important to note that bundles do not have any special meaning in terms of immutability. They install files over paths that are mutable in the system, as they are simply overlaid during the boot process. This means that you can use bundles to make changes to your system at any time, even after it has been deployed.\nCreate bundles To build your own bundle, start by creating a Dockerfile along with any required files and scripts. A bundle is essentially a container image that packages all the assets needed for a specific task.\nWhen defining your bundle, choose a base image and copy the relevant files and scripts into it. For example, the following Dockerfile creates a bundle that deploys everything inside assets to a Kubernetes cluster:\nFROM alpine COPY ./run.sh / COPY ./assets /assets And the associated run.sh that installs the assets depending on a cloud-config keyword can be:\n#!/bin/bash K3S_MANIFEST_DIR=\"/var/lib/rancher/k3s/server/manifests/\" mkdir -p $K3S_MANIFEST_DIR # IF the user sets `example.enable` in the input cloud config, we install our assets if [ \"$(kairos-agent config get example.enable | tr -d '\\n')\" == \"true\" ]; then cp -rf assets/* $K3S_MANIFEST_DIR fi This Dockerfile creates an image based on the Alpine base image, and copies over a script file and some assets to the image. You can then add any additional instructions to the Dockerfile to install necessary packages, set environment variables, or perform any other tasks required by your bundle.\nOnce you have created your Dockerfile and any necessary script files, you can build your bundle image by running docker build and specifying the path to your Dockerfile.\nFor example:\ndocker build -t \u003cimage\u003e . This command will build an image with the name you specify ( replace \u003cimage\u003e accordingly ) based on the instructions in your Dockerfile.\nAfter building your bundle image, you will need to push it to a registry so that it can be accessed by Kairos. You can use a public registry like Docker Hub. To push your image to a registry, use the docker push command. For example:\ndocker push \u003cimage\u003e This will push the \u003cimage\u003e to your specified registry.\nAnd use it with Kairos:\n#cloud-config bundles: - targets: # e.g. run://quay.io/...:tag - run://\u003cimage\u003e example: enable: true See the community-bundles repository for further examples.\n","categories":"","description":"Bundles are a powerful feature of Kairos that let you customize and configure your operating system. This section explains how to use and build custom bundles.","excerpt":"Bundles are a powerful feature of Kairos that let you customize and …","ref":"/docs/advanced/bundles/","tags":"","title":"Bundles"},{"body":"Kairos offers the ability to encrypt user data partitions with LUKS. User-data partitions are dedicated to persist data for a running system, stored separately from the OS images. This encryption mechanism can also be used to encrypt additional partitions created during the installation process.\nKairos supports the following encryption scenarios:\nOffline mode - Encryption key for partitions is stored on the machine inside the TPM chip. Online mode (Automated) - Keypair used to encrypt the partition passphrase is stored on the TPM chip, and an external server is used to store the encrypted passphrases. Online mode (Manually configured) - Plaintext passphrase is stored in the KMS server and returned to the node after TPM challenging. Kairos uses the TPM chip to encrypt partition passphrases, and for offline encryption, it stores the passphrase in the non-volatile registries of the chip.\nTo enable encryption, you will need to specify the labels of the partitions you want to encrypt, a minimum configuration for offline encryption can be seen below:\n#cloud-config install: # Label of partitions to encrypt # COS_PERSISTENT is the OS partition # dedicated to user-persistent data. encrypted_partitions: - COS_PERSISTENT Please note that for online mode, you will also need to specify the key management server address that will be used to store the keys, a complete configuration reference is the following:\n#cloud-config # Install block install: # Label of partitions to encrypt # COS_PERSISTENT is the OS partition # dedicated to user-persistent data. encrypted_partitions: - COS_PERSISTENT # Kcrypt configuration block kcrypt: challenger: # External KMS Server address. This must be reachable by the node challenger_server: \"http://192.168.68.109:30000\" # (optional) Custom Non-Volatile index to use to store encoded blobs nv_index: \"\" # (optional) Custom Index for the RSA Key pair c_index: \"\" # (optional) Custom TPM device tpm_device: \"\" # (optional) Instructs the client to lookup the KMS using mdns mdns: false Option Description install.encrypted_partitions Label of partitions to encrypt kcrypt.challenger.challenger_server External KMS Server address kcrypt.challenger.nv_index Custom Non-Volatile index to use to store encoded blobs kcrypt.challenger.c_index Custom Index for the RSA Key pair kcrypt.challenger.tpm_device Custom TPM device kcrypt.challenger.mdns Discover KMS using mdns. Defaults to false Requirements The host machine must have a TPM chip version 2.0 or higher to use encryption with Kairos. A list of TPM chips/HW can be found in the list of certified products, however, any modern machine has a TPM 2.0 chip.\nComponents The Kairos encryption design involves three components to manage partitions encryption and decryption lifecycle:\nkcrypt runs on the machine and attempts to unlock partitions by using plugins to delegate encryption/decryption business logic. kcrypt-discovery-challenger runs on the machine, it is called by kcrypt and uses the TPM chip to retrieve the passphrase as described below. kcrypt-challenger is the KMS (Key Management Server) component, deployed in Kubernetes, which manages secrets and partitions of the nodes. Offline mode This scenario covers encryption of data at rest without any third party or KMS server. The keys used to encrypt the partitions are stored in the TPM chip.\nScenario: Offline encryption A high level overview of the interaction between the components can be observed here:\nA complete cloud config example for this scenario can be:\n#cloud-config install: encrypted_partitions: - COS_PERSISTENT hostname: metal-{{ trunc 4 .MachineID }} users: - name: kairos # Change to your pass here passwd: kairos groups: - admin ssh_authorized_keys: # Replace with your github user and un-comment the line below: # - github:mudler Note, we define a list of partition labels that we want to encrypt. In the example above we set COS_PERSISTENT to be encrypted, which in turns will encrypt all the user-data of the machine (this includes, for instance, Kubernetes pulled images, or any runtime persisting data on the machine).\nOnline mode Online mode involves an external service (the Key Management Server, KMS) to boot the machines. The KMS role is to enable machine to boot by providing the encrypted secrets, or passphrases to unlock the encrypted drive. Authentication with the KMS is done via TPM challenging.\nIn this scenario, we need to first deploy the KMS server to an existing Kubernetes cluster, and associate the TPM hash of the nodes that we want to manage. During deployment, we specify the KMS server inside the cloud-config of the nodes to be provisioned.\nRequirements A Kubernetes cluster Kcrypt-challenger reachable by the nodes attempting to boot Install the KMS (kcrypt-challenger) To install the KMS (kcrypt-challenger), you will first need to make sure that certificate manager is installed. You can do this by running the following command:\nkubectl apply -f https://github.com/jetstack/cert-manager/releases/latest/download/cert-manager.yaml kubectl wait --for=condition=Available deployment --timeout=2m -n cert-manager --all To install kcrypt-challenger on a Kubernetes cluster with helm, you can use the commands below:\n# Install the helm repository helm repo add kairos https://kairos-io.github.io/helm-charts helm repo update # Install the Kairos CRDs helm install kairos-crd kairos/kairos-crds # Deploy the KMS challenger helm install kairos-challenger kairos/kairos-challenger --set service.challenger.type=\"NodePort\" # we can also set up a specific port and a version: # helm install kairos-challenger kairos/kairos-challenger --set image.tag=\"v0.2.2\" --set service.challenger.type=\"NodePort\" --set service.challenger.nodePort=30000 A service must be used to expose the challenger. If using the node port, we can retrieve the address with:\nexport EXTERNAL_IP=$(kubectl get nodes -o jsonpath='{.items[].status.addresses[?(@.type == \"ExternalIP\")].address}') export PORT=$(kubectl get svc kairos-challenger-escrow-service -o json | jq '.spec.ports[0].nodePort') Register a node In order to register a node on the KMS, the TPM hash of the node needs to be retrieved first. The TPM hash is a SHA256 sum of the EK public key, which is part of the EK key pair that is present on the TPM from the manufacturer. The EK private key cannot be changed and is unique to the TPM and therefore, the EK public key can be used to uniquely challenge the device. During the challenge, the node will send its public key to the challenger, which will generate a SHA256 checksum and compare it to a local database of checksums. In order to build this database, the checksum needs to be registered with the challenger.\nYou can get a node TPM hash by running /system/discovery/kcrypt-discovery-challenger as root from the LiveCD:\nkairos@localhost:~\u003e ID=$(sudo /system/discovery/kcrypt-discovery-challenger) kairos@localhost:~\u003e echo $ID 7441c78f1976fb23e6a5c68f0be35be8375b135dcb36fb03cecc60f39c7660bd This is the TPM hash you should use in the definition of the SealedVolume in the examples below.\nScenario: Automatically generated keys The TPM chip generates unique RSA keys for each machine during installation, which are used to encrypt a generated secret. These keys can only be accessed by the TPM and not by the KMS, thus ensuring that both the KMS and the TPM chip are required to boot the machine. As a result, even if the machine or its disks are stolen, the drive remains sealed and encrypted. Deployment using this method, will store the encrypted key used to boot into the KMS, and the keypair used to encrypt it in the TPM chip of the machine during installation. This means that, only the TPM chip can decode the passphrase, and the passphrase is stored in the KMS such as it can’t be decrypted by it. As such, nodes can boot only with the KMS, and the disk can be decrypted only by the node.\nTo register a node to kubernetes, use the TPM hash retrieved before (see section “Register a node”) and replace it in this example command:\ncat \u003c\u003cEOF | kubectl apply -f - apiVersion: keyserver.kairos.io/v1alpha1 kind: SealedVolume metadata: name: test2 namespace: default spec: TPMHash: \"7441c78f1976fb23e6a5c68f0be35be8375b135dcb36fb03cecc60f39c7660bd\" partitions: - label: COS_PERSISTENT quarantined: false EOF This command will register the node on the KMS.\nA node can use the following during deployment, specifying the address of the challenger server:\n#cloud-config install: encrypted_partitions: - COS_PERSISTENT grub_options: extra_cmdline: \"rd.neednet=1\" kcrypt: challenger: challenger_server: \"http://192.168.68.109:30000\" nv_index: \"\" c_index: \"\" tpm_device: \"\" hostname: metal-{{ trunc 4 .MachineID }} users: - name: kairos # Change to your pass here passwd: kairos groups: - admin ssh_authorized_keys: # Replace with your github user and un-comment the line below: # - github:mudler Scenario: Static keys In this scenario the Kubernetes administrator knows the passphrase of the nodes, and sets explicitly during configuration the passphrase for each partitions of the nodes. This scenario is suitable for cases when the passphrase needs to be carried over, and not to be tied specifically to the TPM chip.\nThe TPM chip is still used for authentication a machine. The discovery-challenger needs still to know the TPM hash of each of the nodes before installation.\nTo register a node to kubernetes, replace the TPMHash in the following example with the TPM hash retrieved before, and specify a passphrase with a secret reference for the partition:\ncat \u003c\u003cEOF | kubectl apply -f - apiVersion: v1 kind: Secret metadata: name: mysecret namespace: default type: Opaque stringData: pass: \"awesome-plaintext-passphrase\" --- apiVersion: keyserver.kairos.io/v1alpha1 kind: SealedVolume metadata: name: test2 namespace: default spec: TPMHash: \"7441c78f1976fb23e6a5c68f0be35be8375b135dcb36fb03cecc60f39c7660bd\" partitions: - label: COS_PERSISTENT secret: name: mysecret path: pass quarantined: false EOF The node doesn’t need any specific configuration beside the kcrypt challenger, so for instance:\n#cloud-config install: encrypted_partitions: - COS_PERSISTENT grub_options: extra_cmdline: \"rd.neednet=1\" kcrypt: challenger: challenger_server: \"http://192.168.68.109:30000\" nv_index: \"\" c_index: \"\" tpm_device: \"\" hostname: metal-{{ trunc 4 .MachineID }} users: - name: kairos # Change to your pass here passwd: kairos groups: - admin ssh_authorized_keys: # Replace with your github user and un-comment the line below: # - github:mudler Discoverable Key Management Server (KMS) By setting kcrypt.challenger.mdns to true in the config, Kairos will try to discover the KMS using the mdns protocol. For that to work, the kcrypt.challenger.challenger_server options should also be a domain ending in .local. If a server exists in the same network and responds to the request, Kairos will use the information from the response to connect to the server.\nThe server running in the kubernetes cluster (See the Components section) does not implement the mDNS protocol and thus it won’t respond to the client’s request. That is because the server is running inside Kubernetes, which has its own network and it won’t receive the client’s broadcast message.\nA server is needed that runs in the same network as the Kairos node and responds with the IP address and port where the KMS is reachable. There may be tools that can be configured for the job, but we also provide a little utility that does exactly that: https://github.com/kairos-io/simple-mdns-server/\nThe process to deploy the KMS is similar to the Online mode. An example on how to test this feature locally, can be found in this document.\nTroubleshooting Invoking /system/discovery/kcrypt-discovery-challenger without arguments returns the TPM pubhash. Invoking kcrypt-discovery-challenger with ‘discovery.password’ triggers the logic to retrieve the passphrase, for instance can be used as such: echo '{ \"data\": \"{ \\\"label\\\": \\\"LABEL\\\" }\"}' | sudo /system/discovery/kcrypt-discovery-challenger \"discovery.password\" The url of the KMS server is looked up in the kairos config (In directories/oem and /sysroot/oem).\nNotes If encryption is enabled and COS_PERSISTENT is set to be encrypted, every cloud config file in /usr/local/cloud-config will be protected and can be used to store sensitive data. However, it’s important to keep in mind that although the contents of /usr/local are retained between reboots and upgrades, they will not be preserved during a resets.\n","categories":"","description":"This section describes how to encrypt partition with LUKS in Kairos.","excerpt":"This section describes how to encrypt partition with LUKS in Kairos.","ref":"/docs/advanced/partition_encryption/","tags":"","title":"Encrypting User Data with Kairos"},{"body":"Bandwidth Optimization Bandwidth Optimized Upgrades: Optimize bandwidth usage during OS upgrades using distributed caching solutions like k3s embedded registry. K3s Examples K3s Stages: Run stages along with k3s for post-deployment tasks. Troubleshooting Troubleshooting common issues: This page provides solutions to some common issues that you may encounter while using Kairos. ","categories":"","description":"Explore hands-on examples that show how to deploy Kairos clusters using K3s, bundles, VPNs, airgapped environments, and more.\n","excerpt":"Explore hands-on examples that show how to deploy Kairos clusters …","ref":"/docs/examples/","tags":"","title":"Examples"},{"body":" Network This feature is experimental and has only been tested on local setups. Run in production servers at your own risk. Feedback and bug reports are welcome, as we are improving the p2p aspects of Kairos. Deploying Kubernetes at the Edge can be a complex and time-consuming process, especially when it comes to setting up and managing multiple clusters. To make this process easier, Kairos leverages peer-to-peer technology to automatically coordinate and create Kubernetes clusters without the need of a control management interface.\nWith this feature, users don’t need to specify any network settings. They can just set the desired number of master nodes (in the case of an HA cluster) and the necessary configuration details, and Kairos will take care of the rest. The peer-to-peer technology allows the nodes in the cluster to communicate and coordinate with each other, ensuring that the clusters are set up correctly and efficiently with K3s.\nThis makes it easier to deploy and manage Kubernetes clusters at the Edge, saving user’s time and effort, allowing them to focus on running and scaling their applications. For more information about how does it work behind the scenes, check out the architecture section.\nYou can find full examples in our examples section:\nFull end to end example to bootstrap a self-coordinated cluster with Kairos and AuroraBoot Self-coordinated K3s HA cluster with KubeVIP Multi-node, single master setup Multi-node, HA setup Single-node setup This feature is currently experimental and can be optionally enabled by adding the following configuration to the node deployment file. If you are not familiar with the installation process, it is suggested to follow the quickstart:\np2p: # Disabling DHT makes co-ordination to discover nodes only in the local network disable_dht: true #Enabled by default # Automatic cluster deployment configuration auto: ha: # Enables HA control-plane enable: true # number of HA master node (beside the one used for init) for the control-plane master_nodes: 2 # network_token is the shared secret used by the nodes to co-ordinate with p2p. # Setting a network token implies auto.enable = true. # To disable, just set auto.enable = false network_token: \"YOUR_TOKEN_GOES_HERE\" The token p2p.network_token is a base64 encoded string which contains an edgevpn token.\nTo enable the automatic cluster deployment with peer-to-peer technology, specify a p2p.network_token. To enable HA, set p2p.auto.ha.master_nodes to the number of wanted HA/master nodes. Additionally, the p2p block can be used to configure the VPN and other settings as needed.\nWith these settings used to deploy all the nodes, those will automatically communicate and coordinate with each other to deploy and manage the Kubernetes cluster without the need for a control management interface and user intervention.\nConfiguration A minimum configuration file, that bootstraps a cluster with a simple single-master topology, can look like the following:\n#cloud-config hostname: \"kubevip-{{ trunc 4 .MachineID }}\" users: - name: \"kairos\" passwd: \"kairos\" ssh_authorized_keys: - github:mudler p2p: network_token: \"YOUR_TOKEN_GOES_HERE\" The p2p block is used to configure settings related to the mesh functionalities. The minimum required argument is the network_token and there is no need to configure k3s manually with the k3s block as it is already implied.\nNote The k3s block can still be used to override other k3s settings, e.g. args. The network token is a shared secret available to all the nodes of the cluster. It allows the node to co-ordinate and automatically assign roles. To generate a network token, see documentation.\nSimply applying the same configuration file to all the nodes should eventually bring one master and all the other nodes as workers. Adding nodes can be done also in a later step, which will automatically setup the node without any further configuration.\nFull example:\n#cloud-config install: auto: true device: \"auto\" reboot: true hostname: \"kubevip-{{ trunc 4 .MachineID }}\" users: - name: \"kairos\" passwd: \"kairos\" ssh_authorized_keys: - github:mudler ## Sets the Elastic IP used in KubeVIP kubevip: eip: \"192.168.1.110\" # Specify a manifest URL for KubeVIP. Empty uses default manifest_url: \"\" # Enables KubeVIP enable: true # Specifies a KubeVIP Interface interface: \"ens18\" p2p: role: \"\" # Set an hardcoded role, optional # Disabling DHT makes co-ordination to discover nodes only in the local network disable_dht: true #Enabled by default # Configures a VPN for the cluster nodes vpn: create: false # defaults to true use: false # defaults to true env: ..... # Automatic cluster deployment configuration auto: # Enables Automatic node configuration (self-coordination) # for role assignment enable: true # HA enables automatic HA roles assignment. # A master cluster init is always required, # Any additional master_node is configured as part of the # HA control plane. # If auto is disabled, HA has no effect. ha: # Enables HA control-plane enable: true # Number of HA additional master nodes. # A master node is always required for creating the cluster and is implied. # The setting below adds 2 additional master nodes, for a total of 3. master_nodes: 2 # Use an External database for the HA control plane external_db: \"external-db-string\" # network_token is the shared secret used by the nodes to co-ordinate with p2p network_token: \"YOUR_TOKEN_GOES_HERE\" In the YAML configuration example, there are several important keywords that control the behavior of the automatic cluster deployment:\nKeyword Description p2p Configures the peer to peer networking of the cluster p2p.disable_dht Disables the distributed hash table for cluster discovery p2p.network_token The shared secret used by the nodes to coordinate with p2p p2p.network_id Optional, unique identifier for the kubernetes cluster. It allows bootstrapping of multiple cluster using the same network token p2p.role Force a specific role for the node of the cluster p2p.vpn Configures a VPN for the cluster nodes p2p.vpn.create Enables the creation of the VPN p2p.vpn.use Enables the use of the VPN for routing Kubernetes traffic of the cluster p2p.vpn.env Configures the environment variables used to start for the VPN p2p.vpn.auto Configures the automatic deployment of the cluster p2p.auto.enable Enables automatic node configuration for role assignment p2p.auto.ha Configures the high availability settings for the cluster p2p.auto.ha.enable Enables the high availability settings p2p.auto.ha.master_nodes The number of additional HA master nodes expected in the cluster. p2p.auto.ha.external_db The external database used for high availability DNS When the p2p.dns is set to true the embedded DNS is configured on the node. This allows to propagate custom records to the nodes by using the blockchain DNS server. For example, this is assuming kairosctl bridge is running in a separate terminal:\ncurl -X POST http://localhost:8080/api/dns --header \"Content-Type: application/json\" -d '{ \"Regex\": \"foo.bar\", \"Records\": { \"A\": \"2.2.2.2\" } }' It will add the foo.bar domain with 2.2.2.2 as A response. Every node with DNS enabled will be able to resolve the domain after the domain is correctly announced.\nYou can check out the DNS in the DNS page in the API, see also the EdgeVPN docs.\nFurthermore, it is possible to tweak the DNS server which are used to forward requests for domain listed outside, and as well, it’s possible to lock down resolving only to nodes in the blockchain, by customizing the configuration file:\n#cloud-config p2p: network_token: \"....\" # Enable embedded DNS See also: https://mudler.github.io/edgevpn/docs/concepts/overview/dns/ dns: true vpn: env: # Disable DNS forwarding DNSFORWARD: \"false\" # Set cache size DNSCACHESIZE: \"200\" # Set DNS forward server DNSFORWARDSERVER: \"8.8.8.8:53\" Elastic IP If deploying a cluster in a Local network, it might be preferable to disable the VPN functionalities.\nWe use KubeVIP to provide an elastic ip for the control plane that can be configured via a specific block:\np2p: network_token: \"..\" vpn: # Disable VPN, so traffic is not configured with a VPN create: false use: false ## Sets the Elastic IP used in KubeVIP kubevip: eip: \"192.168.1.110\" # Specify a manifest URL for KubeVIP. Empty uses default manifest_url: \"\" # Enables KubeVIP enable: true # Specifies a KubeVIP Interface interface: \"ens18\" Keyword Description kubevip Block to configure KubeVIP for the cluster kubevip.eip The Elastic IP used for KubeVIP. Specifying one automatically enables KubeVIP. Choose a free IP that is not in a DHCP range of your network. kubevip.manifest_url The URL for the KubeVIP manifest kubevip.enable Enables KubeVIP for the cluster kubevip.interface The interface used for KubeVIP A full example, with KubeVIP and HA:\n#cloud-config install: auto: true device: \"auto\" reboot: true hostname: \"kubevip-{{ trunc 4 .MachineID }}\" users: - name: \"kairos\" passwd: \"kairos\" ssh_authorized_keys: - github:mudler p2p: network_token: \"...\" ha: master_nodes: 2 vpn: # Disable VPN, so traffic is not configured with a VPN create: false use: false kubevip: eip: \"192.168.1.110\" network_token The network_token is a unique code that is shared among nodes and can be created with the Kairos CLI or edgevpn. This allows nodes to automatically connect to the same network and generates private/public key pairs for secure communication using end-to-end encryption.\nTo generate a new token, run: docker CLI docker run -ti --rm quay.io/mudler/edgevpn -b -g kairos generate-token Join new nodes To add new nodes to the network, follow the same process as before and use the same configuration file for all machines. Unless you have specified roles for each node, no further changes to the configuration are necessary. The machines will automatically connect to each other, whether they are on a local or remote network.\nConnect to the nodes To connect to the nodes, you can use kairosctl and provide the network_token to establish a tunnel to the nodes network.\nsudo kairosctl bridge --network-token \u003cTOKEN\u003e This command creates a TUN device on your machine and allows you to communicate with each node in the cluster.\nNote The command requires root permissions in order to create a TUN/TAP device on the host. An API will be also available at localhost:8080 for inspecting the network status.\nGet kubeconfig To get the cluster kubeconfig, you can log in to the master node and retrieve it from the engine (e.g., it is located at /etc/rancher/k3s/k3s.yaml for K3s) or use the Kairos CLI. If using the CLI, you must be connected to the bridge or logged in from one of the nodes and run the following command in the console:\nkairosctl get-kubeconfig \u003e kubeconfig Note Note that you must run kairos bridge in a separate window as act like kubectl proxy and access the Kubernetes cluster VPN. Keep the kairos bridge command running to operate the cluster. ","categories":"","description":"Install Kairos with p2p support","excerpt":"Install Kairos with p2p support","ref":"/docs/installation/p2p/","tags":"","title":"P2P support"},{"body":"Things can go wrong. This section tries to give guidelines in helping out identify potential issues.\nIt is important first to check out if your issue was already submitted in the issue tracker.\nKairos Kairos UKI Gathering logs To gather useful logs and help developers spot right away issues, it’s suggested to boot with console=tty0 rd.debug enabled for example:\nTo edit the boot commands, type ’e’ in the boot menu. To boot with the changes press ‘CTRL+X’.\nIn case logs can’t be acquired, taking screenshot or videos while opening up issues it’s strongly recommended!\nAnother option that can be enabled is immucore debug logs with rd.immucore.debug\nAfter booting, you can find the logs from immucore under /run/immucore/ (whether you enabled debug output or not). Check the immucore README for more configuration parameters.\nIn order to gather the logs, you can use the kairos-agent logs command:\nlocalhost:~# kairos-agent logs 2025-07-07T10:14:09Z INF Kairos Agent version=v0.0.1 2025-07-07T10:14:09Z INF creating a runtime 2025-07-07T10:14:09Z INF detecting boot state 2025-07-07T10:14:09Z INF Boot Mode boot_mode=active_boot 2025-07-07T10:14:09Z INF Boot in uki mode result=false 2025-07-07T10:14:09Z INF Kairos Agent version=v0.0.1 2025-07-07T10:14:09Z INF creating a runtime 2025-07-07T10:14:09Z INF detecting boot state 2025-07-07T10:14:09Z INF Boot Mode boot_mode=active_boot 2025-07-07T10:14:09Z INF Boot in uki mode result=false 2025-07-07T10:14:09Z INF [2814] systemd not available, skipping journal log collection 2025-07-07T10:14:09Z INF [2814] Logs collected successfully to ./kairos-logs.tar.gz This command will collect logs from various components of the system and package them into a tarball named kairos-logs.tar.gz. You can then attach this file when reporting issues.\nInitramfs breakpoints Initramfs can be instructed to drop a shell in various phases of the boot process. For instance:\nrd.break=pre-mount rd.shell: Drops a shell before setting up mount points. rd.break=pre-pivot rd.shell: Drops a shell before switch-root Disable immutability It is possible to disable immutability by adding rd.cos.debugrw to the kernel boot commands.\nSee also Dracut debug docs Note The workflow for building UKI images is not yet set in stone and might change in the future so currently we cannot advise how to proceed here to build the UKI images with debug options yet. This will be updated in the future.\nCurrently on UKI mode we cannot easily change the cmdline to provide debug options easily as the cmdline is measured by the TPM and the system will not boot if the cmdline is changed.\nEnable Immucore debug logs In case of boot issues, Immucore accepts the rd.immucore.debug parameter in the cmdline to enable debug logs.\nAfter booting, you can find the logs from immucore under /run/immucore/ (whether you enabled debug output or not).\nDisabling SecureBoot In case of issues with SecureBoot, it’s possible to disable it from the UEFI settings and make Immucore boot without it by setting the rd.immucore.securebootdisabled parameter in the cmdline. Note that this is not supported and its only provided for troubleshooting purposes as it defeats the purpose of the security features provided by SecureBoot.\nRoot permission By default, there is no root user set. A default user (kairos) is created and can use sudo without password authentication during LiveCD bootup.\nGet back the kubeconfig On all nodes, which are deployed with the P2P full-mesh feature of the cluster, it’s possible to invoke kairos get-kubeconfig to recover the kubeconfig file.\nReporting issues If you are reporting a bug, please open an issue on the Kairos GitHub repository\nWhen reporting issues, please provide as much information as possible. This includes:\nThe version of Kairos you are using (Attach the full /etc/os-release file) The hardware you are running Kairos on (In case of suspecting hardware compatibility issues) The steps to reproduce the issue Any logs or screenshots you have Any other relevant information If you are using a custom configuration, please provide the configuration file Some small things you can do to provide us with the best information possible:\nWhen running kairos-agent commands, please add the --debug flag and attach the output to the issue You can also run kairos-agent state and attach the output to the issue as that provides information about various components of the system You can also run kairos-agent config and attach the output to the issue as that provides the current cloud-config for the system (Be aware that any sensitive information should be redacted before attaching the output) Immucore logs, available under /run/immucore/ can be useful to attach as well Agent logs, available under /var/log/kairos/*.log which are stored on each run of the agent Journalctl logs for the following services (use journalctl -u \u003cservice\u003e to get the logs) can be useful: cos-setup-network cos-setup-fs cos-setup-boot kairos Trusted boot TPM Event Logs The Trusted Platform Module (TPM) logs contain important security-related measurements that are taken during the boot process. These logs are instrumental in ensuring that the boot process is secure and has not been tampered with.\nViewing TPM Event Logs To view TPM event logs, you can use the tpm2_eventlog command as follows:\ntpm2_eventlog /sys/kernel/security/tpm0/binary_bios_measurements However, for a more detailed and structured view, especially when analyzing what contributes to a specific PCR (Platform Configuration Register) value like PCR 7, the systemd-pcrlock tool provides enhanced visibility:\n/usr/lib/systemd/systemd-pcrlock This tool is particularly useful for understanding the sequence of events that influence each PCR value during the boot process.\nKey Management in Firmware To ensure the integrity and security of your system, it is critical to manage and verify the keys used by the firmware, especially when dealing with upgrades and security patches.\nListing Enrolled Keys You can list keys that are currently enrolled in the firmware using sbctl:\nsbctl list-enrolled-keys For further details and usage, refer to the sbctl tool’s GitHub repository: https://github.com/Foxboron/sbctl\nVerifying EFI Signature To verify if the EFI executables are signed with the correct keys, you can use tools like goverify:\ngo get -u github.com/Foxboron/go-uefi/cmd/goverify goverify This step ensures that any upgrades or installations are performed using authorized and validated keys, thereby preserving the security of the system. More information and source code can be found on GitHub: https://github.com/Foxboron/go-uefi/blob/master/cmd/goverify/main.go\nChecking if Secure Boot is Enabled Secure Boot is a security standard that helps to ensure that a device boots using only software that is trusted by the Original Equipment Manufacturer (OEM). When Secure Boot is enabled, the firmware checks each piece of boot software for a valid digital signature.\nCommand to Check Secure Boot Status To check if Secure Boot is enabled on your system, use the following command:\ndmesg | grep -i secure The output will display entries related to Secure Boot, indicating whether it is enabled and showing details about the X.509 certificates loaded during the boot process, as seen in the example output provided in your notes.\n","categories":"","description":"","excerpt":"Things can go wrong. This section tries to give guidelines in helping …","ref":"/docs/reference/troubleshooting/","tags":"","title":"Troubleshooting"},{"body":"Trusted boot is a combination of technologies that allows us to enhance the security posture of a running system. It is composed by FDE, Secure Boot and Measured Boot. Trusted boot is an architectural requirement of SENA (Secure Edge Native Architecture) and is a key component of Kairos.\nYou can read more about Trusted Boot in https://0pointer.de/blog/brave-new-trusted-boot-world.html and about SENA here: https://kairos.io/blog/2023/04/18/kairos-is-now-part-of-the-secure-edge-native-architecture-by-spectro-cloud-and-intel/\nBy combining Secure Boot, Measured Boot and FDE we can guarantee that a system was not tampered with, and the user-data is protected by cold attacks, we refer to this combination of technologies stacked together with “Trusted Boot” or “Trusted Boot Experience”.\nFDE stands for Full Disk Encryption. It is a security measure that encrypts the entire contents of a disk drive, including the operating system, system files, and user data. The purpose of FDE is to protect data stored on the disk from unauthorized access in the event of theft or loss of the device.\nSecure Boot is a security feature in modern computer systems that ensures only properly signed and authenticated OSes are allowed to run during the boot process. It helps prevent the loading of malicious or unauthorized code at the early stages of the system startup, thus helping in protecting the integrity of the boot process.\nMeasured Boot, on the other hand, is a security mechanism that goes beyond Secure Boot. It involves creating a record, or “measurements,” of each step in the boot process and storing these measurements in a specific hardware dedicated to cryptographically secure operations ( trusted platform module, or TPM) or a similar secure storage. This allows the system to verify the integrity of each component of the boot process and detect any unauthorized changes. Measured Boot provides a more comprehensive and continuous security check throughout the boot sequence.\nUKI and USI UKI is a specific file format tailored to achieve a tamper-proof system and encryption of user-data bound to the silicon by relying on HW capabilities.\nUKI stands for “Unified Kernel Images”. UKI files are a single, fat binary that encompasses the OS and the needed bits in order to boot the full system with a single, verified file. You can read technical details here: Brave New Trusted Boot World.\nTrusted Boot in Kairos works by generating UKI images from container images. The UKI file is a single, fat binary that encompasses the OS and the needed bits in order to boot the full system with a single, verified file. This file can be used for upgrades and used as usual in the lifecycle of the Kairos node.\nThe UKI file is signed with the Secure Boot keys, and the user-data is encrypted with the PCR policies. The UKI file is then loaded by the firmware and booted directly, without any second stage or system pivoting. This is why the UKI file can grow large, and why it requires a specific firmware that supports booting large EFI files.\nDue to the design choice to not boot into a second stage, this is being refered nowadays as USI (Unified System Image) instead of UKI, or more simply “Bootstrap Image”. The benefits of using Unified system images in opposite of UKIs are that the system can be upgraded without the need of a second stage, and that the entire system is verified and signed.\nBuilding process Trusted Boot requires you to create a specific installable ISO file that includes the UKI files and a container image used for upgrades. Each time you upgrade a system, you must regenerate these assets using the same keys and then use the newly generated container image for the upgrade process.\nThe UKI files are generated from container images as usual.\nThe process to generate the installable medium is described in the Trustedboot installation documentation. Internally we rely on the osbuilder tool to generate all the installation artifacts from a single container image.\nBooting process The booting process of an installed system with Trusted Boot is different from a standard Kairos installation, and can be split down in 4 steps:\nThe Firmware loads the systemd-boot bootloader from the EFI partition systemd-boot loads the UKI file from the EFI partition (that can be either the Active system, the passive or recovery system) The EFI system starts. The kernel and the initrd are loaded from the UKI file, and the kernel is booted with the command line specified in the UKI file. The initrd will decrypt the user-data and mount the portions of it in the root filesystem. This includes for instance any changes to /etc (like adding new users and passwords), /usr/local, and all the mount bindpoints specified in the configuration file (see Bind mount documentation ). There is no second stage loaded and no system pivoting, the system is booted directly from the UKI file. Booting system There is no difference with a layout of a standard Kairos system (as explained in the Immutable OS page), however in this setup now the partitions containing data are always encrypted:\nList of persistent data path overlayed and encrypted (see also the bind mount documentation to customize it with your own paths). /oem encrypted /usr/local encrypted /etc ephemeral /usr read only / immutable User data encryption and key generation It is required in order to generate USI images to have a set of keys and certificates. In order to understand why those keys are required, we need to understand how the user-data is encrypted and how the system is booted.\nThe keys are used to sign the UKI file, and to generate a PCR policy keypair required later on by the system in order to decrypt the encrypted partitions. The keys and certificates are generated with the auroraboot tool, that is available in the auroraboot container image.\nExpanding the system with system extensions Check the relevant documentation on how to extend the system with system extensions and how to use /opt with system extensions.\nTrusted Boot - Boot Assessment See the Trusted Boot - Boot Assessment documentation for more information on how to enable automatic boot assessment with Trusted Boot and how to make your own services participate in the boot assessment process.\nConsiderations Booting command lines UKI file’s signatures are including also the kernel command line, so any change to the kernel command line will require a new UKI file to be generated and the installer image to be rebuilt. This implies that you cannot change the booting options once the system is installed (and the system won’t be able to access the encrypted data)\nReferences UEFI specification UEFI (SLE documentation) https://cdrdv2-public.intel.com/671120/a-tour-beyond-bios-implementing-uefi-authenticated-variables-in-smm-with-edkii.pdf SUPPORT OF SECURE BOOT IN SYSTEMD-BOOT PROJECT UKI file format Secure Boot For more information about installing Kairos with Trusted Boot, see the Trusted Boot Installation Guide.\nFor more information about customizing Kairos images, see the Customizing Images guide.\nFor more information about the immutable nature of Kairos, see the Immutable Architecture guide.\nFor more information about system extensions, see the System Extensions guide.\nFor more informations about revoking certificates, see Revoking secure boot access\nFor a complete example of boot assessment with Trusted Boot, see the Boot Assessment with Trusted Boot guide.\n","categories":"","description":"","excerpt":"Trusted boot is a combination of technologies that allows us to …","ref":"/docs/architecture/trustedboot/","tags":"","title":"Trusted Boot Architecture"},{"body":"“Trusted Boot” is a combination of technologies that allows us to guarantee that a system was not tampered with, and the user-data is protected by cold attacks, it is composed by FDE, Secure Boot and Measured Boot.\nIf you want to learn more on what Trusted Boot is and how it works, see the Trusted Boot Architecture page. This page describes how to enable Trusted Boot support in Kairos.\nKairos supports Trusted boot by generating specific installable medium. This feature is optional and works alongside how Kairos works.\nRequirements The Hardware that will run Kairos needs to have the following requirements:\nSecure boot available in the system The Hardware should have a TPM chip or fTPM enabled Fast boot disabled at BIOS level ( See also https://github.com/kairos-io/kairos/issues/2579 ) The Hardware should be capable of booting large EFI files (\u003e32MB) Base image of the OS needs to have at least systemd 252 or newer ( for example ubuntu \u003e=23.10 or fedora \u003e=38 ) To build the installable medium you need the following installed in the system you use to build the installable medium:\nDocker Git A Linux machine with KVM (for testing the images locally) Usage In order to boot into UKI mode, you need to build a special ISO file with the UKI files. To build this medium you have to generate a set of keypairs first: one for the Secure boot and one for the PCR policies required to encrypt the user-data.\nAny change, or upgrade of the node to a new version of the OS requires those assets to be regenerated with these keypairs, including the installer ISO, and the EFI files used for upgrading. The keys are used to sign and verify the EFI files, and the PCR policies are used to encrypt and decrypt the user-data, and thus are required to be the same for the whole lifecycle of the node.\nThe steps below will guide you into generating the installable assets, and how to re-generate the assets to upgrade the node to a new version of the OS.\nKey generation Keys can be generated from scratch, or the Microsoft certificates can be used, alternatively, if you can export the keys from your BIOS/UEFI you can use the same PK keys.\nBy default keys are generated including the Microsoft ones, but you can skip them if you know what you are doing (see below).\nTo generate the Secure boot certificates and keys along with the Microsoft keys run the following commands:\nMY_ORG=\"Acme Corp\" # Generate the keys docker run -v $PWD/keys:/work/keys -ti --rm quay.io/kairos/auroraboot:v0.13.0 genkey --expiration-in-days 365 -o /work/keys \"$MY_ORG\" Warning Substitute $MY_ORG for your own string, this can be anything but it help identifying the Keys. The keys duration can specified with --expiration-in-days. It is not possible to create keys that do not expire, but it is possible to specify an extremely large value (e.g. 200 years, etc.) Warning It is very important to preserve the keys generated in this process in a safe place. Loosing the keys will prevent you to generate new images that can be used for upgrades. Custom certificates by skipping Microsoft certificate keys Warning Some firmware is signed and verified with Microsoft’s or other vendor keys when secure boot is enabled. Removing those keys could brick them. It is preferable to not use the Microsoft certificates for precaution and security reasons, however as this might be potentially a dangerous action, only do this if you know what you are doing. To check if your firmware is signed with Microsoft’s keys, you can check https://github.com/Foxboron/sbctl/wiki/FAQ#option-rom. See also: https://wiki.archlinux.org/title/Unified_Extensible_Firmware_Interface/Secure_Boot#Enrolling_Option_ROM_digests. If your hardware supports booting with custom Secure Boot keys, you can optionally create keys from scratch and skip the Microsoft certificate keys. To do so, run the following command:\nMY_ORG=\"Acme Corp\" # Generate the keys docker run -v $PWD/keys:/work/keys -ti --rm quay.io/kairos/auroraboot:v0.13.0 genkey --skip-microsoft-certs-I-KNOW-WHAT-IM-DOING --expiration-in-days 365 -o /work/keys \"$MY_ORG\" Exporting keys from BIOS/UEFI and using them This is useful if you want to handle mass-installations where you don’t want to enroll the generated keys manually in your bios/uefi.\nSome hardware might require additional vendor keys, that could be e.g. used to sign additional firmware. In order to re-use the same set of keys in the machine, you can export the keys from the BIOS/UEFI and use them to generate the UKI files.\nSome BIOS/UEFI allows to export the keys to USB stick directly from the BIOS/UEFI setup menu (for example HPE). If you have the keys in a USB stick, you can copy in a directory and use them to generate the UKI files.\nMY_ORG=\"Acme Corp\" MACHINE_CERTS=\"$PWD/path/to/machine-certs\" # ~$ tree $MACHINE_CERTS # /path/to/machine-certs/ # ├── db # ├── dbx # ├── KEK # └── PK # Generate the keys docker run -v $MACHINE_CERTS:/work/machine-keys -v $PWD/keys:/work/keys -ti --rm quay.io/kairos/auroraboot:v0.13.0 genkey --custom-cert-dir /work/machine-keys --expiration-in-days 365 -o /work/keys \"$MY_ORG\" Warning This command can be combined with the --skip-microsoft-certs-I-KNOW-WHAT-IM-DOING flag to avoid auto-enrolling the Microsoft keys if not needed (or already present in the “custom certs”) Using a hardware key for signing You can use hardware keys which are compatible with the PKCS #11 standard, like for example a nitrokey to sign the UKI files. In order to do so, you need to have the keys generated in the hardware key and then generate a certificate signed by the key like so:\nGenerate your key in the hardware key, for example with a nitrokey:\ngpg --card-edit # In the gpg console, run the following commands: admin generate # Follow the instructions to generate the key, you can use the default values # After the key is generated, you can run the following command to list the keys: $ pkcs11-tool --module opensc-pkcs11.so -L Available slots: Slot 0 (0x0): Nitrokey Nitrokey Start (FSIJ-1.2.19-C5B562D9) 00 00 token label : OpenPGP card (User PIN) token manufacturer : OpenPGP project token model : PKCS#15 emulated token flags : login required, rng, token initialized, PIN initialized hardware version : 2.0 firmware version : 2.0 serial num : fffec5b562d9 pin min/max : 6/127 uri : pkcs11:model=PKCS%2315%20emulated;manufacturer=OpenPGP%20project;serial=fffec5b562d9;token=OpenPGP%20card%20%28User%20PIN%29 Slot 1 (0x1): Nitrokey Nitrokey Start (FSIJ-1.2.19-C5B562D9) 00 00 token label : OpenPGP card (User PIN (sig)) token manufacturer : OpenPGP project token model : PKCS#15 emulated token flags : login required, rng, token initialized, PIN initialized hardware version : 2.0 firmware version : 2.0 serial num : fffec5b562d9 pin min/max : 6/127 uri : pkcs11:model=PKCS%2315%20emulated;manufacturer=OpenPGP%20project;serial=fffec5b562d9;token=OpenPGP%20card%20%28User%20PIN%20%28sig%29%29 # You can use the slot-id and id to refer to the key in the hardware key # For example, to refer to the key in slot 1, you can use the following pkcs11 url: # pkcs11:slot-id=1;id=%01 Now create your SSL config in order to use the hardware key to sign the certificate:\n$ cat \u003c\u003cEOF \u003e openssl-pkcs11.conf openssl_conf = openssl_init [openssl_init] providers = provider_sect [provider_sect] default = default_sect base = base_sect pkcs11 = pkcs11_sect [default_sect] activate = 1 [base_sect] activate = 1 [pkcs11_sect] activate = 1 pkcs11-module-path = /usr/lib/pkcs11/opensc-pkcs11.so # Change to the path of yours, can differ by OS EOF Generate a certificate signing request (CSR) with the private key in the hardware key:\n$ OPENSSL_CONF=openssl-pkcs11.conf openssl req -new -key \"pkcs11:slot-id=1;id=%01\" \\ -sha256 -out nitrokey.csr.pem -subj \"/CN=SecureBoot Key/\" You can append ;pin-value= to the pkcs11 url to not have to write it. If you don’t add it, openssl will ask you for it\nSign the CSR with the private key in the hardware key:\n$ OPENSSL_CONF=openssl-pkcs11.conf openssl x509 -req -days 3650 -in nitrokey.csr.pem \\ -signkey \"pkcs11:slot-id=1;id=%01\" -out nitrokey.crt.pem Now you can use the hardware key and certificate to sign the UKI files.\nBuilding installable medium To build the installable medium you need to run the following commands:\nFrom a container image From a directory CONTAINER_IMAGE=quay.io/kairos/@flavor:@flavorRelease-core-amd64-generic-master-uki docker run -ti --rm -v $PWD/build:/result -v $PWD/keys/:/keys quay.io/kairos/auroraboot:v0.13.0 build-uki -t iso -d /result/ --public-keys /keys --tpm-pcr-private-key $PATH_TO_TPM_KEY --sb-key $PATH_TO_SB_KEY --sb-cert $PATH_TO_SB_CERT $CONTAINER_IMAGE # to build an EFI file only docker run -ti --rm -v $PWD/build:/result -v $PWD/keys/:/keys quay.io/kairos/auroraboot:v0.13.0 build-uki -t uki -d /result/ --public-keys /keys --tpm-pcr-private-key $PATH_TO_TPM_KEY --sb-key $PATH_TO_SB_KEY --sb-cert $PATH_TO_SB_CERT $CONTAINER_IMAGE # Assuming you have a \"rootfs\" directory with the content of the OS # If the image is in a directory ($PWD/rootfs) you can use the following command docker run -ti --rm -v $PWD/build:/result -v $PWD/rootfs:/rootfs -v $PWD/keys/:/keys quay.io/kairos/auroraboot:v0.13.0 build-uki -t iso -d /result/ --public-keys /keys --tpm-pcr-private-key $PATH_TO_TPM_KEY --sb-key $PATH_TO_SB_KEY --sb-cert $PATH_TO_SB_CERT dir:/rootfs/ Let’s explain some of the flags used here, especially the ones related to Trusted Boot keys:\n-t iso specifies that the output should be an installable medium (ISO file). You can use -t uki to generate an EFI file only. Or -t container to generate a container image with the UKI files on it, perfect to push to a remote registry for upgrades.\\ -d /result/ specifies the output directory where the output type will be saved. --tpm-pcr-private-key specifies the path to the private key used to sign the PCR policies. This is required for the user-data encryption. --sb-key specifies the path to the Secure Boot key used to sign the UKI files. --sb-cert specifies the path to the Secure Boot certificate used to sign the UKI files. --public-keys /keys specifies the directory where the public keys are stored. These are the keys that are auto enrolled on first boot. They are 3 (DB,KEK, PK) and are in .auth format. Keys format The .auth format in Secure Boot refers to EFI Signature Lists (ESLs) wrapped in an authentication structure, which allows UEFI firmware to verify and install Secure Boot keys securely. These files are used when manually enrolling Secure Boot keys into a UEFI system, typically through the firmware setup interface. .auth files are signed versions of .esl files. An .esl file is a binary format defined by the UEFI specification that wraps one or more EFI Signature Data entries (usually X.509 certs or SHA256 hashes). For a more secure process you can use a hardware key to generate and sign the certificate and sign the EFI files, so you dont need to keep the private key in the filesystem.\nTo do so, --sb-key accepts a pkcs11 url, for example:\ndocker run -ti --rm -v /run/pcscd:/run/pcscd -v $PWD/build:/result -v $PWD/keys/:/keys quay.io/kairos/auroraboot:v0.13.0 build-uki -t iso -d /result/ --public-keys /keys --tpm-pcr-private-key $PATH_TO_TPM_KEY --sb-key \"pkcs11:slot-id=1;id=%01\" --sb-cert $PATH_TO_SB_CERT $CONTAINER_IMAGE Warning Notice that for this to work you need to have the pcscd service running in your system, and the hardware key must be connected to the system. The command mounts the /run/pcscd directory to the container, so that the container can access the hardware key. For more info about pkcs11 urls, see the RFC 7512 which describes the format of the pkcs11 urls.\nBundling system extensions during the installable medium build System extensions can be bundled in the installable medium. To bundle system extensions, you need to create a new image with the extensions you want to add. System extensions are bundled in the root on the media install. When building your Trusted Boot image, you can add the system extensions to the iso by using the --overlay-iso flag and pointing it to the directory containing the system extensions.\nFrom a container image From a directory # Assuming your system extensions are stored on $PWD/system-extensions CONTAINER_IMAGE=quay.io/kairos/@flavor:@flavorRelease-core-amd64-generic-master-uki docker run -ti --rm -v $PWD/system-extensions:/system-extensions -v $PWD/build:/result -v $PWD/keys/:/keys quay.io/kairos/auroraboot:v0.13.0 build-uki -t iso -d /result/ --public-keys /keys --tpm-pcr-private-key $PATH_TO_TPM_KEY --sb-key $PATH_TO_SB_KEY --sb-cert $PATH_TO_SB_CERT --overlay-iso /system-extensions $CONTAINER_IMAGE # to build an EFI file only docker run -ti --rm -v $PWD/build:/result -v $PWD/keys/:/keys quay.io/kairos/auroraboot:v0.13.0 build-uki -t uki -d /result/ --public-keys /keys --tpm-pcr-private-key $PATH_TO_TPM_KEY --sb-key $PATH_TO_SB_KEY --sb-cert $PATH_TO_SB_CERT $CONTAINER_IMAGE # Assuming you have a \"rootfs\" directory with the content of the OS # If the image is in a directory ($PWD/rootfs) you can use the following command # Assuming your system extensions are stored on $PWD/system-extensions docker run -ti --rm -v $PWD/system-extensions:/system-extensions -v $PWD/build:/result -v $PWD/rootfs:/rootfs -v $PWD/keys/:/keys quay.io/kairos/auroraboot:v0.13.0 build-uki -t iso -d /result/ --public-keys /keys --tpm-pcr-private-key $PATH_TO_TPM_KEY --sb-key $PATH_TO_SB_KEY --sb-cert $PATH_TO_SB_CERT --overlay-iso /system-extensions dir:/rootfs/ Config files Included in the artifact will be a configuration file. On the installation/upgrade media, it is called norole.conf but once it has been installed it will be named active.conf, passive.conf or recovery.conf which represents the configuration for the “cos”, “fallback” and “recovery” images respectively.\nThis configuration file includes the “title” of the artifact in the boot menu and the location of the “efi” file to boot. Here’s an example:\n# cat /efi/loader/entries/active.conf efi /EFI/kairos/active.efi title Kairos Branding You can overwrite the default “Kairos” title if you pass the --boot-branding flag to auroraboot.\nCONTAINER_IMAGE=quay.io/kairos/@flavor:@flavorRelease-core-amd64-generic-master-uki docker run -ti --rm -v $PWD/build:/result -v $PWD/keys/:/keys quay.io/kairos/auroraboot:v0.13.0 build-uki -t iso -d /result/ --public-keys /keys --tpm-pcr-private-key $PATH_TO_TPM_KEY --sb-key $PATH_TO_SB_KEY --sb-cert $PATH_TO_SB_CERT --boot-branding \"My Awesome OS\" $CONTAINER_IMAGE Your config file should look now like this:\n# cat /efi/loader/entries/active.conf efi /EFI/kairos/active.efi title My Awesome OS You can use a custom boot splash screen by specifying the --splash flag when calling auroraboot.\nCONTAINER_IMAGE=quay.io/kairos/@flavor:@flavorRelease-core-amd64-generic-master-uki docker run -ti --rm -v $PWD/build:/result -v $PWD/keys/:/keys -v $PWD/splash/:/splash quay.io/kairos/auroraboot:v0.13.0 build-uki -t iso -d /result/ --public-keys /keys --tpm-pcr-private-key $PATH_TO_TPM_KEY --sb-key $PATH_TO_SB_KEY --sb-cert $PATH_TO_SB_CERT --boot-branding \"My Awesome OS\" --splash /splash/my-awesome-splash.bmp $CONTAINER_IMAGE Info Splash images must be provided in 32 bit BMP format, no other image formats besides BMP are supported. Warning Remember when you build upgrade mediums, to also add your branding, otherwise the new active image will be named differently than your fallback and recovery ones. Version and cmdline in the config files In addition to having the “title” and “efi” attributes, you can include in the config files the OS “version” and “cmdline”. To do so, pass --include-version-in-config or --include-cmdline-in-config flags to auroraboot.\nInfo The cmdline, will only show what is different between the default cmdline and the currently used cmdline. If you pass --include-cmdline-in-config to a default installation, the attribute cmdline will be empty. cmdline awesome=true title Awesome OS efi /EFI/kairos/active.efi version master Installation The installation process is performed as usual and the Installation instructions can be followed, however the difference is that user-data will be automatically encrypted (both the OEM and the persistent partition) by using the TPM chip and the Trusted Boot mechanism.\nEnroll the keys in Secure Boot If your machine is in UEFI setup mode Secure Boot keys will be automatically enrolled. To enter UEFI Setup mode you need to clear the Secure Boot keys (PKs) from the BIOS/UEFI.\nIf UEFI setup mode is not available, you need to enroll the keys manually in the BIOS/UEFI.\nThis process can vary depending on the vendor, but in general you need to enter the BIOS/UEFI setup during early boot and import the keys, for an example outline you can check the steps for HPE Hardware.\nA video of the process of importing keys in QEMU is available here.\nUpgrades See the Trusted Boot Upgrade page.\nTesting the images locally To test the ISO file locally QEMU can be used. In order to test Secure Boot components you need an ed2k firmware with secureboot in QEMU. If you don’t have QEMU locally and/or you don’t have the correct dependencies you can follow the steps below that build a container image with QEMU and the needed dependencies and use that container to run the ISO file in a VM with Docker.\nBuild the container image with the QEMU/Secure Boot dependencies (note to replace disk size/cpu/ram not needed): docker build -t fedora-qemu -\u003c\u003cDOCKER FROM fedora RUN dnf install -y dnf-plugins-core RUN dnf config-manager --add-repo http://www.kraxel.org/repos/firmware.repo RUN dnf install -y edk2.git-ovmf-x64 qemu RUN dnf install -y swtpm wget WORKDIR / RUN wget -q https://github.com/mudler/cos-demo-labs/raw/tests/efivars.fd -O /efivars.fd WORKDIR /work RUN \u003c\u003cEOF echo \"#!/bin/bash -ex\" \u003e\u003e /entrypoint.sh echo '[ ! -e /work/disk.img ] \u0026\u0026 qemu-img create -f qcow2 \"/work/disk.img\" 35G' \u003e\u003e /entrypoint.sh echo '/usr/bin/qemu-system-x86_64 -drive if=pflash,format=raw,unit=0,file=\"/usr/share/edk2/ovmf/OVMF_CODE.secboot.fd\",readonly=on -drive if=pflash,unit=1,format=raw,file=\"/efivars.fd\" -accel kvm -cpu host -m 8096 -drive file=/work/disk.img,if=none,index=0,media=disk,format=qcow2,id=disk1 -device virtio-blk-pci,drive=disk1,bootindex=0 -boot order=dc -vga virtio -cpu host -smp cores=4,threads=1 -machine q35,smm=on -chardev socket,id=chrtpm,path=/tmp/swtpm-sock -tpmdev emulator,id=tpm0,chardev=chrtpm -device tpm-tis,tpmdev=tpm0 \\$@' \u003e\u003e /entrypoint.sh EOF RUN chmod +x /entrypoint.sh ENTRYPOINT [ \"/entrypoint.sh\" ] DOCKER Start a TPM socket: docker run --privileged --entrypoint swtpm -v $PWD/tpmstate:/tmp --rm -ti fedora-qemu socket --tpmstate dir=/tmp/ --ctrl type=unixio,path=/tmp/swtpm-sock --log level=20 --tpm2 Note: you need to keep the TPM container up and running for the VM to boot. Run the commands below in another terminal window.\nRun the container image with the ISO file (replace the iso file name with yours): # console only docker run --privileged -v $PWD/tpmstate:/tmp -v $PWD:/work -v /dev/kvm:/dev/kvm --rm -ti fedora-qemu -nographic -cdrom kairos-@flavor-@flavorRelease-core-amd64-generic-master.uki.iso Note: To stop the QEMU container you can use Ctrl-a x or Ctrl-a c to enter the QEMU console and then quit to exit.\nAfter installation, you can run the container image by booting only with the disk # console only docker run --privileged -v $PWD/tpmstate:/tmp -v $PWD:/work -v /dev/kvm:/dev/kvm --rm -ti fedora-qemu -nographic Note: you need to keep the TPM container up and running for the VM to boot.\nData Encryption The user-data will be automatically encrypted during installation, along with the OEM and the persistent partition by using the TPM chip and the Trusted Boot mechanism.\nAdditional partitions Additional partitions can be encrypted and specified as part of the cloud-config used during the installation process, for example:\ninstall: extra-partitions: - name: second_partition size: 100 fs: ext2 label: PARTITION_TWO encrypted_partitions: - PARTITION_TWO A full example can be:\ninstall: device: \"auto\" # Install to the biggest drive auto: true # Enables auto installation partitions: persistent: size: 500 # Set persistent partition to 500MB (otherwise takes the whole disk) extra-partitions: - name: second_partition size: 100 fs: ext2 label: PARTITION_TWO encrypted_partitions: - PARTITION_TWO Notes Install additional files from the Installer ISO to the encrypted portions The ISO installer images can be used to install additional content in the encrypted portion of the disk.\nThe osbuilder image can overlay additional files into the iso. For example specify --overlay-iso /additional/path to have added the files in the folder inside the ISO. The content can be accessed during installation in /run/initramfs/live. For example, to install content of the ISO inside the OEM partition, and to restore those after a reset we can use the following cloud config:\n#cloud-config install: device: \"/dev/vda\" auto: true partitions: oem: size: 4000 fs: ext4 users: - name: \"kairos\" passwd: \"kairos\" stages: after-install: - commands: - echo \"Copying files to oem and persistent\" - /usr/lib/systemd/systemd-cryptsetup attach persistent $(findfs PARTLABEL=persistent) - tpm2-device=auto - /usr/lib/systemd/systemd-cryptsetup attach oem $(findfs PARTLABEL=oem) - tpm2-device=auto - mount /dev/mapper/persistent /usr/local - mount /dev/mapper/oem /oem - mkdir -p /usr/local/.state/opt.bind - mkdir -p /oem/opt.bind - cp -rfv /run/initramfs/live/data/* /usr/local/.state/opt.bind - cp -rfv /run/initramfs/live/data/* /oem/opt.bind - umount /dev/mapper/persistent - umount /dev/mapper/oem - cryptsetup close /dev/mapper/persistent - cryptsetup close /dev/mapper/oem after-reset: - commands: - | /bin/bash \u003c\u003c'EOF' #!/bin/bash set -e echo \"Copying files from oem to persistent\" # /oem was mounted in my tests. Let's umount it to be sure. umount /oem || true # Close all encrypted partitions for p in $(ls /dev/mapper/vda*); do cryptsetup close $p; done /usr/lib/systemd/systemd-cryptsetup attach persistent $(findfs PARTLABEL=persistent) - tpm2-device=auto /usr/lib/systemd/systemd-cryptsetup attach oem $(findfs PARTLABEL=oem) - tpm2-device=auto mount /dev/mapper/persistent /usr/local mount /dev/mapper/oem /oem mkdir -p /usr/local/.state/opt.bind cp -rfv /oem/opt.bind/* /usr/local/.state/opt.bind umount /dev/mapper/persistent umount /dev/mapper/oem cryptsetup close /dev/mapper/persistent cryptsetup close /dev/mapper/oem EOF Mount partitions after install /oem and /usr/local can be mounted after installation to prepare content before first-boot.\n# Note: replace /dev/vda2 with the oem partition location (see with `blkid`) # [root@ ~]# blkid # /dev/sr0: BLOCK_SIZE=\"2048\" UUID=\"2024-02-05-17-00-05-00\" LABEL=\"UKI_ISO_INSTALL\" TYPE=\"iso9660\" # /dev/vda2: UUID=\"8bfa06f9-ca4f-56dc-90c9-49cf20f4f45e\" TYPE=\"crypto_LUKS\" PARTLABEL=\"oem\" PARTUUID=\"63deb673-ec99-46f6-9cb6-8399315e4f19\" # /dev/vda3: UUID=\"85c39d0f-4867-5227-8334-f5eec606d9eb\" TYPE=\"crypto_LUKS\" PARTLABEL=\"persistent\" PARTUUID=\"d01d9b51-d61a-4b7e-bb1a-8af5c212a213\" # /dev/vda1: LABEL_FATBOOT=\"COS_GRUB\" LABEL=\"COS_GRUB\" UUID=\"1C4C-97AA\" BLOCK_SIZE=\"512\" TYPE=\"vfat\" PARTLABEL=\"efi\" PARTUUID=\"6e42d80e-d67a-462b-b99c-2c1b5dda91cf\" # /dev/mapper/oem: LABEL=\"COS_OEM\" UUID=\"d10fc63d-9387-442c-9db8-a00e081858ec\" BLOCK_SIZE=\"1024\" TYPE=\"ext4\" # Mount OEM /usr/lib/systemd/systemd-cryptsetup attach oem $(findfs PARTLABEL=oem) - tpm2-device=auto mount /dev/mapper/oem /oem # Mount persistent /usr/lib/systemd/systemd-cryptsetup attach persistent $(findfs PARTLABEL=persistent) - tpm2-device=auto mount /dev/mapper/persistent /usr/local To mount /oem and /usr/local after install you can also manually call kcrypt unlock-all. However this isn’t supported yet.\nForce certificates auto-enrollments Warning Don’t run auto-enrollments by default! this option is here after you are sure that the certificates generated are correct and after you have verified that manuall enrolling the certificates does not brick your device!\nThis is specific for large-scale deployments to generate auto-installing ISOs that are meant to be used on similar hardware multiple times (thus the process only needs to be manually verified once).\nIf you want to force the auto-enrollment of the certificates in the BIOS/UEFI, you can use the --secure-boot-enroll flag in the build-uki command.\nCONTAINER_IMAGE=quay.io/kairos/@flavor:@flavorRelease-core-amd64-generic-master-uki docker run -ti --rm -v $PWD/build:/result -v $PWD/keys/:/keys quay.io/kairos/auroraboot:v0.13.0 build-uki --secure-boot-enroll force -t iso -d /result/ --public-keys /keys --tpm-pcr-private-key $PATH_TO_TPM_KEY --sb-key $PATH_TO_SB_KEY --sb-cert $PATH_TO_SB_CERT $CONTAINER_IMAGE Additional efi entries When booting in UKI mode, the cmdline is part of the signed artifact. This means that in order to pass additional options through the cmdline, this has to be done at build time. For that reason, auroraboot build-uki supports the following arguments:\n--extend-cmdline: Use this option to add cmdline parameters to the default entries (active/passive/recovery/auto-reset). This option will not generate additional entries in the systemd-boot menu. --extra-cmdline: Use this option to generate additional entries in the systemd-boot menu. For each one of the default entries, a new entry will be created with the specified cmdline (added to the default cmdline). --single-efi-cmdline: Use this option to generate entries that are not related to default ones. This option will generate a single entry with the provided cmdline (added to the default cmdline). This argument can be used multiple times. Examples:\nBuilding with a command like this:\nauroraboot build-uki ...more options... --single-efi-cmdline \"Lets debug: rd.debug rd.immucore.debug\" Will create a boot menu with the default entries, plus one more: “Lets debug” which will have the debug parameters set. Notice how the title of the entry and the cmdline to be used are split by a colon.\nBuilding with a command like this:\nauroraboot build-uki ...more options... --extra-cmdline \"rd.debug rd.immucore.debug\" Will create a boot menu with the default entries, plus one more for each of the active/passive/recovery entries. Each of these entries will have the debug parameters set.\nBuilding with a command like this:\nauroraboot build-uki ...more options... --extend-cmdline \"rd.debug rd.immucore.debug\" Will create a boot menu with just the default entries but this time they will have the debug parameters set.\nNOTE: --boot-branding is applied to --single-efi-cmdline too. For example, this command:\nauroraboot build-uki ...more options... --boot-branding \"My awesome OS\" --single-efi-cmdline \"Debug logs: rd.debug rd.immucore.debug\" will create this entry:\nMy awesome OS (Debug logs) with the additional cmdline parameters.\n","categories":"","description":"Learn how to enable Trusted Boot with Secure Boot, full disk encryption, and measured boot to protect systems from tampering.","excerpt":"Learn how to enable Trusted Boot with Secure Boot, full disk …","ref":"/docs/installation/trustedboot/","tags":"","title":"Trusted Boot Installations"},{"body":" Warning This section is still a work in progress and only available in Kairos v3.x releases and alphas. This section covers how to upgrade a Kairos node with Trusted Boot enabled.\nSee the Trusted Boot Installation and Trusted Boot Architecture pages for more references.\nUpgrades In order to upgrade a node to a new version of the OS, you need to generate again the installable medium with the same keys used in the steps before.\nNote The resulting container image can be used for upgrades with kairos-agent. The process will generate an EFI file which we will pack into a container image that will be used to upgrade the node.\nGenerate the upgrade image Build the Container image used for upgrades CONTAINER_IMAGE=quay.io/kairos/@flavor:@flavorRelease-core-amd64-generic-master docker run --rm -v $PWD/keys:/keys -v $PWD:/work -ti quay.io/kairos/auroraboot:v0.13.0 build-uki -t container --public-keys /keys --tpm-pcr-private-key $PATH_TO_TPM_KEY --sb-key $PATH_TO_SB_KEY --sb-cert $PATH_TO_SB_CERT $CONTAINER_IMAGE Push the upgrade image to a registry # Now you can load upgrade_image.tar to a registry and use it with kairos-agent docker load -i *.tar #401b8e83daf6: Loading layer [==================================================\u003e] 1.263GB/1.263GB # Loaded image: kairos_uki:v3.0.0-alpha2 docker push \u003cIMAGE_NAME\u003e Upgrade with kairos-agent Warning For upgrading use the image that has been loaded with docker load in the step earlier. That contains the EFI files for the upgrade process. Do not use the source/base image used ($CONTAINER_IMAGE in the example above) used as input! Let’s assume an upgrade image named acme.com/acme/kairos has been built and pushed as described in the section above. From a shell inside a running Kairos OS, the following command will upgrade to the new version:\nkairos-agent upgrade --source oci:acme.com/acme/kairos Upgrades with Kubernetes To upgrade Kairos with Kubernetes, the Kairos operator needs to be deployed on the target cluster. Read the instructions here.\nA NodeOp resource needs to be created which will use the image generated in the step above. Since that image only contains the EFI files for the upgrade and in order to be able use any ImagePullSecrets defined on the cluster, we will create an image that can be used to start a Pod and also contains the efi and conf files for the upgrade.\nAssuming an upgrade image named acme.com/acme/kairosUpgradeImage was built using a Kairos image named acme.com/acme/baseImage, the following dockerfile will create an image that can be used to start a Plan for upgrade:\nFROM acme.com/acme/kairos as upgradeImage FROM acme.com/acme/baseImage COPY --from=upgradeImage / /trusted-boot (Let’s call the image built with this dockerfile planImage:vx.y.z)\nThe following NodeOp can now be deployed on the cluster:\napiVersion: operator.kairos.io/v1alpha1 kind: NodeOp metadata: name: trusted-boot-upgrade namespace: default spec: # NodeSelector to target specific nodes nodeSelector: matchLabels: kairos.io/managed: \"true\" # The container image containing the upgrade files image: \"planImage\" # Custom command to execute the trusted boot upgrade command: - sh - -c - | set -e rm -rf /host/usr/local/trusted-boot mkdir -p /host/usr/local/trusted-boot mount --rbind /trusted-boot /host/usr/local/trusted-boot chroot /host kairos-agent --debug upgrade --source dir:/usr/local/trusted-boot # Path where the node's root filesystem will be mounted hostMountPath: /host # Whether to cordon the node before running the operation cordon: true # Drain options for pod eviction drainOptions: enabled: true force: false gracePeriodSeconds: 30 ignoreDaemonSets: true deleteEmptyDirData: false timeoutSeconds: 300 # Whether to reboot the node after successful operation rebootOnSuccess: true # Maximum number of nodes that can run the operation simultaneously concurrency: 1 # Whether to stop creating new jobs when a job fails stopOnFailure: true Note To understand more on how this works, see the Kairos operator documentation for general information about the operator and the regular upgrade process for non-trusted boot upgrades. ","categories":"","description":"Learn how to upgrade a Kairos node with Trusted Boot enabled","excerpt":"Learn how to upgrade a Kairos node with Trusted Boot enabled","ref":"/docs/upgrade/trustedboot/","tags":"","title":"Trusted Boot Upgrades"},{"body":"","categories":"","description":"Access essential reference materials for configuring, building, and troubleshooting Kairos nodes and images.\n","excerpt":"Access essential reference materials for configuring, building, and …","ref":"/docs/reference/","tags":"","title":"Reference"},{"body":"Kairos upgrades are atomic in the sense that the new version of the OS fully replaces the old one or it doesn’t replace it at all. This allows users to test the upgrade in the lab, before they upgrade any of their nodes and be sure that the upgrade will work the same way in production.\nWhile this is very useful strategy, failed upgrades cannot be completely avoided. Some difference in hardware, a network hick-up or even a human error, can result in a failed upgrade. For systems in remote locations, upgrading to a non-bootable OS is one of the worst scenarios.\nFor this reason, Kairos is taking various measures to minimize the possibility of a non-bootable system. This section describes those measure and how they work.\nAutomatic reboot in case of a kernel panic Simple installations: By default, Kairos adds panic=5 to the kernel cmdline. This instructs the kernel to reboot after 5 seconds if a panic occurs.\n“Trusted boot” installations: The same option are included in the default cmdline, when an image is built with aurorabootk(trusted boot docs).\nAutomatic reboot in case of systemd crash Simple installations: By default, Kairos adds rd.shell=0 systemd.crash_reboot=yes to the kernel cmdline. This makes systemd restart in case it crashes (Read more)\n“Trusted boot” installations: The same options are included in the default cmdline, when an image is built with auroraboot (trusted boot docs).\nBooting to fallback Simple installations: In the sections above, we described how Kairos configures the system so that it automatically reboots when a failure occurs. Additionally, Kairos uses a combination of grub environment variables and sentinel files to detect that the failed boot, occurred after an upgrade. In that case, it sets the “fallback” boot entry as the default one. In other words, if the system fails to boot after an upgrade, the system will reboot automatically to the previous version of Kairos (the one before the upgrade).\n“Trusted boot” installations: While a similar solution exists in systemd to automatically reboot to a fallback entry, it’s not yet implemented in Kairos. You can monitor the tracking issue for updates.\nValidating the image signatures (Trusted boot installations) When Kairos is installed in trusted boot mode, the OS image comes as a single signed file. The certificate signing the image has to be enrolled in the system’s firmware database otherwise the system won’t allow booting it. This is also true when Kairos is being upgraded to a new version (which is a new image). For this reason, when upgrading, the kairos-agent will perform a check to see if the certificate that signs the new image is enrolled (and not blacklisted) in the firmware database. If not, the upgrade will be aborted to avoid a situation where booting is not possible.\n","categories":"","description":"Learn how Kairos handles failed upgrades","excerpt":"Learn how Kairos handles failed upgrades","ref":"/docs/upgrade/boot_assessment/","tags":"","title":"Boot assessment"},{"body":"The Kairos recovery mode can be used to recover a damaged system or to regain access remotely (with assistance) to a machine which has been lost access to. The recovery mode is accessible only from the GRUB menu, from both the LiveCD, and an installed system.\nNote On installed system, there are two recovery modes available during boot. Below describes only how the Kairos remote recovery works. It can be used to reset the A/B partitions (with the user/pass used during setup) and perform any other operation without remote access. Boot into recovery mode Kairos recovery mode can be accessed either via ISO or from an installed system.\nA GRUB menu will be displayed: Select the last entry kairos (remote recovery mode) and press enter.\nAt this point the boot process starts, and you should be welcomed by the Kairos screen:\nAfter few seconds, the recovery process starts, and right after a QR code will be printed out of the screen along with a password which can be used later to SSH into the machine:\nAt this stage, take a screenshot or a photo and save the image with the QR code.\nConnect to the machine In the another machine that you are using to connect to your server, (your workstation, a jumpbox, or other) use the Kairos CLI to connect over the remote machine:\n$ ./kairosctl bridge --qr-code-image /path/to/image.png INFO Connecting to service kAIsuqiwKR INFO SSH access password is yTXlkak INFO SSH server reachable at 127.0.0.1:2200 INFO To connect, keep this terminal open and run in another terminal 'ssh 127.0.0.1 -p 2200' the password is yTXlkak INFO Note: the connection might not be available instantly and first attempts will likely fail. INFO Few attempts might be required before establishing a tunnel to the host. INFO Starting EdgeVPN network INFO Node ID: 12D3KooWSTRBCTNGZ61wzK5tgYvFi8rQVxkXJCDUYngBWGDSyoBK INFO Node Addresses: [/ip4/192.168.1.233/tcp/36071 /ip4/127.0.0.1/tcp/36071 /ip6/::1/tcp/37661] INFO Bootstrapping DHT At this point, the bridge should start, and you should be able to see connection messages in the terminal. You can connect to the remote machine by using ssh and pointing it locally at 127.0.0.1:2200. The username is not relevant, the password is print from the CLI.\nThe bridge operates in the foreground, so you have to shut it down by using CTRL-C.\n","categories":"","description":"","excerpt":"The Kairos recovery mode can be used to recover a damaged system or to …","ref":"/docs/reference/recovery_mode/","tags":"","title":"Recovery mode"},{"body":"Kairos supports takeover installations. Here are a few summarized steps:\nFrom the dedicated control panel (OVH, Hetzner, etc.), boot in rescue mode Install docker and run for example: export DEVICE=/dev/sda export IMAGE=quay.io/kairos/@flavor:@flavorRelease-core-amd64-generic-master cat \u003c\u003c'EOF' \u003e config.yaml #cloud-config users: - name: \"kairos\" passwd: \"kairos\" groups: - admin ssh_authorized_keys: - github:mudler EOF export CONFIG_FILE=config.yaml docker run --privileged -v $PWD:/data -v /dev:/dev -ti $IMAGE kairos-agent manual-install --device $DEVICE --source dir:/ /data/$CONFIG_FILE Note --source flag refers to the source of installation. If you want to install from the pulled docker image you can set the --source flag to dir:/ to use the root dir of the image as install source. Otherwise you can point it to an oci image with the oci: prefix. Switch back to booting from HD and reboot. ","categories":"","description":"Learn how to enable Trusted Boot support in Kairos, which combines FDE, Secure Boot, and Measured Boot to protect your system from tampering and cold attacks.","excerpt":"Learn how to enable Trusted Boot support in Kairos, which combines …","ref":"/docs/installation/takeover/","tags":"","title":"Takeover"},{"body":"","categories":"","description":"Want to help develop Kairos? This section will teach you about our development process so you can become a contributor\n","excerpt":"Want to help develop Kairos? This section will teach you about our …","ref":"/docs/development/","tags":"","title":"Development"},{"body":" Network This feature is experimental and has only been tested on local setups. Run in production servers at your own risk. Feedback and bug reports are welcome, as we are improving the p2p aspects of Kairos. Kairos has two Kubernetes Native extensions ( entangle and entangle-proxy ) that allows to interconnect services between different clusters via P2P with a shared secret.\nThe clusters won’t need to do any specific setting in order to establish a connection, as it uses libp2p to establish a connection between the nodes.\nEntangle can be used to connect services running on different clusters or can be used with entangle-proxy to control another cluster remotely via P2P.\nPrerequisites To entangle two or more clusters you need one or more Kubernetes cluster; entangle depends on cert-manager:\nkubectl apply -f https://github.com/jetstack/cert-manager/releases/latest/download/cert-manager.yaml kubectl wait --for=condition=Available deployment --timeout=2m -n cert-manager --all entangle needs to run on all the clusters that you wish to interconnect. It provides capabilities to interconnect services between clusters entangle-proxy only on the cluster that you wish to use as control cluster Install the CRD and entangle First, add the kairos helm repository:\nhelm repo add kairos https://kairos-io.github.io/helm-charts helm repo update Install the CRDs with:\nhelm install kairos-crd kairos/kairos-crds Install entangle:\nhelm install kairos-entangle kairos/entangle ## To use a different image: ## helm install kairos-entangle kairos/entangle --set image.serviceTag=v0.18.0 --set image.tag=latest Install entangle-proxy Now install entangle-proxy only on the cluster which is used to control, and which dispatches manifests to downstream clusters.\nhelm install kairos-entangle-proxy kairos/entangle-proxy Controlling a remote cluster To control a remote cluster, you need a cluster where to issue and apply manifest from (the control cluster, where entangle-proxy is installed) and a cluster running entangle which proxies kubectl with a ServiceAccount/Role associated with it.\nThey both need to agree on a secret, which is the network_token to be able to communicate, otherwise it won’t work. There is no other configuration needed in order for the two cluster to talk to each other.\nGenerating a network token Generating a network token is described in the p2p section\nManaged cluster The cluster which is the target of our manifests, as specified needs to run a deployment which entangles kubectl:\napiVersion: v1 kind: Secret metadata: name: mysecret namespace: default type: Opaque stringData: network_token: YOUR_NETWORK_TOKEN_GOES_HERE --- apiVersion: v1 kind: ServiceAccount metadata: name: entangle namespace: default --- apiVersion: rbac.authorization.k8s.io/v1 kind: ClusterRole metadata: name: entangle rules: - apiGroups: - \"\" resources: - pods verbs: - create - delete - get - list - update - watch - apiGroups: - \"\" resources: - events verbs: - create --- apiVersion: v1 kind: List items: - apiVersion: rbac.authorization.k8s.io/v1 kind: ClusterRoleBinding metadata: name: entangle subjects: - kind: ServiceAccount name: entangle namespace: default roleRef: kind: ClusterRole name: entangle apiGroup: rbac.authorization.k8s.io --- apiVersion: apps/v1 kind: Deployment metadata: labels: app: agent-proxy name: agent-proxy namespace: default spec: selector: matchLabels: app: agent-proxy replicas: 1 template: metadata: labels: app: agent-proxy entanglement.kairos.io/name: \"mysecret\" entanglement.kairos.io/service: \"foo\" entanglement.kairos.io/target_port: \"8001\" entanglement.kairos.io/direction: \"entangle\" spec: serviceAccountName: entangle containers: - name: proxy image: \"quay.io/kairos/kubectl\" imagePullPolicy: Always command: [\"/usr/bin/kubectl\"] args: - \"proxy\" Note: replace YOUR_NETWORK_TOKEN_GOES_HERE with the token generated with the kairos-cli.\nControl To control, from the cluster that has entangle-proxy installed we can apply:\napiVersion: v1 kind: Secret metadata: name: mysecret namespace: default type: Opaque stringData: network_token: YOUR_NETWORK_TOKEN_GOES_HERE --- apiVersion: entangle-proxy.kairos.io/v1alpha1 kind: Manifests metadata: name: hello namespace: default labels: entanglement.kairos.io/name: \"mysecret\" entanglement.kairos.io/service: \"foo\" entanglement.kairos.io/target_port: \"9090\" spec: serviceUUID: \"foo\" secretRef: \"mysecret\" manifests: - | apiVersion: v1 kind: Pod metadata: name: test namespace: default spec: containers: - name: hello image: busybox:1.28 command: ['sh', '-c', 'echo \"Hello, ssaa!\" \u0026\u0026 sleep 3600'] restartPolicy: OnFailure Note: replace YOUR_NETWORK_TOKEN_GOES_HERE with the token generated with the kairos-cli and used in the step above.\nExpose services The entangle CRD can be used to interconnect services of clusters, or create tunnels to cluster services.\nCan inject a sidecar container to access a remote services exposed Can create a deployment which exposes a remote service from another cluster Deployment entangle can be used to tunnel a connection or a service available from one cluster to another.\nIn the image above, we can see how entangle can create a tunnel for a service running on Cluster A and mirror it to to Cluster B.\nIt can also expose services that are reachable from the host Network: Consider the following example that tunnels a cluster 192.168.1.1:80 to another one using an Entanglement:\nCluster A (where 192.168.1.1:80 is accessible) Cluster B (which will have a ClusterIP available on the Kubernetes service network) apiVersion: v1 kind: Secret metadata: name: mysecret namespace: default type: Opaque stringData: network_token: _YOUR_SECRET_GOES_HERE_ --- apiVersion: entangle.kairos.io/v1alpha1 kind: Entanglement metadata: name: test2 namespace: default spec: serviceUUID: \"foo2\" secretRef: \"mysecret\" host: \"192.168.1.1\" port: \"80\" hostNetwork: true --- apiVersion: v1 kind: Secret metadata: name: mysecret namespace: default type: Opaque stringData: network_token: _YOUR_SECRET_GOES_HERE_ --- apiVersion: entangle.kairos.io/v1alpha1 kind: Entanglement metadata: name: test3 namespace: default spec: serviceUUID: \"foo2\" secretRef: \"mysecret\" host: \"127.0.0.1\" port: \"8080\" inbound: true serviceSpec: ports: - port: 8080 protocol: TCP type: ClusterIP Sidecar injection The controller can inject a container which exposes a connection (in both directions):\napiVersion: v1 kind: Secret metadata: name: mysecret namespace: default type: Opaque stringData: network_token: _YOUR_SECRET_GOES_HERE_ --- apiVersion: v1 kind: Pod metadata: name: hello namespace: default labels: # Here we use the labels to refer to the service on the network, and the secret which contains our network_token entanglement.kairos.io/name: \"mysecret\" entanglement.kairos.io/service: \"foo\" entanglement.kairos.io/target_port: \"9090\" spec: containers: - name: hello image: busybox:1.28 command: ['sh', '-c', 'echo \"Hello, Kubernetes!\" \u0026\u0026 sleep 3600'] restartPolicy: OnFailure Or we can combine them together:\nCluster A Cluster B apiVersion: v1 kind: Secret metadata: name: mysecret namespace: default type: Opaque stringData: network_token: _YOUR_SECRET_GOES_HERE_ --- apiVersion: apps/v1 kind: Deployment metadata: labels: app: entangle-proxy name: entangle-proxy namespace: default spec: selector: matchLabels: app: entangle-proxy replicas: 1 template: metadata: labels: app: entangle-proxy entanglement.kairos.io/name: \"mysecret\" entanglement.kairos.io/service: \"foo\" entanglement.kairos.io/target_port: \"8001\" entanglement.kairos.io/direction: \"entangle\" name: entangle-proxy spec: containers: - name: proxy image: \"quay.io/mudler/k8s-resource-scheduler:latest\" imagePullPolicy: Always command: [\"/usr/bin/kubectl\"] args: - \"proxy\" apiVersion: v1 kind: Secret metadata: name: mysecret namespace: default type: Opaque stringData: network_token: _YOUR_SECRET_GOES_HERE_ --- apiVersion: entangle.kairos.io/v1alpha1 kind: Entanglement metadata: name: test namespace: default spec: serviceUUID: \"foo\" secretRef: \"mysecret\" host: \"127.0.0.1\" port: \"8080\" inbound: true serviceSpec: ports: - port: 8080 protocol: TCP type: ClusterIP ","categories":"","description":"Inter-connecting Kubernetes clusters without the need of exposing any service to the public via E2E P2P encrypted networks.","excerpt":"Inter-connecting Kubernetes clusters without the need of exposing any …","ref":"/docs/reference/entangle/","tags":"","title":"Entangle CRDs"},{"body":"What is the difference between Kairos compared to Talos/Sidero Metal and Flatcar? Kairos is distro-agnostic by design. Currently, you can pick among a list from the supported matrix, but we are working on CRDs to let assemble OSes from other bases in a Kubernetes native way.\nThe key difference, is that the OS is distributed as a standard container, similar to how apps are distributed with container registries. You can also use docker run locally and inspect the OS, and similarly, push customizations by pointing nodes to a new image.\nAlso, Kairos is easy to setup. The P2P capabilities allow nodes to self-coordinate, simplifying the setting up of a multi-node cluster.\nWhat would be the difference between Kairos and Fedora Coreos? Kairos is distribution agnostic. It supports all the distributions in the supported matrix. In addition, we plan to have K3s automatically deploy Kubernetes (even by self-coordinating nodes).\nAdditionally, Kairos is OCI-based, and the system is based from a container image. This makes it possible to also run it locally with docker run to inspect it, as well to customize and upgrade your nodes by just pointing at it. Think of it like containers apps, but bootable.\nIf the OS is a container, what is running the container runtime beneath? There is no real container runtime. The container is used to construct an image internally, that is then used to boot the system in an A/B fashion, so there is no overhead at all. The system being booted is actually a snapshot of the container.\nDoes this let the OS “containers” install extra kernel extensions/drivers? Every container/OS ships its own kernels and drivers within a single image, so you can customize that down the road quite easily. Since every release is a standard container, you can customize it just by writing your own Dockerfile and point your nodes at it. You can also use the CRDs, that allow you to do that natively inside Kubernetes to automate the process even further.\nKairos also supports live overlaying, but that doesn’t apply to kernel modules. However, that is somewhat discouraged, as it introduces snowflakes in your clusters unless you have a management cluster.\nHow is the P2P mesh formed? Is there an external service for discovery? The P2P mesh is optional and internally uses libp2p. You can use your own discovery bootstrap server or use the default already baked in the library. Furthermore you can limit and scope that only to local networks. For machines behinds a NAT, nodes operate automatically as relay servers (hops) when they are detected to be capable of it. You can limit that to specific nodes, or let automatic discovery handle it.\n","categories":"","description":"","excerpt":"What is the difference between Kairos compared to Talos/Sidero Metal …","ref":"/docs/reference/faq/","tags":"","title":"FAQ"},{"body":" Warning This page describes features that are still experimental in Kairos. There are a lot of things that can be improved and might be more streamlined in the future. Confidential computing is a type of secure computing that allows users to encrypt and decrypt data on a secure, isolated computing environment. It works by encrypting the data before it is sent to the cloud or other computing resources. This allows users to keep their data private and secure, even if it is accessed by unauthorized parties. This makes it useful for sensitive data such as financial information, health records, and other confidential data.\nOne important aspect of Confidential Computing is the ability to encrypt data even in-memory. This document describes how to setup Kairos to use enclave-cc in order to run confidential workloads.\nCreate a Kairos cluster The coco community bundle is supported since Kairos version v2.0.0-alpha3 (“coco” stands for “Confidential Computing”).\nA configuration file like the following should be used (see the bundles section):\n#cloud-config bundles: - targets: - run://quay.io/kairos/community-bundles:cert-manager_latest - run://quay.io/kairos/community-bundles:kairos_latest - run://quay.io/kairos/community-bundles:coco_latest install: auto: true device: auto reboot: true k3s: enabled: true users: - name: kairos passwd: kairos groups: - admin The bundle is making some changes on the host’s filesystem (installs a customized containerd binary among other things) and a restart of the node is needed in order for the changes to be applied fully. When this file appears, reboot the node: /etc/containerd/.sentinel.\nAdditional steps Label our node: kubectl label --overwrite node $(kubectl get nodes -o jsonpath='{.items[].metadata.name}') node-role.kubernetes.io/worker=\"\" Deploy the operator kubectl apply -k github.com/confidential-containers/operator/config/release?ref=v0.4.0 [Deploy the ccruntime resource] kubectl apply -k github.com/confidential-containers/operator/config/samples/ccruntime/ssh-demo?ref=v0.4.0 (wait until they are all running: kubectl get pods -n confidential-containers-system --watch)\nDeploy a workload\nThe last part with the verification will only work from within a Pod because the IP address is internal:\nssh -i ccv0-ssh root@$(kubectl get service ccv0-ssh -o jsonpath=\"{.spec.clusterIP}\")\nYou can create a Pod like this:\napiVersion: v1 kind: Pod metadata: name: kubectl spec: containers: - name: kubectl image: opensuse/leap command: [\"/bin/sh\", \"-ec\", \"trap : TERM INT; sleep infinity \u0026 wait\"] Get a shell to it and run the verification commands (You will need to install ssh in the Pod first).\nKnown limitations The above solution has some known limitations that might be addressed in future releases of Kairos. Namely:\nAfter a Kairos upgrade, the above process has to be repeated in order to install the customized containerd and the relevant configuration. There is no simple way to upgrade to newer versions of the bundle (this is a general bundles limitation). ","categories":"","description":"","excerpt":" Warning This page describes features that are still experimental in …","ref":"/docs/advanced/coco/","tags":"","title":"Confidential computing setup"},{"body":"Articles Livin’ Kubernetes on the (Immutable) Edge with Kairos Project on The New Stack Slides Kairos and libp2p Videos Meet Kairos, an OSS project building the immutable Kubernetes edge How we build and maintain Kairos CNCF TAG-Runtime Meeting 09-14-2023 (Kairos) Starts at 9:23:\nConferences ","categories":"","description":"Discover Kairos in action through videos, presentation slides, and other media resources.","excerpt":"Discover Kairos in action through videos, presentation slides, and …","ref":"/docs/media/","tags":"","title":"Media"},{"body":"This document describes how an administrator can prevent certain OS images from booting on their hardware in the context of “Trusted Boot”.\nTwo different scenarios will be covered, with the process being only slightly different for each case.\nScenario 1 - Signing certificate is no longer trusted The process of creating signed images that can be trusted to boot, requires the signing keys to be safe and only accessible to the vendor that produces the OS images.\nIn the unfortunate case in which a signing certificate has been compromised, it’s important that the key is blacklisted on every machine. Failing to do so, will make it possible for the malicious actor with access to the key, to generate OS images that the system will accept to boot. This will allow them boot and decrypt the filesystem, thus gaining full access to the data on that machine and full control over it.\nThe UEFI firmware has a special “database” in which blacklisted certificates can be “enrolled” (image hashes can also be enrolled, but more on that on the next scenario).\nIf only one machine is running the signed OS images, and physical access to that machine is possible, then usually the simplest way is to boot into the machine’s UEFI management utility and enroll the certificate in the dbx database manually.\nIf this process has to be performed on thousands of machines remotely, then it’s clear that another approach is required.\nBefore going over the steps on how to “blacklist” a certificate by enrolling it in dbx, here is some useful information which will make the steps more clear.\nFrom “pem” to “auth” The signing certificates are usually stored in one of the well known certificate formats, e.g. “pem”. If a certificate is stored in a different format, it’s usually possible to convert to “pem” with some utility. For this reason we will assume the certificate is in the “pem” format from now on.\nThere are various utilities in Linux that allow enrolling certificates in the UEFI databases but they require those certs to be in a specific format usually in the ESL (EFI Signature List) or the signed version of the same format, the “signed variables” format. Using the signed version has the benefit that no private keys need to be present in order to do the enrollement, while using the unsigned format needs the private keys to be present.\nIf you followed the instructions to create signing keys, you should have a directory with a db.pem certificate. This is the certificate we will blacklist. To do so, we need to convert it to the “esl” format and then sign it to create the final .auth file, which will be used for enrollement. The utilities used are usually shipped in the various distros under a name like efitools (e.g. in Ubuntu). Here are the commands to generate the needed signed authenticated variables file:\nexport UUID=`uuidgen` cert-to-efi-sig-list -g \"Kairos-$UUID\" keys/db.pem db-dbx.esl sign-efi-sig-list -c keys/KEK.pem -k keys/KEK.key dbx db-dbx.esl db-dbx.auth (Notice how we sign the .auth file using the “KEK” key so that enrollement is allowed)\nPCRs and disk encryption As described in the “Trusted Boot” documentation page, the decryption of the disk partitions is bound to some PCR registers on the TPM chip. Specifically registers 11 and 7. There are 2 ways to bind to a PCR register, the direct and the indirect. You can read more in the systemd-cryptenroll docs but in a nutshell, the indirect binding allows the actual value of the PCR to change while the direct one does not. For this reason, in Kairos, the decryption is bound indirectly to PCR 11 (which allows us to upgrade to newer kernels) and directly to PCR 7, which prevents booting if the UEFI databases have been altered, e.g. by enrolling a new key.\nAnd although this makes sense, security wise, it’s exactly what we are trying to achieve here. We want to enroll a certificate in dbx. This would change the value of PCR 7 thus decryption of the disk partitions will no longer be possible after reboot.\nThe method we use to overcome this issue is this:\nWe “unbind” decryption from PCR 7, binding only to PCR 11 indirectly. We enroll the blacklisted cert in dbx. We reboot to the upgraded system. We bind decryption again both to PCR 11 and 7 (as before). When we rebind to PCR 7, the register has the new value which includes the cert in dbx.\nSteps With the above now clarified, here are the steps to revoke the db.pem certificate from user space in Kairos.\nGenerate a new db cert Since it’s only the db key that we are trying to replace, we can start by copying the original directory generated by auroraboot genkey and remove the old db files.\ncp keys new-keys rm new-keys/db.* Now create the new db files:\nuuidgen --random \u003e GUID.txt openssl req -newkey rsa:4096 -nodes -keyout db.key -new -x509 -sha256 -days 3650 -subj \"/CN=NewKairosDB/\" -out db.crt openssl x509 -outform DER -in db.crt -out db.cer cert-to-efi-sig-list -g \"$(\u003c GUID.txt)\" db.crt db.esl sign-efi-sig-list -g \"$(\u003c GUID.txt)\" -k KEK.key -c KEK.pem db db.esl db.auth If you are planning to use auroraboot build-uki command to prepare the new OS image, create the following formats too or the command might complain if they are missing:\n$ openssl x509 -outform der -in db.crt -out db.der $ openssl x509 -inform DER -outform PEM -in db.der -out db.pem Sign the new image with the new certificate The new-keys directory created above can be used to prepare the new image with auroraboot. E.g.\ndocker run --rm -v $PWD/unpacked:/unpacked \\ -v $PWD/build:/result \\ -v $PWD/new-keys/:/keys \\ quay.io/kairos/auroraboot:v0.13.0 \\ build-uki \\ --output-dir /result \\ --keys /keys \\ --output-type container \\ --boot-branding \"KairosNewUKI\" \\ dir:/unpacked Enroll the new certificate in db From withing Kairos (still booted in the old image), enroll the new db key:\nsudo chattr -i /sys/firmware/efi/efivars/{PK,KEK,db}* efi-updatevar -f db.auth db Warning You will need the efitools installed on the Kairos image for this commands to work! Upgrade to the new image kairos-agent upgrade --source oci:myimage Unbind PCR 7 Bind partition encryption only to PCR 11 policy. From within Kairos again:\necho \"Generating temporary passphrase\" dd if=/dev/random bs=32 count=1 of=/tmp/random_keyfile echo \"Adding password slot\" # We temporarily switch to a passphrase decryption here, so that we can remove the \"tpm\" decryption # option below. After we add the \"tpm\" option back (bound to different PCR registers), # we remove the passphrase again. cryptsetup luksAddKey --token-type systemd-tpm2 /dev/vda2 /tmp/random_keyfile cryptsetup luksAddKey --token-type systemd-tpm2 /dev/vda3 /tmp/random_keyfile echo \"Removing the tpm2 slot\" systemd-cryptenroll /dev/vda2 --wipe-slot=tpm2 systemd-cryptenroll /dev/vda3 --wipe-slot=tpm2 echo \"Adding tpm2 slot again (pcr 11 policy only, no pcr 7)\" systemd-cryptenroll --unlock-key-file=/tmp/random_keyfile --tpm2-public-key=/run/systemd/tpm2-pcr-public-key.pem --tpm2-public-key-pcrs=11 --tpm2-pcrs= --tpm2-signature=/run/systemd/tpm2-pcr-signature.json --tpm2-device-key=/run/systemd/tpm2-srk-public-key.tpm2b_public /dev/vda2 systemd-cryptenroll --unlock-key-file=/tmp/random_keyfile --tpm2-public-key=/run/systemd/tpm2-pcr-public-key.pem --tpm2-public-key-pcrs=11 --tpm2-pcrs= --tpm2-signature=/run/systemd/tpm2-pcr-signature.json --tpm2-device-key=/run/systemd/tpm2-srk-public-key.tpm2b_public /dev/vda3 echo \"Removing the password slot\" systemd-cryptenroll --wipe-slot=password /dev/vda2 systemd-cryptenroll --wipe-slot=password /dev/vda3 Make sure you use the correct device paths for your encrypted partitions (/dev/vda2 and /dev/vda3 in the example).\nEnroll the old certificate in dbx As describe earlier in this document, you should generate a db-dbx.auth file from the old db key. This should now be enrolled in dbx. In user space again (booted in the old Kairos image):\nsudo efi-updatevar -f db-dbx.auth dbx Reboot to the upgraded image: reboot Bind again to PCR 11 and 7 After rebooting to the upgraded image, bind the encryption again to PCR 7 (and 11):\necho \"Generating temporary passphrase\" dd if=/dev/random bs=32 count=1 of=/tmp/random_keyfile echo \"Adding password slot\" cryptsetup luksAddKey --token-type systemd-tpm2 /dev/vda2 /tmp/random_keyfile cryptsetup luksAddKey --token-type systemd-tpm2 /dev/vda3 /tmp/random_keyfile echo \"Removing the tpm2 slot\" systemd-cryptenroll /dev/vda2 --wipe-slot=tpm2 systemd-cryptenroll /dev/vda3 --wipe-slot=tpm2 echo \"Adding tpm2 slot again (pcr 11 policy AND pcr 7)\" echo \"There is probably a bug in systemd preventing us from using the --tpm2-device-key\" echo \"giving this error: 'Must provide all PCR values when using TPM2 device key.'\" echo \"coming from here: https://github.com/systemd/systemd/blob/7c6028bbcbd03f91e1c4b84dcf46b45e9672c2b6/src/cryptenroll/cryptenroll-tpm2.c#L362\" echo \"so we'll opt for --tpm-device=auto\" echo \"The command that fails was originally tried in this script:\" echo \"https://github.com/kairos-io/kairos/issues/2429#issuecomment-2136728261\" systemd-cryptenroll --unlock-key-file=/tmp/random_keyfile --tpm2-public-key=/run/systemd/tpm2-pcr-public-key.pem --tpm2-public-key-pcrs=11 --tpm2-pcrs=7 --tpm2-signature=/run/systemd/tpm2-pcr-signature.json --tpm2-device=auto /dev/vda2 systemd-cryptenroll --unlock-key-file=/tmp/random_keyfile --tpm2-public-key=/run/systemd/tpm2-pcr-public-key.pem --tpm2-public-key-pcrs=11 --tpm2-pcrs=7 --tpm2-signature=/run/systemd/tpm2-pcr-signature.json --tpm2-device=auto /dev/vda3 echo \"Removing the password slot\" systemd-cryptenroll --wipe-slot=password /dev/vda2 systemd-cryptenroll --wipe-slot=password /dev/vda3 The following command:\ncryptsetup luksDump /dev/vda2 | grep \"pcrs: \" should show output similar to this:\ntpm2-hash-pcrs: 7 tpm2-pubkey-pcrs: 11 which means the decryption is now bound again to PCR 7 (directly) and PCR 11 (indirectly).\nThis command:\nsystemd-cryptenroll /dev/vda2 should only show tmp2 (and not password)\n(same thing for /dev/vda3)\nYou can now reboot to check if everything works correctly\nreboot Verifying that it worked Check what keys are enrolled in UEFI db: mokutil --list-enrolled --db | grep -E 'Issuer:|Subject:' You should find your new certificate in the list\nCheck if the old certificate is in dbx: mokutil --list-enrolled --dbx | grep -E 'Issuer:|Subject:' You should find your old certificate in the list.\nCheck that the old images are not bootable Just reboot and when presented with the boot menu, select “fallback” or “recovery”. Since those images haven’t been upgraded yet, they are still signed with the old certificate. They should not boot and UEFI should show an error.\nUpgrading recovery After successfully booting to “active”, you can now upgrade recovery to make can still boot if the next update goes wrong.\nObviously the recovery image will also need to be signed with the new certificate.\nThe upgrade command is:\nkairos-agent upgrade --recovery --source oci:\u003cYOUR_UPGRADE_IMAGE_HERE\u003e The “fallback” image will be upgraded on the next upgrade. Read the “Container based” document to understand more.\nScenario 2 - One specific image is no longer trusted In the previous scenario, we assumed that the signing certificate was compromised, which meant no image signed by that certificate should be bootable anymore.\nIn this scenario, only one specific image is considered “bad”. For example, an image shipped with a well known security vulnerability. In order to prevent this image from booting, its “hash” can be blacklisted.\nThe process is exactly the same as above, but instead of enrolling a certificate in the dbx database, we will enroll an image hash.\nFollow the same instructions as in scenario 1, but skipping the steps where a new db key is created.\nTo generate the file which will be enrolled to dbx, we need access to the .efi image. If there is a system still running the image that is going to be blacklisted, the file will be: /efi/EFI/kairos/active.efi.\nSince the image is not considered safe (that’s the reason for blacklisting it after all), it’s advisable to not boot it or use it for the calculation. If it’s possible, boot from another, safe image, mount the efi partition and find the active.efi file on the disk.\nAs a last option, if you still have access to the original install ISO that was used to install the “bad” image, you can extract the .efi file from that using standard linux tools.\nThe steps to generate the .auth file are (assumes you have efitools installed):\n# generate the esl file hash-to-efi-sig-list /efi/EFI/kairos/active.efi active.esl Now scp that file to the machine where you have access to the image signing keys generated by auroraboot. The command to sign the esl file is similar to the one for the certificates:\nsign-efi-sig-list -c keys/KEK.pem -k keys/KEK.key dbx active.esl active.auth active.auth is the file that should be enrolled to dbx, exactly like we did with the db-dbx.auth file in the previous scenario. The rest of the steps are similar.\nAlways keep in mind that after enrolling something (key or hash) to dbx, blacklisted OSes won’t boot. Make sure you have successfully completed all the needed steps before you reboot, otherwise you may not be able to boot.\n","categories":"","description":"","excerpt":"This document describes how an administrator can prevent certain OS …","ref":"/docs/advanced/revoking-secureboot-access/","tags":"","title":"Revoking secure boot access"},{"body":"Welcome to the Kairos master Documentation\nKairos is the open-source project that simplifies Edge, cloud, and bare metal OS lifecycle management. With a unified Cloud Native API, Kairos is community-driven, open source, and distro agnostic.\nIn this documentation, you will find everything you need to know about Kairos, from installation and configuration, to examples and advanced features.\nFor more information, go ahead and check the sections bellow. If you have any questions or feedback, feel free to open an issue or join our community forum.\n","categories":"","description":"Explore the official documentation for Kairos and discover how it simplifies lifecycle management across edge, cloud, and bare metal environments.","excerpt":"Explore the official documentation for Kairos and discover how it …","ref":"/docs/","tags":"","title":"Documentation"},{"body":"Kairos has always been more than just a Linux distro—it’s a bold idea: bring the power and flexibility of Kubernetes to the edge, with a truly immutable, secure, and cloud-native operating system anyone can use, contribute to, and trust.\nOver the past few years, we’ve seen Kairos go from idea to implementation, and from experimentation to production. Today, it’s running in large-scale, real-world deployments across diverse edge use cases. But more importantly—it’s been growing as a community.\nToday, we’re excited to share a new milestone in that journey:\n👉 Kairos now has official commercial support options available from Spectro Cloud.\nBefore we dive into what that means, let’s take a step back and talk about what makes an open-source project reliable—especially when the stakes are high.\nWhat Makes Open Source Reliable? In our view, there are two key ingredients that help teams adopt open source with confidence:\n1. Good Governance A mature open-source project needs a clear structure, transparency, and community direction. You want to know that decisions aren’t made behind closed doors, and that the project will be around for the long term.\nWe’re proud to say that Kairos achieved this last year, when it became a CNCF Sandbox Project. This step formalized our commitment to open governance and alignment with the broader cloud-native ecosystem.\nJoining the CNCF means more than just a badge—it puts Kairos on a clear track for growth and maturity, with expectations around documentation, contributor diversity, quality, and more. We’re not building in isolation—we’re growing with a strong open foundation.\n2. Professional Support Options Even the best community can’t always meet the needs of production environments. Some organizations—especially those in regulated industries or enterprise IT—need a guaranteed path to support when something breaks.\nUntil now, support for Kairos has been best-effort, through GitHub issues, Slack discussions, and community help. That’s still available, and it works well for many teams.\nBut for those who need enterprise-grade support, we now have an answer.\nAnnouncing Commercial Support for Kairos Kairos now has official enterprise-grade support options, thanks to our support partner: Spectro Cloud. As a major contributor and original creator of the project, Spectro Cloud is in a strong position to help organizations adopt Kairos with confidence.\nTheir support offerings range from basic ticketing-based assistance to full 24×7 production support, including hotfixes, CVE patching, onboarding, and SLAs. This is ideal for teams who want to standardize on Kairos at scale, and need more than community-based help.\nThis move does not change the open-source nature of the project—Kairos remains 100% open, with no commercial forks or gated features. It simply expands the ecosystem and offers more choice to adopters.\nLearn more Why This Matters (Even If You Don’t Need Support) You might be thinking, “I use Kairos just fine without needing a support contract. Why should this matter to me?”\nHere’s why:\nIt strengthens the project. When support partners invest in Kairos, it helps sustain long-term development and community health. It builds trust. Organizations that rely on SLAs or need commercial assurances now have a clearer path to adoption. It keeps options open. Whether you’re using Kairos as a solo developer or deploying across thousands of edge devices, you now have the flexibility to get help when you need it. Open source isn’t just about free (as in beer) software. It’s about freedom—of choice, of contribution, and of collaboration.\nWhat’s Next? This milestone is just one step in a broader journey.\nWe want to continue maturing Kairos and reach CNCF Incubation—a critical next phase in the project’s evolution. To do that, we need to keep growing the community, broadening adoption, and building deeper collaboration.\nSo we’re putting out a call:\n🔔 If you’re a company invested in Kairos, there are two key ways you can help:\nContribute Maintainers\nIf you’re already using Kairos in your products or infrastructure, chances are you’re making improvements, tooling, or fixes internally. Why not share that value with the broader community? We invite you to become a maintainer, help guide the roadmap, and collaborate with us to shape the future of edge Linux together.\nBecome a Support Partner\nIf you’re recommending Kairos to your customers and helping them succeed with it, we’d love to recognize your role. Let’s work together to include your organization in our list of Kairos support partners—making it clear that help is available from more than one source, and encouraging adoption from companies that want a trusted hand.\n📩 Interested? Reach out to us at members@kairos.io to start the conversation.\nLet’s continue building Kairos as an open, shared foundation—one that scales with our collective ambition.\nTry Kairos — and Get Help When You Need It If you’re exploring Kairos or already using it in production, we’re here to support you—both as a community and through professional channels.\nKairos now has official enterprise-grade support partners, for those who need additional help beyond the community. Spectro Cloud, the original creator and a key maintainer of the project, is the first company to offer commercial support for Kairos.\nWe expect more organizations to join as support providers as adoption continues to grow.\nAs always, you can get involved with Kairos via GitHub, our Slack community, and other open channels. We welcome contributors, feedback, and discussion.\nThanks to everyone who’s helped bring Kairos this far—we’re excited to keep improving it with you.\n🧡 The Kairos Maintainers\n","categories":"","description":"","excerpt":"Kairos has always been more than just a Linux distro—it’s a bold idea: …","ref":"/blog/2025/08/08/growing-stronger-introducing-commercial-support-for-kairos/","tags":"","title":"Growing Stronger: Introducing Commercial Support for Kairos"},{"body":"","categories":"","description":"","excerpt":"","ref":"/","tags":"","title":"Kairos"},{"body":" 🚀 Kairos 3.5.0: A Milestone for the Kairos Ecosystem We’re excited to announce the release of Kairos 3.5.0! This release represents a significant milestone for the broader Kairos ecosystem, marking the completion of several key features across multiple projects that work together to improve the overall Kairos experience.\n⚙️ Meet the Kairos Operator The biggest addition to the Kairos ecosystem is the Kairos operator - a standalone Kubernetes operator that provides a native way to manage upgrades and operations on Kairos nodes. While this operator can work with previous Kairos versions, 3.5.0 marks the point where we consider it mature enough to be the recommended approach for Kubernetes-based management.\nThe operator provides two custom resources:\nNodeOp: For generic operations on Kubernetes nodes (Kairos or not). It allows mounting the host’s root filesystem to perform operations or run scripts.\nNodeOpUpgrade: A Kairos-specific custom resource for upgrading Kairos nodes. It automatically creates a NodeOp with the appropriate upgrade script and configuration.\nThis means you can now manage your Kairos nodes using familiar Kubernetes patterns. The operator automatically detects Kairos nodes and labels them with kairos.io/managed: true, making it easy to target Kairos nodes specifically in hybrid clusters.\nTo get started with the operator, simply deploy it to your cluster:\nkubectl apply -k https://github.com/kairos-io/kairos-operator/config/default Then trigger an upgrade by creating a NodeOpUpgrade resource:\napiVersion: operator.kairos.io/v1alpha1 kind: NodeOpUpgrade metadata: name: kairos-upgrade namespace: default spec: image: quay.io/kairos/ubuntu:22.04-standard-amd64-generic-v3.5.0-k3s-v1.33.2-k3s1 nodeSelector: matchLabels: kairos.io/managed: \"true\" concurrency: 1 stopOnFailure: true For more details on using the operator, check out our Kairos Operator documentation.\nThe Kairos operator opens up new possibilities for node management. You can now:\nPerform custom operations on nodes using NodeOp resources Control upgrade concurrency and failure handling Apply configuration changes after installation Reset nodes with custom configurations Handle trusted boot upgrades through Kubernetes The operator provides fine-grained control over deployment strategies, allowing you to implement canary deployments, rolling updates, or bulk operations depending on your needs.\n🔧 Enhanced AuroraBoot Experience Another significant milestone is the completion of the API-based workflow in AuroraBoot, our tool for building bootable images. AuroraBoot provides a comprehensive solution for creating custom bootable images, with both a user-friendly WebUI and powerful automation capabilities.\nIntroducing AuroraBoot WebUI AuroraBoot includes a web-based interface that makes image building accessible to everyone. The WebUI provides an intuitive way to configure and build custom Kairos images without needing to remember complex command-line parameters. You can easily specify container images, cloud configurations, and various build options through a clean, modern interface.\nExperimental Feature The AuroraBoot WebUI is currently under heavy development and should be considered experimental. While functional, it may have limitations and could undergo significant changes in future releases. For production use, we recommend using the command-line interface. To get started with the AuroraBoot WebUI, simply run:\ndocker run --rm -v /var/run/docker.sock:/var/run/docker.sock \\ --privileged \\ -v $PWD/build/:/output \\ -p 8080:8080 \\ quay.io/kairos/auroraboot web Then visit http://localhost:8080 in your browser to access the interface. For more details on using the WebUI, check out our AuroraBoot documentation.\nAPI-Based Automation The new API-based workflow in AuroraBoot enables automation of image building operations. This means you can now integrate image building into your CI/CD pipelines and infrastructure automation workflows. The API provides programmatic access to all AuroraBoot features, allowing you to build images as part of your automated deployment processes.\nThe API workflow complements the existing WebUI, giving you both the ease of use for manual operations and the power of automation for production environments.\n📁 Configuration Path Migration Reminder While the configuration path change from /etc/elemental/config.yaml to /etc/kairos/config.yaml has been moved to v4.0.0, we want to remind users to start migrating their configurations now. This change will align our configuration paths with the Kairos branding and provide a cleaner, more consistent experience.\nIf you’re using the old path, we recommend updating your configurations to use the new location (/etc/kairos/config.yaml) to prepare for deprecation. The old path will continue to work for now, but migrating early will ensure a smooth transition.\n🔮 What’s Next The completion of these ecosystem features represents a significant step toward more integrated and automated Kairos workflows. We’re excited to see how the community uses these new capabilities to build more sophisticated deployment and management workflows.\nWe’re already working on the next milestone - Kairos v3.6.0. Check out the ticket to see what’s coming next.\nAs always, we encourage you to try out the new features and share your feedback. Whether you’re upgrading existing clusters, automating image builds, or deploying new ones, these tools should make your day-to-day operations smoother and more predictable.\nFor a complete list of changes in this release, visit the v3.5.0 release page on GitHub. For detailed upgrade instructions and examples, visit our upgrade documentation. If you run into any issues or have questions, join the conversation in our Slack or GitHub Discussions. If you want to participate in the development process, you can always join our office hours - the calendar is available on our community page.\nHappy upgrading!\n","categories":"","description":"Kairos release v3.5.0","excerpt":"Kairos release v3.5.0","ref":"/blog/2025/07/25/kairos-release-v3.5.0/","tags":"","title":"Kairos release v3.5.0"},{"body":" Kairos 3.4.0: Building Forward with Simplicity, Cloud Reach, and Runtime Flexibility Every now and then, a release isn’t just a version bump—it’s a reset button. Kairos 3.4.0 is one of those releases.\nIn the last few months, we’ve been heads-down rethinking what makes Kairos powerful, approachable, and future-proof. The result is a foundational overhaul: a brand-new build system, native images for all major cloud providers, streamlined customization through system extensions, and a tighter Kubernetes story with built-in support for k0s. This release isn’t just packed with features—it reflects where the project is heading and how our community is shaping that direction.\nLet’s walk through the biggest milestones in Kairos 3.4.0, how they work, and why they matter.\n🛠️ Rebuilding Kairos from the Ground Up with kairos-init We’ve officially said goodbye to Earthly and rebuilt the Kairos build system from scratch. The new system, centered around kairos-init, is faster, cleaner, and deeply inspired by the declarative configuration model we’ve honed through Yip.\n“We didn’t just want to make the build system better—we wanted to make it understandable and hackable.” — from our deep dive\nThis change makes it easier than ever to create your own derivative OS with Kairos. If you’re customizing images or building new flavors, the kairos-factory documentation is your new best friend. It’s built for composability, consistency, and speed.\n☁️ Kairos Goes Native on the Cloud With 3.4.0, Kairos now offers official cloud images for AWS, Azure, and GCP. That means you can deploy Kairos right from your cloud console—no manual image imports, no extra setup.\nLaunch on AWS Deploy via Azure Marketplace Spin up on GCP This makes it easier to integrate Kairos into hybrid, multi-cloud, or CI-driven workflows—and aligns with our goal to make Kairos ready-to-go for every environment.\n🚀 Kubernetes with k0s: The Meta-Distribution Vision With native support for k0s, Kairos is now a meta-distribution for Kubernetes—fully declarative, fully integrated. Whether you’re deploying edge nodes or building HA clusters, k0s + Kairos is a minimal, clean foundation.\nYou can now toggle between k3s and k0s setups in the updated docs and configure each using the same Kairos cloud-init model.\n👉 Blog deep dive\n🧩 Live Customization with System Extensions Say goodbye to image rebuilds for small tweaks. System extensions let you declaratively add or modify behavior on a running Kairos system, and 3.4.0 ships with built-in CLI support to install, remove, enable, and disable extensions.\n👉 Learn more\n👉 System Extensions docs\nThis opens the door to layered, profile-based systems that evolve post-deployment—great for both immutable infrastructure and rapidly changing environments.\n🙌 New Contributor Shout-Out Huge thanks to @jimmycathy for their first contribution to the project! 🎉\nPR: https://github.com/kairos-io/kairos/pull/3255\nWe’re always excited to welcome new contributors. Whether it’s documentation, ideas, or PRs—every bit helps make Kairos stronger for everyone.\nStay tuned as we continue evolving Kairos with your feedback and use cases in mind. Got a question or want to share how you’re using 3.4.0? Join the conversation in our Discord or GitHub Discussions.\n","categories":"","description":"Kairos release v3.4.0","excerpt":"Kairos release v3.4.0","ref":"/blog/2025/04/24/kairos-release-v3.4.0/","tags":"","title":"Kairos release v3.4.0"},{"body":"🔧 System Extensions, Simplified: Live Customization with Kairos Kairos has always focused on making operating system deployments more predictable, reproducible, and manageable—especially in edge and embedded environments. But for all the benefits of immutability, one challenge has lingered:\nHow do you safely and easily customize a system after it’s built—without breaking the image or going full Dockerfile rebuild?\nToday, we’re introducing a much cleaner answer: the new system extension management framework, now available via the Kairos Agent CLI and when Kairos 3.4.x releases.\n💡 What’s New System extensions in Kairos are not new—but managing them used to be a manual process. You had to mount things yourself, know where to drop files, and keep track of boot profiles by hand.\nThat’s all changed.\nYou can now:\n🔄 Download raw extension images over https, file, or even oci (alpha) ✅ Enable or disable extensions declaratively per boot profile ⚡ Activate extensions live with --now 🔍 List which extensions are installed or currently active 🧹 Remove extensions safely—links and all All from one CLI entrypoint:\nkairos-agent sysext 🗂️ How It Works Under the hood, the new system works using a simple but powerful layout:\nPath What it does /var/lib/kairos/extensions/ Stores downloaded disk images .../active, .../passive, etc. Symlink folders tied to boot profiles /run/extensions/ Where active sysexts live at runtime When the system boots, immucore checks the current boot profile and links the relevant extensions into /run/extensions/, which are then loaded by systemd-sysext.\nIt’s all ephemeral, safe, and declarative.\n🔐 Compatible with Trusted Boot System security remains a top priority in Kairos. If you’re using Trusted Boot, rest assured: this new extension mechanism doesn’t compromise your system’s integrity.\nOnly signed and trusted extension images are allowed to load. This means:\nThe bootloader and init system verify authenticity Unsigned or tampered images are simply ignored Your measured boot chain remains intact and auditable So yes—you can safely extend your system at runtime without sacrificing security.\n🛠️ Real-World Usage Here’s what it looks like in action:\n# Download a disk image directly kairos-agent sysext download https://example.org/extensions/mytool.img # Enable it for active boots, and load it now kairos-agent sysext enable --active --now mytool # See what’s active kairos-agent sysext list --active # Remove it completely kairos-agent sysext remove --now mytool And yes—you don’t need to type full names. All sysext commands accept regex patterns. That means mytool will match something like mytool-v1.0.sysext.raw.\n🚧 Looking Ahead This is just the beginning. We’re planning further enhancements to system extension workflows—like declarative integration in the config layer, improved OCI support, and better visibility tooling.\nWe’d love to hear how you use it—and what would make it even better. Jump into the Kairos community and let us know what you think.\nFor a more detailed breakdown, including supported commands and advanced examples, check out the System Extensions documentation.\n","categories":"","description":"Introducing the new CLI-driven system extension management in Kairos. Download, enable, and manage system extensions live without rebuilding your OS image.","excerpt":"Introducing the new CLI-driven system extension management in Kairos. …","ref":"/blog/2025/04/15/system-extensions-simplified-live-customization-with-kairos/","tags":"","title":"System Extensions, Simplified: Live Customization with Kairos"},{"body":"Up until now, Kairos has shipped with first-class support for k3s out of the box. If you wanted something else? You had to rely on community-powered providers. And let’s be honest, while amazing in their own right, they’ve been… fragmented.\nWe’ve seen providers emerge for kubeadm, nodeadm, microk8s, and even another flavor of k3s. Each one added the ability to run Kubernetes on Kairos, but they were often self-contained efforts. Most didn’t plug into our cloud-init-style configuration, which meant you had to know your way around their specific setup to get them running. Power users and contributors made them work, but new users? Not so much.\nThen there’s the provider-kairos, which offers a consistent configuration layer across setups and makes Kubernetes orchestration a breeze, especially for decentralized, peer-to-peer deployments powered by EdgeVPN. But it had one limitation: it only worked with k3s.\nThat is… until now.\nEnter k0s: The Zero Friction Kubernetes A few months back, William Rizzo, a CNCF and Linkerd Ambassador, approached us with a pull request and a mission: bring k0s to Kairos with the same first-class integration as k3s.\nFor those unfamiliar, k0s is a lightweight, CNCF-certified Kubernetes distribution that bundles everything into a single binary. No OS dependencies. No frills. Just Kubernetes, as it should be — simple to run, easy to maintain, and friendly across bare metal, cloud, edge, and IoT. It supports containerd, Kube-Router by default (with optional Calico), and can run anywhere Linux does. It’s perfect for Kairos.\nWe sat down together and made it happen. This is the magic of open-source: two CNCF Sandbox Projects, Kairos and k0s, coming together to build something bigger than either could alone.\nNow, thanks to this collaboration, you can run k0s using the same Kairos-native cloud-init configuration that previously only supported k3s. That means P2P cluster formation with EdgeVPN, full configuration via YAML, and all the DX goodness you’re used to, with a new Kubernetes engine under the hood.\nWhat This Means Kairos just leveled up.\nWe’re not just a Linux meta-distribution anymore. We’re taking our first real steps into becoming a Kubernetes meta-distribution.\nSo meta.\n“Wait… Kairos is also a Kubernetes meta-distribution now?” — “Always has been.”\nFrom now on, users don’t have to choose between clean, declarative setups and their preferred Kubernetes flavor. The Kairos provider now supports both k3s and k0s natively. Want to try it? We’ve added examples to our repository showing both side-by-side so you can pick the flavor that suits your use case best.\nAnd it doesn’t stop there. As noted in the recent CNCF blog post, combining k0s’s single-binary simplicity with Kairos’s secure, immutable images is a huge win for edge computing. Together, we’re making it easier to build secure-by-default, production-ready clusters from the ground up.\nTry it Today The k0s integration will be fully stable in Kairos v3.4, but you don’t have to wait. It’s already available in our beta or nightly images for Ubuntu, Rockylinux, and openSUSE, or you can build your own with AuroraBoot.\nTo get started:\nCheck out the Kairos provider repository Explore the updated examples with k3s and k0s Spin up a custom image And let us know what you build! ","categories":"","description":"","excerpt":"Up until now, Kairos has shipped with first-class support for k3s out …","ref":"/blog/2025/04/02/kairos-meets-k0s-a-meta-distribution-for-kubernetes-is-born/","tags":"","title":"Kairos Meets k0s: A Meta-Distribution for Kubernetes is Born"},{"body":"🧱 Introduction Building Kairos has always been about more than assembling images — it’s about shaping a flexible, powerful OS tailored for the edge. Over the past couple of years, we’ve learned a lot while navigating how to build and maintain Kairos across a growing list of base distributions, architectures, and board-specific targets.\nToday, we’re excited to introduce something that marks a turning point in how Kairos is built: kairos-init.\nThis post isn’t just about a new tool. It’s about simplifying complexity, rediscovering clarity, and embedding hard-earned lessons into something lean and extensible. If you’ve followed our journey, you’ll know that we’ve gone from Dockerfiles to Earthly, and now to a new approach centered on declarative simplicity with Yip.\nIn this post, we’ll take you behind the scenes:\nWhy the old ways worked (until they didn’t) How we outgrew our tooling What kairos-init changes — and why it matters This is the story of how we rebuilt the foundation of Kairos, one layer at a time.\n🐳 The Early Days: Building with Dockerfiles When Kairos first took shape, our needs were simple. We wanted a predictable, repeatable way to turn a base Linux distribution into a fully working Kairos image. And at the time, the most straightforward path was through a set of handcrafted Dockerfiles — one for each supported base.\nNeed Kairos on Ubuntu? There was a Dockerfile for that. OpenSUSE? Another one. Want to tweak something for a Raspberry Pi or test an Alpine variant? Copy, paste, modify.\nThis worked — until it didn’t.\nAt first, it was empowering. We could iterate quickly, test changes, and get a Kairos image up and running in no time. Each Dockerfile acted like a scriptable recipe for image creation. But as we started expanding support for more:\n🧱 Distributions (Ubuntu 24.04, 22.04, 20.04, OpenSUSE, Alpine, Rocky, Fedora…) 📦 Releases (LTS, rolling, and experimental flavors across versions) ⚙️ Architectures (x86_64, ARM64) 📦 Boards and platforms (Raspberry Pi, NVIDIA Jetson, cloud providers…) … the number of Dockerfiles exploded.\nKairos Dockerfiles in the wild (Version 2.3.0) Kairos Dockerfiles in the wild (Version 2.3.0)\nMaintaining them became a slow grind. Every distro had its quirks. Every version bump required tweaks. Repeating common steps across files introduced subtle bugs. And worst of all, it was hard to scale — we were reinventing the same process in N different places, each slightly different, each easy to break.\nWe needed a better way.\n🌍 Scaling Painfully: Enter Earthly As the number of supported distros, architectures, and boards grew, so did the pain of managing our Dockerfiles. It wasn’t just about duplication anymore — it was about orchestration, coordination, and staying sane across a forest of moving parts.\nSo we took the next step and adopted Earthly.\nEarthly gave us a powerful framework to unify and modularize our builds. Instead of one Dockerfile per case, we could define reusable steps, build pipelines, and cache logic across flavors and platforms. It brought real structure to our chaos.\nWith Earthly, we gained:\n🔁 Reusable modules for common build logic 🧩 Parameterized builds across distros and versions 🧠 Smarter caching, which cut build times drastically 🤖 Better CI integration, especially in GitHub Actions 🏗️ Cross-architecture support (building ARM images from x86 machines) It was a massive improvement. Kairos became easier to scale, easier to extend, and more robust in CI/CD pipelines. We could merge several Dockerfiles into a single one for that given Flavor and make earthly trigger the right steps based on the target architecture or version.\nLess lines is always better, right? (Version 3.3.1) But the cost of complexity caught up with us.\nEarthly was powerful, but also heavy. Setting up the environment required more tooling and context. Onboarding new contributors meant explaining layers of build abstraction. And eventually, even Earthly couldn’t fully hide the fact that we were fighting the complexity we had slowly built up.\nAs our use cases expanded — with more devices, more customization, more flavors — our build logic became a tangled web of conditionals and overrides.\nThe Earthly pipeline grows instead (Version 3.3.1) We needed a reset. A system built from the start for clarity and adaptability.\n🧨 Reaching the Breaking Point By this stage, we had all the signs of a powerful yet brittle system.\nEvery new board support request — whether for a Jetson device, RPi variant, or some obscure industrial box — meant another round of “how do we fit this into the Earthly pipeline without breaking 10 other things?” Even small changes to the build logic carried unintended ripple effects. Maintenance became a game of regression whack-a-mole.\nOur internal complexity had quietly outpaced the benefits of our tooling.\nWe were juggling multiple abstraction layers Supporting a new architecture meant rewriting or extending multiple Earthfiles Troubleshooting builds required context-switching between tools and environments Most importantly, it became hard for contributors to participate That’s when we stopped and asked: What if we could start fresh?\nThe answer was kairos-init.\n🛠️ We Tried Other Tools First Before diving into kairos-init, we explored other tools and frameworks hoping to find a better fit. But none of them quite matched the specific constraints of our ecosystem — building across multiple distros, versions, and architectures in a consistent, maintainable way.\nSo, we did what every developer dreams of: we built our own.\n✨ The Breakthrough: kairos-init and Yip kairos-init is a CLI tool that encapsulates everything needed to transform any base Linux image into a fully Kairos system — with nothing more than a standard Dockerfile.\nBy running kairos-init BUILD in a Dockerfile, it:\nInspects the base image Generates a Yip configuration Executes that config to: Install necessary packages Add the Kairos framework (binaries, configs, init hooks) Patch the initramfs Link kernel files Apply any workarounds or tweaks The result is a container image that’s Kairos-ready — no extra steps, no complex layering.\nYou can then:\nUse it as an upgrade image Extend it with Kubernetes runtimes like k3s or k0s using flags Convert it into various formats with AuroraBoot: ISO Raw image Cloud image PXE bootable EFI Unified System Image And because it’s just a Dockerfile, it drops right into any existing workflow:\nCI/CD pipelines Local testing Version-controlled infra repos 🌤 A Cleaner Future With kairos-init, we’ve:\nSimplified builds down to one step Removed the need for Earthly or complex pipelines Made Kairos accessible to new contributors and integrators Enabled future growth without growing technical debt We’re no longer managing complexity — we’ve replaced it with clarity.\n🔭 What’s Next What’s ahead:\nMore kairos-init examples and docs Deep dives on Yip Custom build guides Community use cases We’re also eager to hear how you use it — tag us, open issues, or just say hi.\n🧭 Conclusion From handcrafted Dockerfiles to Earthly-powered pipelines and now to a clean, declarative build system — this journey has been about making Kairos better not just for us, but for you.\nWith kairos-init, we’ve laid the groundwork for a more open, composable, and hackable future.\nThanks for building with us. 🚀\n","categories":"","description":"","excerpt":"🧱 Introduction Building Kairos has always been about more than …","ref":"/blog/2025/03/26/how-we-rebuilt-kairos-building-from-the-ground-up/","tags":"","title":"How We Rebuilt Kairos building From the Ground Up"},{"body":"Picture this: You’re deep in focus, coding away, and the doorbell rings. Except… you never hear it. The mailman leaves, and now your package is on an adventure you didn’t plan for. That’s exactly what happened to me, so I did what any geek would do—turn my dumb doorbell into a smart one using Kairos.\nDisclaimer The information provided in this post is for educational and informational purposes only. I am not a professional electrician, engineer, or security expert. Any modifications to electrical systems, including doorbells, carry inherent risks, such as damage to property, malfunction, or personal injury. If you choose to follow the steps outlined here, you do so at your own risk. Always take proper safety precautions, consult professionals when necessary, and ensure compliance with local laws and regulations. I cannot be held liable for any consequences resulting from the use or misuse of the information in this post. Requirements For this setup I used the following hardware\nHoneywell D117 doorbell using the D780 transformer Finder 40.52.8.006.0000 6V AC relay and 95.85.3 relay socket Doorbell cable and male to female jumper cables Raspberry Pi 4B 8G with its power adapter and an SD card and a RJ45 cable to connect it to the internet And on the software side of things all you need is docker, but you can also use other container engine if you have experience building and running images there.\nHardware My doorbell is a Honeywell D117, aka Ding Dong (gotta love the name). It’s using the D780 transformer with the following connection (diagram on the left of the picture).\nThe transformer takes 220V and produces an 8V current\nWe can now use a relay to detect when the circuit is closed. I wasn’t able to find an 8V AC relay, so I tried with a 12V AC and a 6V AC. The 12V one didn’t work but the 6V one did. I cannot tell how bad an idea this is so only try this at your own risk. The brand is FINDER and the part number is 40.52.8.006.0000, I bought it from Reichelt and got it sent to Belgium\nConnect A1 and A2 to 0F and T3 respectively.\nWe will use the always open circuit on the other side of the relay so connect a male to female cable on 11 and 14 which will go to the Raspberry Pi.\nI connected it to GPIO 23 and Ground (14)\nSoftware System Image Now that all the cabling is done, let’s prepare the software. First I’m going to prepare the image for the SD card. For that I’ll use Kairos because it will make it easier for me to do upgrades in the future and because it’s immutable so I can have my infrastructure as code easily. All I have to do is build a raw image which I can then copy to the SD card.\ndocker run --rm --privileged \\ -v /var/run/docker.sock:/var/run/docker.sock \\ -v $PWD/build/:/output \\ quay.io/kairos/auroraboot:latest \\ --debug \\ --set \"disable_http_server=true\" \\ --set \"disable_netboot=true\" \\ --set \"container_image=quay.io/kairos/opensuse:leap-15.6-standard-arm64-rpi4-v3.3.1-k3sv1.32.1-k3s1\" \\ --set \"state_dir=/output\" \\ --set \"disk.raw=true\" There’s a lot in this command, which you can learn in the Kairos documentation but the one important thing to know is that I’m consuming the Kairos image that is specially built for Raspberry Pi 4 and that it contains the K3s Kubernetes distribution in it so I can deploy my script via Kubernetes and leave the base image unchanged.\nIf everything is successful you should get the image build/kairos-opensuse-leap-15.6-standard-arm64-rpi4-v3.3.1-k3s1.32.1.raw so let’s copy it into the SD card, I’m on Linux so I can run the following\nsudo dd if=./build/kairos-opensuse-leap-15.6-standard-arm64-rpi4-v3.3.1-k3s1.32.1.raw \\ of=/dev/mmblk0 \\ oflag=sync \\ status=progress \\ bs=10MB Once it finishes copying, you can insert it on the Raspberry Pi and boot it. It will do an initial boot to setup the system and reboot on its own. In the meantime let’s prepare the software we are going to deploy.\nTelegram Bot I will be using a telegram bot to send the messages. This way anyone in the family can subscribe to it and mute it if they want. If you prefer a different solution just adapt the Python script in the next section.\nOpen Telegram and search for BotFather. Type /newbot and follow the steps to create a bot. Copy the API token that BotFather gives you. Interact with your bot by sending any message Extract the Chat ID by going to a browser using the api token you got in one of the previous steps https://api.telegram.org/botYOUR_API_TOKEN/getUpdates Python Script This script will detect when there’s a signal in GPIO 23, write a log and send a Telegram message\nimport os import lgpio as GPIO import time import requests from datetime import datetime # Read environment variables BOT_TOKEN = os.environ.get(\"TELEGRAM_BOT_TOKEN\") CHAT_ID = os.environ.get(\"TELEGRAM_CHAT_ID\") # GPIO Setup GPIO_PIN = 23 h = GPIO.gpiochip_open(0) GPIO.gpio_claim_input(h, GPIO_PIN, GPIO.SET_PULL_UP) def send_telegram_message(message): if not BOT_TOKEN or not CHAT_ID: print(\"❌ Error: Missing environment variables for Telegram bot.\") return url = f\"\u003chttps://api.telegram.org/bot{BOT_TOKEN}/sendMessage\u003e\" payload = {\"chat_id\": CHAT_ID, \"text\": message} requests.post(url, json=payload) print(\"🚀 Doorbell monitor started...\") while True: if GPIO.gpio_read(h, GPIO_PIN) == 0: # Button pressed timestamp = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\") message = f\"🚪 Doorbell pressed at {timestamp}!\" print(f\"[{timestamp}] 🔔 {message}\") send_telegram_message(message) time.sleep(2) # Avoid spamming messages time.sleep(0.1) Dockerfile We won’t run the script directly on the Raspberry Pi, it will be running in a Docker container. This is very useful when having the underlying system be immutable because I can still iterate without having to rebuild and burn a new SD card and restart the system. Instead all I have to do is install the packages I need in this container and run it in privilege mode. This of course can open some security risks, but again, this is just for learning purposes and for fun.\n# Use a lightweight Python base image for Raspberry Pi (ARM) FROM python:3.9-slim # Install system dependencies RUN apt update \u0026\u0026 apt install -y python3-dev gcc RUN pip install lgpio requests # Set the working directory inside the container WORKDIR /app # Copy the Python script into the container COPY doorbell.py /app/doorbell.py # Set the script to run on startup CMD [\"python3\", \"/app/doorbell.py\"] Since the image is going to be running on the Raspberry Pi but I’m going to build it on an AMD platform, then I need to cross-compile. Use any tag you prefer, just keep in mind it is the same tag you have to use in the deployment in the next section\ndocker buildx build --platform=linux/arm64 \\ -t quay.io/mauromorales/doorbell:telegram-v1 \\ . And publish it to a repository, I’m using Quay but you can use any repo you prefer\ndocker push quay.io/mauromorales/doorbell:telegram-v1 Yes that image is public and will remain static, but I’d recommend you build your own instead of using mine because at any moment I might decide to remove it.\nKubernetes Last piece of the puzzle is the Kubernetes deployment. Let’s log into our Kairos system with user kairos and password kairos\nssh kairos@IP Switch to root\nsudo -i We want to create some secrets and pass them as environment variables in the container. Take the telegram bot token and chat id and encode them. For example if my Chat ID is 1234567890 then I’d do as follows:\necho -n \"1234567890\" | base64 do the same with the bot token, e.g. if my bot token was 999999999:FooooooBaaaaaaaaR then I’d do\necho -n \"999999999:FooooooBaaaaaaaaR\" | base64 and copy them in a file called telegram-secrets.yaml and add the following content (remember to replace with your actual encoded values). Yes Kairos is immutable in some areas but the home directory is writable.\napiVersion: v1 kind: Secret metadata: name: telegram-secrets type: Opaque data: bot_token: OTk5OTk5OTk5OkZvb29vb29CYWFhYWFhYWFS chat_id: MTIzNDU2Nzg5MA== Save the file and apply it\nkubectl apply -f telegram-secret.yaml Now we create a deployment.yaml file to connect everything together\napiVersion: apps/v1 kind: Deployment metadata: name: rpi-doorbell spec: replicas: 1 selector: matchLabels: app: doorbell template: metadata: labels: app: doorbell spec: containers: - name: doorbell image: quay.io/mauromorales/doorbell:telegram-v1 imagePullPolicy: Always # Forces Kubernetes to always pull the latest image securityContext: privileged: true # Allows access to GPIO hardware resources: limits: memory: \"128Mi\" cpu: \"100m\" env: - name: TELEGRAM_BOT_TOKEN valueFrom: secretKeyRef: name: telegram-secrets key: bot_token - name: TELEGRAM_CHAT_ID valueFrom: secretKeyRef: name: telegram-secrets key: chat_id And apply it\nkubectl apply -f deployment.yaml If everything went as expected, you should see your pod running\nlocalhost:~ # kubectl get pods NAME READY STATUS RESTARTS AGE rpi-doorbell-67c78c5969-s6pw8 1/1 Running 0 20h And you can get logs from that pod tracking every time someone pressed your doorbell in addition to sending the Telegram message\nlocalhost:~ # kubectl logs rpi-doorbell-67c78c5969-s6pw8 🚀 Doorbell monitor started... [2025-03-05 13:51:34] 🔔 🚪 Doorbell pressed at 2025-03-05 13:51:34! [2025-03-05 15:41:25] 🔔 🚪 Doorbell pressed at 2025-03-05 15:41:25! [2025-03-05 15:42:13] 🔔 🚪 Doorbell pressed at 2025-03-05 15:42:13! [2025-03-05 18:28:26] 🔔 🚪 Doorbell pressed at 2025-03-05 18:28:26! [2025-03-06 09:59:18] 🔔 🚪 Doorbell pressed at 2025-03-06 09:59:18! Conclusion With this setup me and my partner can subscribe to the Telegram Bot and get notifications to our phones. I can also mute the bot if I’m in a meeting and don’t want to be disturbed. I could extend the system to not notify at certain hours of the day but still keep logs. If I’m not happy with Telegram then I can connect it to a different system. Whenever there’s a new version of Kairos I can upgrade the system remotely via the command line or through Kubernetes. If you have a similar setup but don’t want to buy a smart doorbell just yet, I can only recommend this setup.\n","categories":"","description":"","excerpt":"Picture this: You’re deep in focus, coding away, and the doorbell …","ref":"/blog/2025/03/18/how-i-automated-my-doorbell-with-kairos-and-you-can-too/","tags":"","title":"How I Automated My Doorbell with Kairos (and You Can Too)"},{"body":"We’re thrilled to announce that since April 2024, Kairos has been officially part of the Cloud Native Computing Foundation (CNCF) as a Sandbox Project. This marks an important milestone for Kairos and, more importantly, for our entire community of users and contributors.\nYou might be wondering - what does being part of the CNCF mean for you? The CNCF is known for backing the most innovative, reliable, and secure projects in the cloud-native space, and having their support brings a number of crucial benefits. Let’s dive into why this matters.\nStability and Long-Term Support First and foremost, being part of the CNCF ensures that Kairos is built to last. The CNCF provides a stable and structured environment that safeguards against a project disappearing overnight, something that can be a concern for open-source users. With CNCF backing us, you can rest assured that Kairos is part of a thriving, open-source ecosystem with the resources and backing to stay sustainable for the long term.\nHigher Standards and Best Practices We’ve been quiet about this milestone until now because the CNCF onboarding process is rigorous, and we wanted to make sure we did it right. This onboarding process wasn’t just about paperwork—it pushed us to adopt even higher levels of best practices. You can see some of the measures we’ve adopted as part of this effort here. These improvements have been critical in refining Kairos, and we’ve emerged from the onboarding process more secure, more reliable, and better aligned with industry standards.\nGrowing and Collaborating with the Cloud Native Community Joining the CNCF has opened many doors for us to connect with other projects and communities. This past year, we’ve had the privilege of participating in KubeCon + CloudNativeCon in Hong Kong as part of the Project Pavilion, where we had a booth to engage with users and fellow contributors. We’ll be doing the same at KubeCon North America, and we’d love to see you there!\nCome visit us on Wednesday, November 13, from 10:30 am to 3:15 pm at the Project Pavilion area of KubeCon North America. Through these events, we’ve had the chance to collaborate with other CNCF project maintainers, share knowledge, and learn from one another. The spirit of open collaboration is at the heart of everything we do, and we’ve gained valuable insights that are helping us make Kairos even better for you, our users.\nImproved CI with ARM Support Another benefit of being part of the CNCF is access to resources we didn’t have before. Thanks to the CNCF, we now have ARM machines integrated into our CI pipelines, allowing us to test and build more efficiently. This not only speeds up our development process but also ensures Kairos runs smoothly on a wider range of hardware platforms, enhancing performance and broadening the scope of where you can use it.\nA Community Effort Finally, we want to emphasize that none of this would have been possible without you—our community. Your support, feedback, and contributions are what drive Kairos forward. Whether it’s through submitting pull requests, reporting issues, or simply sharing your experiences, you have been an integral part of our journey. We are grateful to have such a passionate and engaged community that keeps us motivated and moving in the right direction.\nAs we continue to grow within the CNCF and the cloud-native space, we’re excited about the opportunities that lie ahead. We remain committed to building Kairos in a way that benefits all of us, and we can’t wait to share more with you in the future.\nSee you at KubeCon North America, and thank you for being with us every step of the way!\n","categories":"","description":"","excerpt":"We’re thrilled to announce that since April 2024, Kairos has been …","ref":"/blog/2024/10/08/kairos-joins-the-cncf-as-a-sandbox-project/","tags":"","title":"Kairos Joins the CNCF as a Sandbox Project"},{"body":"\nI can’t believe it’s already October. I’ve been so busy following up on different events where we’ve brought some of the Kairos love that I forgot about Hacktoberfest. Thankfully, the month isn’t over so let’s get our hands dirty!\nLike last year Kairos is taking part in this event. This means that you can open PRs in any of our repositories, and we’ll make sure to tag them correctly so they count towards your progress (please contact us if this is not the case).\nIf you’d like to participate, start by looking at the tickets with the hacktoberfest label, but you can hack on any other ticket and we’ll be happy to add the tag for you.\nThis year, we will offer Kairos t-shirts and stickers to the first 10 contributions. So don’t wait too long to participate.\nDisclaimer: Only accepted PRs will count and Kairos is just an open-source project and we cannot guarantee that we can send swag anywhere in the world.\n","categories":"","description":"","excerpt":"\nI can’t believe it’s already October. I’ve been so busy following up …","ref":"/blog/2024/10/07/hacktoberfest-2024/","tags":"","title":"Hacktoberfest 2024"},{"body":"Kairos is versatile, supporting various Linux distributions such as openSUSE Leap, Ubuntu 24.04, and more. Our primary objective is to empower users to continue working with their preferred Linux distribution seamlessly. However, our previous documentation fell short of this goal. The scripts were tied to a default image, which often led to issues for users who preferred a different flavor.\nDuring our first Hackweek, we took a significant step forward by updating our documentation to allow users to select their preferred flavor, thereby enhancing the overall experience.\nHow does it work? To select your preferred flavor, simply use the drop-down menu at the top of the navigation bar on our website. Your selection is saved locally, meaning that if you access the documentation from a different browser or device, you’ll need to reselect your flavor.\nSelecting your preferred flavor will automatically update the scripts displayed on the page. If an example is specific to a certain flavor, such as the installation on Nvidia AGX Orin, you will see a warning at the top of the page, and the scripts will not switch to your selected flavor. When only a subset of flavors is available, and your preferred one is not among them, a warning message will appear above the script, informing you of the available flavors. The script will default to the first available option.\nGive us feedback! We hope this new feature enhances your experience with our documentation. If you have any feedback or suggestions, please don’t hesitate to reach out to us on the CNCF Slack channel\n","categories":"","description":"Discover how our new customizable flavor menu in Kairos documentation streamlines your experience by allowing you to select your preferred Linux distribution.","excerpt":"Discover how our new customizable flavor menu in Kairos documentation …","ref":"/blog/2024/08/14/enhancing-kairos-documentation-with-a-customizable-flavor-menu/","tags":"","title":"Enhancing Kairos Documentation with a Customizable Flavor Menu"},{"body":"Last week was a special week. We had our first hackweek! With everyone in the team connecting from their own corners of the world, Belgium, Greece, Italy and Spain, we came together not in person, but through a shared passion for hacking. Unlike typical team-building events, this hackweek was all about individual exploration — each team member diving into their own projects, driven by personal curiosity and the opportunity to experiment.\nOver the course of the week, we set aside our usual tasks to focus entirely on these self-driven projects, tackling challenges we rarely have time to address or exploring new avenues. In this post, I’ll share a glimpse of some of the projects we worked on.\nDuring the first day we got together to brainstorm. Many good ideas came from each of us on what to work on. It lifts my spirit to see that our project while very stable, still has lots of avenues to explore and that everyone in our team is keen on making Kairos a better project.\nKairos Kiosk \u0026 Quiz App by Dimitris Since we have some upcoming events where we will have a booth, we were thinking of a way to break the ice, start conversations with people and keep them engaged.\nOne idea was to set up a Kairos Kiosk with an app that quizzes people passing by with questions that introduce them to the ideas behind Kairos. Those who answer correctly will receive a prize. This approach makes it feel like a game while also serving as a conversation starter, depending on the quiz questions.\nWe had a look and couldn’t find an open source tool that would allow us to build such a quiz. Since I have been out of web development for quite some time, I thought it would be a refreshing project. I took the fastest road to success, using tools I was already familiar with, like gin-gonic, gorm and SQLite.\nFor the front end I used tailwindcss because I’m hearing that’s what the cool kids use these days. Since I’m terribly rusty in CSS, I had ChatGPT helping me out (actually doing most of the work at the front end).\nThe result of this effort is this repository: https://github.com/jimmykarily/quizmaker\nFuture plans include:\nimprove the README make it configurable so other teams can use their own logo and text create an easy way to collect results create an easy deployment method (kustomization / helm chart / other) I’m hoping to have it in good shape to be used in one of our upcoming events. Stay tuned!\nRaspberry Pi 5 \u0026 Docs Flavor Picker by Mauro As for me, I first decided to try to install Kairos on a Raspberry Pi 5. I wasn’t sure where to start since I’m not an expert on how ARM boards boot a system. So I opened a new session with ChatGPT and went on a mission to understand how such systems boot. While I cannot call this experiment a success, since my Raspberry is not properly booting, I did learn a lot and feel much more confident to tackle some booting issue with these boards in the future. I decided not to pursue this further because the Kernel is not ready for it, but I put my learnings on a blog post.\nI decided to use the rest of the week to do some major change on our docs by introducing a Flavor Picker, where you can choose which Kairos Flavor (e.g. Ubuntu, openSUSE, etc) you prefer to read the docs and all scripts will show the selected flavor so that you don’t have to be changing them manually but just copy and paste. A blog post about this change is also in the works.\nConclusion While we each worked independently, the results of our individual efforts have already begun to ripple through the Kairos project, influencing our roadmap and inspiring new directions for future development. This hackweek wasn’t about team building — it was about giving each person the space to innovate on their own terms, and the results speak for themselves.\nThe projects we embarked on, whether big or small, demonstrated the power of focused, independent exploration. We’re excited to see how these initial ideas will evolve as we continue to develop and refine them in the coming months. This hackweek has shown us that sometimes, the best way to move forward is to give everyone the freedom to explore their own path, and we’re eager to see where those paths will lead Kairos next.\n","categories":"","description":"Our First Hackweek","excerpt":"Our First Hackweek","ref":"/blog/2024/08/13/our-first-hackweek/","tags":"","title":"Our First Hackweek"},{"body":"We recently added analytics to our website. We were hesitant about this before because we are committed to your privacy and didn’t like the idea of setting up Google Analytics. Thankfully, we’ve found a good open-source solution that is designed for privacy. You can find more about them on their website https://plausible.io/about\nThe main reason for this change is to understand a bit better how our documentation and blog are working for you and the impact that our different efforts have when we share talks, articles and videos.\nIf you have any questions or comments, please don’t hesitate to share them through any of our official channels!\nCNCF Slack #kairos channel GitHub Discussions ","categories":"","description":"Using a privacy focused analytics service on kairos.io","excerpt":"Using a privacy focused analytics service on kairos.io","ref":"/blog/2024/08/01/adding-analytics-to-kairos.io/","tags":"","title":"Adding Analytics to Kairos.io"},{"body":"The recent CrowdStrike update fiasco that wreaked havoc across industries, causing BSODs and crippling critical systems, starkly highlights the need for resilient and immutable infrastructure. If affected organizations had adopted an immutable OS like Kairos, their systems would have shown greater resilience against such widespread disruptions.\nTL;DR CrowdStrike’s faulty update led to massive system crashes. Kairos’s immutable and decentralized architecture offers a robust alternative, ensuring system integrity and swift recovery in such scenarios.\nCrowdStrike Update: What Went Wrong On July 19, 2024, a flawed update from CrowdStrike led to a significant number of Windows machines experiencing BSODs (Blue Screen of Death). This impacted operations across airlines, banks, broadcasters, and more. The issue arose from a misconfiguration in a file responsible for screening named pipes, leading to system crashes with a specific stop code. The resolution involved manual intervention on each affected device (imagine addressing this on a big flee of edge devices!), highlighting the vulnerability of traditional IT infrastructures to such disruptions.\nHow Kairos Ensures Resilience Kairos, offers a fundamentally more resilient approach:\nImmutable Infrastructure: Helps ensure that system states are consistent and unchangeable, significantly reducing the risk of configuration drift and unauthorized changes that could introduce vulnerabilities.\nDecentralized Deployment: Allows each node to operate independently. This decentralization means that an issue affecting one node does not necessarily propagate across the entire network.\nRapid Recovery: Kairos’s architecture allows for quick rollback to previous stable states, minimizing downtime. In the case of the CrowdStrike incident, affected systems could have been swiftly reverted to their last known good state.\nEnhanced Security: By leveraging a secure boot process and a read-only root filesystem, Kairos minimizes attack vectors. Any attempt to alter system files or configurations would be futile, thereby protecting the system’s integrity. Since the update came from CrodStrike, this would not have played a role in this specific incident, but it is still worth mentioning.\nReflecting on the xz Utils Backdoor Incident To draw a parallel, let’s recall the xz Utils backdoor incident. Kairos handled it with efficiency, demonstrating the robustness of our approach. When a backdoor was found in xz Utils, affecting versions 5.6.0 and 5.6.1, Kairos’s response was swift and effective. We identified that only Kairos Tumbleweed versions 3.0.1 and 3.0.2 were affected, removed the compromised artifacts, and provided clear guidance for remediation.\nConclusion The CrowdStrike incident underscores the necessity for resilient, immutable infrastructure in today’s digital landscape. Kairos offers a proven solution, designed to withstand such disruptions and ensure continuity of operations. By adopting Kairos, organizations can protect themselves from similar future incidents, ensuring their systems remain secure, stable, and operational.\nFor a detailed understanding of our approach and how we handled the xz Utils backdoor, refer to our detailed post on the incident.\nStay resilient, stay secure.\n","categories":"","description":"","excerpt":"The recent CrowdStrike update fiasco that wreaked havoc across …","ref":"/blog/2024/07/22/the-importance-of-resilient-infrastructure-how-kairos-could-have-mitigated-the-crowdstrike-outage/","tags":"","title":"The Importance of Resilient Infrastructure: How Kairos Could Have Mitigated the CrowdStrike Outage"},{"body":" We are thrilled to announce the release of Kairos v3.1.1! This patch release is a bugfix and security release.\nKey Features and Updates All binaries built with latest go version (1.22.5) To enhance security and address existing CVEs, we updated the build version of GoLang across all our binaries. This involved systematically bumping the GoLang version in our build configurations to the latest stable release. By doing so, we ensured that our applications benefit from the latest security patches, performance improvements, and bug fixes. This proactive measure not only mitigates vulnerabilities but also aligns our development environment with best practices, fortifying our software against emerging threats. Alpine RPI fixes We concentrated our efforts on resolving the remaining issues with Alpine RPI. Our team diligently identified and addressed the last of the reported bugs, ensuring that the system operates smoothly and efficiently on Raspberry Pi devices. Through rigorous testing and meticulous updates, we successfully enhanced the stability and performance of the platform, providing a more reliable and seamless experience for our users. Full Changelog For a detailed list of changes, improvements, and known issues, please visit the full changelog on GitHub.\nThis release is a testament to our commitment to providing a robust and secure edge computing platform. We extend our deepest gratitude to our community for their invaluable contributions and support.\nThank you for being a part of the Kairos journey. Happy upgrading!\nStay tuned for more updates and enhancements.\n","categories":"","description":"Kairos release v3.1.1","excerpt":"Kairos release v3.1.1","ref":"/blog/2024/07/19/kairos-release-v3.1.1/","tags":"","title":"Kairos release v3.1.1"},{"body":" We are thrilled to announce the release of Kairos v3.1.0! This update brings a host of significant enhancements to further secure and streamline your edge computing environment.\nKey Features and Updates Unified Kernel Images (UKI) Measured systemd-sysext: Introducing the ability to measure systemd extensions, enhancing security and integrity. Verify Upgrade Artifacts Signature: Ensuring the authenticity of upgrade EFI files before applying updates. Autoenroll Keys: Automatically enroll keys during live CD boot in setup mode for seamless initialization. Slim Artifacts to address memory constrains on some devices. Base Distribution Updates Updated support for Fedora 40, Ubuntu 24.04, and Leap 15.6, ensuring compatibility with the latest releases. Expanded Support Debian on ARM: Now supporting ARM architecture for Debian, broadening the hardware compatibility. HTTPS Support for IPXE Artifacts: Enhancing security with HTTPS support for IPXE artifacts. Potential Breaking Changes By default, UKI artifacts now exclude Linux modules and firmware to reduce file size for better EFI compatibility. This may impact hardware support. Review the full changelog on GitHub for details on how to customize these artifacts to meet specific hardware needs. Read more about this on the release notes.\nFull Changelog For a detailed list of changes, improvements, and known issues, please visit the full changelog on GitHub.\nThis release is a testament to our commitment to providing a robust and secure edge computing platform. We extend our deepest gratitude to our community for their invaluable contributions and support.\nThank you for being a part of the Kairos journey. Happy upgrading!\nStay tuned for more updates and enhancements.\n","categories":"","description":"Kairos release v3.1","excerpt":"Kairos release v3.1","ref":"/blog/2024/07/11/kairos-release-v3.1/","tags":"","title":"Kairos release v3.1"},{"body":"In the evolving landscape of cybersecurity, protecting the integrity of computing systems from the moment they power on has become very important. As threats become more sophisticated, understanding and implementing advanced boot security mechanisms like Trusted Boot, Full Disk Encryption (FDE), Secure Boot, and Measured Boot are critical for safeguarding data and ensuring system integrity. This article demystifies these concepts, explores their significance, and examines their implementation in modern computing environments, particularly focusing on the Linux ecosystem and the approaches within the Kairos project.\nWe are facing new challenges as 2024 starts. While AI is pushing innovating technologies to the edge, security becomes even more critical for organizations to protect sensitive data.\nImage credits https://www.reddit.com/r/PBSOD/comments/c8nusw/it_is_now_safe_to_turn_off_your_computer_atm_in/\nAs industries evolve over time, security measures are increasing to protect edge devices from attackers. New attack vectors and vulnerabilities are exploited every day by cybercriminals, making it crucial for industries to continuously update and enhance their security systems. In the context of edge devices, which are computing elements that process data at the edge of the network, closer to the source of data generation, this need is even more pronounced. These devices, including ATMs, point-of-sale systems, and IoT devices, are often the front line of cybersecurity battles.\nOne of the primary challenges in securing edge devices is their inherent diversity and distributed nature. Unlike centralized systems, edge devices are spread across various locations, often in unsecured or semi-secured environments, making physical and network security a significant concern. This dispersion also implies a broad attack surface, with each device potentially offering a unique set of vulnerabilities. What we are trying to build here is a new foundation to define how to deploy secure devices which cannot be tampered with, and can keep data secure from malicious actors. We do want to protect the software stack that we run on it .\nOur goal is simple but complex to achieve: No unauthorized access to the software, to the data which is generated by the machine, and finally, we don’t want the machine to execute modified code.\nThe Essence of Boot Security and Encryption Here comes Trusted Boot and SENA to the rescue. Easier to explain with an Alice metaphor.\nYou can read more about Trusted Boot in the Lennart blog and about SENA in our blog post\nLet’s imagine Alice has this super cool treasure chest, chock-full of her most prized stuff. She’s big on security, but in a high-tech way – kind of like how your computer needs to keep its data safe. To get this done, your computer uses some smart tricks, things like Trusted Boot, Full Disk Encryption (FDE), Secure Boot, and Measured Boot.\nTo picture Full Disk Encryption, or FDE, think of it as Alice’s treasure chest having a lock that only opens with a secret code. It’s so clever that if someone breaks into her house and finds the chest, they still can’t get inside without that special code. FDE is like that for your computer – it scrambles all your data so that only someone with the right password can read it.\nNext, there’s Secure Boot. Imagine if Alice had this smart doorbell. When someone presses it, the doorbell checks if they’re on the guest list. If they’re not, no entry. Secure Boot is your computer’s version of this smart doorbell. It checks every piece of software when your computer starts up and only lets the good, trusted stuff in. It keeps out anything that looks fishy.\nAnd then, we have Measured Boot. Picture this as Alice keeping a super detailed logbook. She writes down everything that goes on in her house – who comes in, what they do, you name it. If anything odd pops up, she’s on it. Measured Boot does something similar for your computer. It watches over the startup process, takes notes, and makes sure everything’s running just like it should, no funny business.\nPutting all these together, you’ve got Trusted Boot, which is like Alice’s ultimate home security system – the secret-code lock, the smart doorbell, and the detailed logbook, all working together. We’re going to unpack how these nifty tools keep your computer as secure as Alice’s treasure chest, making sure your digital world is safe every time you power up.\nChallenges and Solutions in Implementing FDE on Linux While FDE is a standard feature in various operating systems, its implementation in Linux presents unique challenges. The initial boot stage, including the kernel and initial file system setup, cannot be fully encrypted as they have to be read by the firmware and the kernel respectively, leaving a portion of the system exposed. The initramfs for instance is responsible for unencrypting the portions of the disk. However, through careful measures, this unencrypted segment can be protected against tampering, thus securing the system’s boot process and the encrypted data it accesses.\nThe Kairos project adopts “Unified Kernel Images” (UKI) to enhance boot security as defined by the UAPI group. UKI files are single, fat binaries that contain the OS and necessary boot components in a single, verified file. This approach simplifies the boot process, allowing for the entire system to be booted securely and efficiently, but it presents as well interesting challenges due to the Firmware restrictions that might depend on the hardware used(such as booting EFI large files, or handling Secure Boot certificates). By leveraging EFI files that can be signed and verified through Secure Boot and measured as a single entity, UKI files represent a significant advancement in boot security.\nCentral to the Trusted Boot mechanism is the Trusted Platform Module (TPM), a dedicated hardware (also emulated by firmware) component designed for secure cryptographic operations. TPM chips play a critical role in storing encryption keys and measurements securely, enabling the system to verify boot integrity and encrypt user data effectively.\nFrom Grub to systemd-boot Trusted Boot is deeply integrated into systemd and is currently being actively developed. Linux offers a variety of init systems and bootloaders, such as GRUB. However, systemd includes tools like systemd-measure and ukify, which, although they work with various bootloaders, are particularly effective with systemd-boot. What sets systemd-boot apart from GRUB and other bootloaders is its focus on hardware compatibility support. As we aim to support modern hardware that must meet specific requirements, we prioritize minimizing the attack surface. Therefore, a narrower scope of support is a better fit for our specific needs and preferred over bootloaders designed for broad hardware compatibility, which often carry legacy code and a large patchset (for instance, you can see the number of patches that SUSE applies on top of standard GRUB package to add support to specific hardware/improve the functionalities). This presents a strong argument for choosing systemd, especially systemd-boot, over other bootloaders which targets recent hardware and a smaller scope.\nWhy this makes sense: We’re now in an era where outdated hardware is not our concern. Modern devices don’t require old drivers, so we have the opportunity to streamline our systems. This translates to a principle of ’less is more’ in our code and in our stack - less software in our stack means enhanced security. We aim for a leaner runtime to run our software, minimizing potential security vulnerabilities. In addition, it also makes sense to use tools that adhere closely to standard specifications.\nConsidering the transition to modern hardware, especially with widespread EFI support, systemd-stub or systemd-boot is a strong contender to replace GRUB. GRUB has been a staple in the Linux community for years, evolving from versions 0.x to 2.x, which if you are seasoned like me, you have witnessed almost all of them! However, it’s worth considering the extensive patches applied by distributions and its expansive codebase. While its extensive support and scripting capabilities were once advantageous, they now detract from its suitability as a lightweight option.\nThe goal is clear: we aim to minimize complexity in building a secure, efficient stack, reusing as much components well trusted and established by the community. By reducing the number of components, we strive for a more secure and streamlined system.\nBooting in Linux When a Linux system starts, it goes through a multi-stage process to load the operating system. This process can be broken down roughly into the following steps:\nFirmware Initialization: The system’s firmware, which could be BIOS or UEFI, performs initial hardware checks and configurations. It then searches for a bootloader, which is typically located in the boot partition or the Master Boot Record (MBR) of the storage device. Bootloader Execution: The bootloader, such as GRUB, is a software responsible for loading the main part of the operating system. Its primary task is to find the Linux kernel, usually located in the same partition, and load it into memory. The bootloader might present a menu or configuration options to choose different kernels or operating systems if multiple are installed. Kernel Loading and Initialization: The kernel is the core of the Linux operating system. Once loaded, it initializes the system’s hardware and sets up essential services. During this phase, the kernel decompresses and loads an initrd (initial ramdisk). The initrd is a temporary root file system loaded into memory. It includes essential tools and scripts needed to mount the real root file system. In modern Linux systems, initrd is often replaced by initramfs, which serves a similar purpose. File System Setup and Transition: Inside initrd, scripts are executed to prepare the actual root file system. This includes tasks like decrypting and encrypted partitions if present. After preparations are complete, the system makes a transition, referred to as “pivoting”, to the actual root file system. Root File System and User Data: The root partition, now mounted as the root file system, contains all the user data, system configurations, binaries, drivers, and other necessary components for the operating system to function. This is where the system runs from once the boot process is complete. Throughout these stages, the components—firmware, bootloader, kernel, initrd, and the root file system—work in a sequence to load the operating system successfully.\nThis chain is the one we in the Linux and tech industry are very much familiar with. Several designs and strategies have been developed to secure this stack over time, for instance Verified kernels and drivers, or Secure Boot to sign the bootloader and be able to trust its authenticity. There are various bootloader implementations as well, tied to more secure HW (like TPM devices).\nHowever, as technologies and security measures evolved, attacks evolved as well. Specific hardware now can be leveraged to increase the security posture of the boot process, and it is time for a change, as the market demands more sophisticated security measures to protect against malicious actors.\nBooting with a TPM-equipped Hardware When a device equipped with TPM, and a sufficiently modern hardware boots, several steps happen sequentially.\nMany hardware platforms use a Core Root of Trust for Measurements (CRTM), which is the very first thing that boots, even before the firmware. The CRTM gets a hash of the firmware and sends it to the TPM chip, which measures the running software and is a requirement for trusted boot. The TPM chip then loads the firmware/BIOS.\nThe TPM takes note of the measurement and stores the hash in a bank of multiple platform configuration registers (PCRs). In order to store measurements, the TPM chip extends the banks from the previous values, as these changes to the stack are easy to recognize during the boot.\nNext, the BIOS or firmware measures the subsequent stage (bootloader or UKI) and sends it to the TPM, then loads the UKI and continues booting.\nOnce the TPM is asked to release an encryption key to unseal the full disk encryption, it will check if the measurements it has are valid. If a bootloader is present, it will have already measured the UKI files, and measurement also happens when initrd starts and when the running system is ready to check any manipulations (e.g., kernel boot command line). The process is also bound to the secure boot signatures, so any manipulation of the UKI files would be allowed only by the key holders.\nIf a malicious attacker tries to load custom software or modify an image in any way, they must extend the PCR value and align it with the value it would have had following an expected boot. The cryptographic robustness of the hash algorithm makes achieving this computationally expensive, thereby helping to support security.\nIf the firmware or bootloader have been tampered with, the hash values stored in the PCR won’t match expected measurements, and the system will flag an alert and prevent boot from happening — intentionally bricking the host device as a failsafe (the host is still recoverable by re-installing everything from scratch).\nKairos and Trusted Boot To gain a deeper understanding of the mechanics behind EFI files and the boot process, let’s delve into the way typical Linux distributions manage booting with Full Disk Encryption (FDE):\nInitially, the firmware retrieves the bootloader from the boot partition (or Master Boot Record - MBR) and transitions control to a kernel. This kernel then unpacks an initrd, which houses essential tools needed for initializing the file system (FS) and transitioning to the operational system environment. In this setup, each component is distinct: the Kernel and Initrd are separate files, whereas the root partition is a unified file system encompassing all user data (such as installed binaries, password files, sensitive information, drivers, etc.). The primary function of the initrd in this scenario is to decrypt partitions and set up mount points as it transitions control to a different partition.\nContrastingly, the Kairos architecture utilizes a singular image file that amalgamates the rootfs, kernel, and initrd. Here, the initrd is responsible for establishing the FS and activating the rootfs.\nThe bootloader plays a pivotal role in this architecture by accessing image files stored within the state partition.\nWhile the state partition remains unencrypted, the system’s operational projection is derived from the encrypted user-data partition. Directories such as /etc and /opt (which can be fully customized) are overlaid, safeguarding sensitive information from being exposed within the state partition. Nevertheless, this design, along with the previously mentioned setup, is vulnerable to “Evil Maid Attacks.” These attacks involve altering the system by booting from a LiveCD and modifying the booted images.\nIn the context of the UKI (Unified Kernel Image) architecture, which closely resembles the Kairos model, the primary distinction lies in the storage of a singular EFI image file within the boot partition. This arrangement facilitates the signing and verification of EFI files to ensure they haven’t been tampered with, leveraging SecureBoot in tandem with Measured Boot. These mechanisms verify that the image is certified by a trusted authority and remains unmodified.\nDuring the boot process, cryptographic assessments are made using the Trusted Platform Module (TPM) to confirm the system’s integrity. Drive decryption during boot is contingent upon consistent signing by the same authority and accurate boot measurement matching. System upgrades involve updating the boot measurement to enable access to the user data partitions.\nSecurity consideration The existing framework guarantees that the initial phase (UKI file) remains immutable, with only this phase capable of decrypting the drive’s encrypted data. Indeed there is no pivoting into another system like in the traditional Linux boot process. This design choice ensures that the system remains a single component, with the UKI file acting as the sole point of entry for the system.\nShifting to a different image would have introduced potential security vulnerabilities and constraints:\nThe current Systemd tools lack the capability to assess another rootfs/file. There is an absence of mechanisms to incorporate PCR with hash checksums of bespoke files in the expanded PCR banks within systemd. The inability to evaluate a secondary phase means any alterations in this stage could remain undetected during operation. Also, implementing measurements for a second phase would significantly complicate the update process, as each version (A/B, Recovery) would require corresponding updates and measurements for every file involved in the transition.\nAn alternative approach involves maintaining a singular initial phase, which would entail measuring all subsequent stages. However, this concept remains a topic of ongoing debate without any concrete solutions thus far.\nA concrete example: should a hacker remotely access the system without measures in place for the transitioned stage, they could alter the entire rootfs during operation without detection. In contrast, by leaving the first stage unaltered in the current method, modifications to the rootfs are prevented.\nLearn More Trusted boot is one of many recommended security techniques — but it’s absolutely critical to ensuring you can trust the integrity of your device, particularly when it’s deployed in the field.\nWe’ve only just scratched the surface of trusted boot as a concept, how it works and the architectural details of the Kairos implementation that contribute to the overall security posture. If you’d like to learn more:\nCheck out our CNCF presentation, available on demand. Learn more secure edge architectures including trusted boot, by downloading the SENA white paper. Check out Kairos’ trusted boot architectural documentation and try it out for yourself. ","categories":"","description":"","excerpt":"In the evolving landscape of cybersecurity, protecting the integrity …","ref":"/blog/2024/04/10/unlocking-the-mysteries-of-trusted-boot-a-deep-dive-into-secure-system-boot-processes/","tags":"","title":"Unlocking the Mysteries of Trusted Boot: A Deep Dive into Secure System Boot Processes"},{"body":"It’s all over the tech news. Someone managed to put a backdoor on xz Utils, a very common package on Linux systems. In this post I want to share with you about what happened, how it impacted Kairos images, and what you should do in case you were affected.\nTL;DR A backdoor that can be used to exploit systemd based Linux via ssh was introduced in xz Utils. Only Kairos Tumbleweed v3.0.1 and v3.0.2 were affected. We deleted all related OCI images from our repos and artifacts from our releases. If you installed it and the system was exposed to the internet, you should do a complete re-install. If you hosted security keys in given system, you should rotate them.\nxz Utils Backdoor Versions 5.6.0 and 5.6.1 of xz Utils, which is used in many Linux distributions, are infected with a backdoor mechanism. Even if you don’t install xz Utils directly, you might still be exposed if you’re on a systemd based distribution, which is the case of most commonly used distros nowadays. This is because the init system, has a dependency on liblzma one of the affected libraries. The attack can be triggered via ssh access, which could have been catastrophic considering how many systems today connect to the internet.\nIf you want to read more about how the backdoor read one of these posts, or read Andres’ disclosure email to the OSS. However, more interesting than the technical implementation of the backdoor, is all the social engineering that took place.\nI would like to bring your attention to the fact that in 2024, many critical open-source projects are still maintained by individuals, who do this on their spare time and who are not paid for it. It’s no wonder that these maintainers are burned out or suffering of other mental health issues, as it is the case of the official xz Utils maintainer. Under such conditions it’s no wonder that he was tricked by a malicious actor. As a community, we must find a solution because this time the issue was caught before it widespread, but we might not be as lucky next time.\nHow it affected us? On Monday, April 1st, after reading about the news, team member Dimitris Karakasilis decided to investigate which of our systems was affected. In his evaluation he noticed that the infected xz packages reached only Kairos Tumbleweed v3.0.1 and v3.0.2. The affected package never reached any of our other flavors since they are not rolling releases and therefore didn’t get a package update.\nAfter the analysis, on April 2nd, we deleted all related artifacts including OCI images from quay.io/kairos/opensuse, and ISOs from out GitHub Releases. Despite the artifacts not being available any longer, we decided to add a note of caution on the affected releases for anyone who wonders why the Tumbleweed artifacts are missing, and to better inform users on how react.\nWhat to do if you were affected? Since only Tumblweed was affected, we decided to point you directly to openSUSE’s recommendation, in which they recommend to:\nFor our openSUSE Tumbleweed users where SSH is exposed to the internet, we recommend installing fresh, as it’s unknown if the backdoor has been exploited.\nDue to the sophisticated nature of the backdoor an on-system detection of a breach is likely not possible.\nAlso rotation of any credentials that could have been fetched from the system is highly recommended.\nhttps://www.suse.com/security/cve/CVE-2024-3094.html\n","categories":"","description":"","excerpt":"It’s all over the tech news. Someone managed to put a backdoor on xz …","ref":"/blog/2024/04/02/xz-utils-backdoor/","tags":"","title":"xz Utils Backdoor"},{"body":"I recently had the opportunity to attend KubeCon 2024. You can find my recap at the Spectro Cloud Blog, but I’d like to add some additional information about the Special Purpose Operating System Panel in which we participated.\nThe panel was scheduled for the last day of KubeCon, which I think probably hurt the attendance since many people had already left. Nevertheless, according to the scheduling tool, 351 people were planning to attend.\nOn set, we had Sean McGinnis from AWS representing Bottlerocket, Danielle Tal from Microsoft representing Flatcar, Felipe Huici from Unikraft representing Unikraft, Justin Haynes from Google representing COS and myself from Spectro Cloud representing Kairos. The panel was moderated by Thilo Fromm also from Microsoft, who did a fantastic job keeping everyone engaged. In the audience we also had representatives from Talos and EveOS. Most of us are part of the Special Purpose Operating System Work Group under the CNCF TAG Runtime.\nWe started with a short introduction about what SPOSes are, and it was refreshing to hear that the audience had a good idea. If you head to the CNCF Work Group page, you will see that we use the following definition:\nSpecial Purpose Operating Systems are designed to run well defined workloads with minimal boilerplate of dependencies suited for niche use cases.\nIn the case of Kairos, our specialization focuses on Day-2 operations on Edge devices, but not excluding other setups including Kubernetes. This means that in our OS we try to pack everything users will need to keep their devices secure, remotely manageable and as lean as possible.\nNext, we presented a slide called Landscape which had one axis going from Highly Specialized to Flexible/Customizable. In my opinion this is misleading because as I previously mentioned Kairos is highly specialized, however it is also very flexible and customizable. My best example for this is that we are the only SPOS which can be based on different Linux distributions. If you are looking for such a specialized solution but come from Alpine, Debian, Fedora, openSUSE, Ubuntu or Rocky Linux, you don’t have to give up your existing OS know-how because there’s a Kairos version for you.\nThe landscape analogy still fits, but instead of having to fit the different SPOSes into made up groups, I think of it as consisting of areas like Kubernetes, Edge Computing, AI, Data Centers, etc. and each of the different SPOSes being specialized in one or more of these areas. On a completely different axis, we could define the flexibility and customization for each individual SPOS. For example, Talos, has an opinionated API to handle the OS, so we could consider it more rigid, while all other projects that give ssh access would be on the other end of the spectrum. And this can be applied to different topics, like configuration management, Kubernetes distribution, etc. That rigidity or flexibility is not necessarily good or bad but better depending on what you are trying to achieve.\nThe rest of the time was dedicated for Q\u0026A.\nLast but not least, let me reiterate like I did in the Spectro Cloud post that, these panels and similar initiatives are important because they help users understand why these projects exist and get an idea of why we have different options and how that makes us a stronger community instead of rivals.\n","categories":"","description":"","excerpt":"I recently had the opportunity to attend KubeCon 2024. You can find my …","ref":"/blog/2024/03/26/spos-panel-at-kubecon-paris-2024/","tags":"","title":"SPOS Panel at KubeCon Paris 2024"},{"body":" The team is very excited to announce the next major release of Kairos, Kairos v3! This release marks a major milestone in our roadmap by adding support for Unified Kernel Images (UKI). This will enhance the level of security that you can achieve on your system with the help of Trusted Boot.\nTrusted Boot At a glance, this feature will enable users of Kairos, to measure and sign with your own keys the Kernel, initrd and boot cmdline, ensuring that only your images can be booted in a given system. An in-depth post will follow explaining the technicalities of how we do this. In the meantime, you can head to our docs:\nTrusted Boot Architecture Trusted Boot Installation Trusted Boot Upgrades Versioned Docs Until now, we only had one source of documentation. We know this can be problematic because you weren’t able to tell from the documentation if a certain feature, configuration or else was meant for the nightly release or if it was already included in the Kairos version you’re running. We did our best adding “notes” on some of the sections, but as you can image this becomes problematic easily. For this reason, we’ve decided to version our documentation page starting with v3. To access it, all you need to do is head to https://kairos.io/docs/ and click on the Releases’ menu in the nav bar, which will list the available versions, including “master” which is our nighly release. Please check it out and let us know what you think.\nKnown Issues 🐛 Raspberry: EFI booting no longer supported on kernels shipped with ubuntu \u003e 22.04 #2249 🐛 Filesystem expansion on rpi4 doesn’t work with Alpine #1995 🐛 cgroup_memory not mounted in Alpine rpi4 #2002 🐛 Upgrade on Alpine arm errors #2135 🐛 reset from the GRUB menu on Alpine amd64, gets stuck in an endless loop #2136 Deprecation Warnings Reading of /etc/elemental/config.yaml was broken for a bit but should be fixed in v3, but it will be eventually deprecated in one of the upcoming releases. If you’re making use of it, please move this configuration to /etc/kairos/config.yaml\nFlavor Updates UKI Ubuntu and non-UKI Ubuntu differ a bit. This is because there is a limitation on the image size that can be loaded. This is defined by the firmware, so we cannot guarantee a common image size that will work on every system. So far we have seen limits of around 800Mb to 1Gb. We will continue working towards this goal so Kairos UKI images can be booted in as many systems as possible. It is now possible to build Ubuntu 24.04 LTS, but we don’t release any artifacts yet, and will only do so when it’s officially released. For a comprehensive view of all the changes in this release, please refer to the full changelog (And be sure to check out the “Known issues” section for any potential hiccups.)\nThis release marks a significant milestone in the evolution of our project, and we want to extend our heartfelt thanks to everyone who contributed to this release. Whether through code contributions, reviews, bug reports, comments, debugging output. Your support and engagement are invaluable!\n","categories":"","description":"Kairos release v3","excerpt":"Kairos release v3","ref":"/blog/2024/03/08/kairos-release-v3/","tags":"","title":"Kairos release v3"},{"body":"Last year I said you could expect Kairos to engage further during FOSDEM. And I’m quite pleased to say that’s exactly what we did! As part of the open-source ecosystem, we recognize the importance of participating in these types of events. We do so, because we understand how critical it is to go where our users are, but this year we went a bit further and we also reached out to other similar projects.\nFrom Free-Lunch to Dog-Fooding: A Culinary Journey in Crafting IaC for Kairos Testing and Building Dimitris did a fantastic job sharing about how we switched our CI/CD pipelines, from using GitHub’s free workers to our own, based on Kairos. You can check out the slides and recordings here (as soon as they become available). We were quite pleased with the level of attendance, and we believe that the message came across.\nIf you attended the talk, and tried to reach out in person, we’re sorry that we didn’t give you a chance, we needed to make space for the next speaker. Please don’t hesitate to use any of our communication channels in the community page.\nSpecial Purpose Operating Systems Panel (SPOS) Originally, representatives from Kairos, Flatcar, and Unikraft wanted to have a panel between the different SPOSs. Unfortunately, FOSDEM didn’t accept our request until the day of the event, so everything had to be done ad-hoc. Which probably ended up being a good thing because we were lucky to also have the participation from Eve-OS, MicroOS and Softiron on top of the gang which originally signed up.\nI personally found it very helpful to hear we were not alone in our struggles and to learn about how each OS tries to solve sometimes the same problem a little differently depending on their target audience and vision.\nWe agreed to meet again for FOSDEM 25, so if you’re working in a single-purpose OS, it would be great to get to know you and your project next year. Please reach out via the CNCF SPOS working group charter.\nAnother good news is that, the panel will still take place because the submission got accepted at KubeCon. Representatives from Bottlerocket (AWS), COS (Google), Flatcar (Microsoft), Kairos (Spectro Cloud), and Unikraft will meet on Friday, March 22nd to talk about what makes our OSs different and what unites us.\nConclusion As you probably know, Kairos is fully open-sourced, for us this means that we will keep working towards reaching out to users and other projects as much as we can. We believe this is the best way to contribute to the community overall and to build a great project.\nIf you want to know about events we are interested in attending, you can find them on our Outreach project. If you are an organizer or know about an event where you think we should participate, please open a Feature Request.\n","categories":"","description":"","excerpt":"Last year I said you could expect Kairos to engage further during …","ref":"/blog/2024/02/06/kairos-at-fosdem-2024/","tags":"","title":"Kairos at FOSDEM 2024"},{"body":" Happy new year to all of you in the Kairos community! This 2024, we have many great plans that we want to achieve. You can find more about them in our roadmap.\nWe start the year with the release of Kairos v2.5.0. This time, we focused on the ground work for two major features that will land later in the year\nImproving the Kairos Factory user experience: On previous releases we shared how our artifact names have changed to make it easier to distinguish between them. In this release we worked on Versioneer, a component that helps build such names in more sofisticated ways than the original script did. This has been aggregated to the kairos-agent upgrade command to help you filter through upgradable versions. Adding support for UKI (Unified Kernel Images): This is still a WIP but we already have a proof of concept, meaning that Kairos will increase its security level by validating signatures using the EFI bootloader. Known Issues We haven’t been able to address the following issues on Alpine:\n🐛 Filesystem expansion on rpi4 doesn’t work with Alpine #1995 🐛 cgroup_memory not mounted in Alpine rpi4 #2002 🐛 Upgrade on Alpine arm errors #2135 🐛 reset from the GRUB menu on Alpine amd64, gets stuck in an endless loop #2136 Flavor Updates Ubuntu 23.04 got updated to 23.10 Alpine 3.18 got updated to 3.19 We know produce Raspberry Pi 3 Artifacts thanks to the contribution to our community member Ludea #1966 For a comprehensive view of all the changes in this release, please refer to the full changelog (And be sure to check out the “Known issues” section for any potential hiccups.)\nThis release marks a significant milestone in the evolution of our project, and we want to extend our heartfelt thanks to everyone who contributed to this release. Whether through code contributions, reviews, bug reports, comments, debugging output, or simply joining our meetings, your support and engagement are invaluable.\n","categories":"","description":"Kairos release v2.5","excerpt":"Kairos release v2.5","ref":"/blog/2024/01/11/kairos-release-v2.5/","tags":"","title":"Kairos release v2.5"},{"body":" Did you ever look at a release and couldn’t decide if it should be a patch, a minor or a major version bump? It has happened to everybody (if not, let me know). This Kairos release was a similar case. We didn’t introduce any breaking changes and it was just bug fixes, which makes it a patch release. At least that’s the story for user facing changes. Because, behind the scenes, we made some heavy changes, in the way we produce the Kairos artifacts and how we name them.\nThe detailed list of changes can be found in the release notes but the most important things to notice are listed below.\nNew artifact names The artifacts (including container images) have consistent names which include all the information about the image. For example, the “standard” image (with k3s) for the Opensuse leap 15.5, targeting amd64 generic devices, in Kairos version 2.4.0, was called:\nquay.io/kairos/kairos-opensuse-leap:v2.4.0-k3sv1.27.3-k3s1 In version 2.4.2 it’s called:\nquay.io/kairos/opensuse:leap-15.5-standard-amd64-generic-v2.4.2-k3sv1.27.6-k3s1 First of all the beginning of the image name reads naturally like the distro is usually named, including the version of the upstream distribution (15.5). Then, there is the information about:\nthe variant (core or standard) the architecture (amd64, arm64 etc) the model (generic, rpi4 etc) the Kairos version (v2.4.2) including the k3s version if it’s a standard image And the same information is there in the bootable artifact name in the same order:\nkairos-opensuse-leap-15.5-standard-amd64-generic-v2.4.2-k3sv1.27.6+k3s1.iso\nLess Earthly, more Dockerfiles Much of the logic from Earthly has been moved to the Dockerfiles. And while this seems like an internal technical detail, it take us closer to a simpler build process, which everyone can replicate in their own CI to build custom Kairos images. The input to the dockerfiles is the information you see in the artifact names (see above) and comes directly from the flavors.json file. If all this sounds complicated, just keep this:\nIn one of the next releases, is will be possible to build a full Kairos image (standard or core) using only one of the dockerfiles (no Earthly) and a block of information like:\n\"flavor\": \"opensuse\", \"flavorRelease\": \"leap-15.5\", \"variant\": \"standard\", \"model\": \"generic\", \"baseImage\": \"opensuse/leap:15.5\", \"arch\": \"amd64\", This allows one to change the base image above, to a derivative of opensuse/leap:15.5 (e.g. on with some additional packages installed). It also justifies the next item, please read on.\nAlma linux removal from artifacts We identified that Rocky linux and Alma linux were completely similar in regards to building steps. Given the resource and time constraints we have, we decided to no longer build and release Alma linux artifacts. The dockerfile for the rhel family supports the almalinux flavor. Building an Alma linux Kairos image from the Kairos repository root, is as simple as:\nearthly --platform=linux/amd64 +base-image \\ --VARIANT=core \\ --FLAVOR=almalinux \\ --FLAVOR_RELEASE=9.2 \\ --BASE_IMAGE=almalinux:9.2 \\ --MODEL=generic \\ --FAMILY=rhel Transition to pure Alpine In our latest release of Kairos, we’ve made a significant shift in the architecture of our Alpine flavor. We’ve transitioned from using the initramfs and kernel from both openSUSE and Ubuntu to a full-fledged Alpine kernel and initramfs system. This change marks a significant milestone in our journey to provide a more robust and efficient system for our users.\nThe Old System Previously, our Alpine flavor of Kairos was built on two different systems. One used the initramfs and kernel from openSUSE, and the other used the initramfs and kernel from Ubuntu. Both systems used the Alpine root file system. While these setups served us well, they had their limitations. The most notable one was the use of systemd during the initramfs and openrc during system boot. This dual system added complexity and potential points of failure to our boot process.\nThe New System In our new release, we’ve written our own implementation of the initramfs script for our Alpine flavor. This script sets up the system, creating a more streamlined and efficient boot process.\nThe switch to the Alpine kernel and initramfs system brings several benefits:\nSimplicity: Alpine Linux is designed to be simple, which makes it easier to maintain and less prone to errors. Security: Alpine Linux uses musl libc and busybox to provide a small binary size with significant security benefits. Efficiency: The Alpine kernel and initramfs system are lightweight, which means Kairos now requires fewer resources to run. Conclusion The transition to a full Alpine kernel and initramfs system is a significant step forward for our Alpine flavor of Kairos. It simplifies our architecture, improves security, and increases efficiency. The changes in the build process make Kairos more friendly to customizations. The naming changes, make it easier to navigate among the numerous artifacts we generate with every release. Finally the various bug fixes and smaller improvements should make it a more stable release for everyone.\nWe’re excited about these changes and look forward to seeing how they benefit our users.\nReach out to us with your comments!\n","categories":"","description":"A patch release that looks bigger than it is","excerpt":"A patch release that looks bigger than it is","ref":"/blog/2023/11/14/kairos-release-v2.4.2/","tags":"","title":"Kairos release v2.4.2"},{"body":"Hacktoberfest 2023 is here, and we are excited to announce that Kairos is taking part by creating some good first issues for new contributors to get started with and also by hosting a Hacktoberfest event on October 19th in Brussels.\nWhy is Hacktoberfest important to us? We believe in open source, this is why all of Kairos’ source code is fully open-sourced, but our commitment doesn’t stop there, we also want to help developers, students and anyone interested to learn more about open source and how they can contribute to it.\nWhy is Kairos a good project to contribute to? Kairos is one of those projects that touches many different areas of software development, from the frontend to the backend. While code is what makes Kairos work, our project would not be easy to use without the documentation, and it would not be possible to maintain without the community.\nFor users If you are a user of Kairos, you can help us by reporting bugs, requesting features, or even by writing documentation. No matter how small you think your contribution is, it will help us make Kairos better for everyone.\nFor developers Whether you are an experienced developer or just getting started, Kairos has something to offer. This means you can contribute to the project using the skills you already have, or you can learn new ones. You don’t need to be an expert in any of these areas to contribute, we are here to help you learn and grow. Have a look at our stack:\nCoding Go Shell scripting JavaScript Testing Linux Distributions (Debian, Ubuntu, Fedora, Arch, etc) Init systems (Systemd and OpenRC) Package managers Initramfs Dracut Immutability Containers and Dockerfiles management Kubernetes Static Site Generators Edge computing User Interfaces Web Command Line Plus you can also learn about unique Kairos related projects like:\nLuet package manager Immucore EdgeVPN How can you contribute? Contributions can be anything from a typo fix to a new feature, we are open to all kinds of contributions. It doesn’t even need to be code, you can also contribute by writing documentation, creating issues, or helping other contributors.\nYou can start by looking at the issues labeled hacktoberfest on our main GitHub repository but feel free to look at any other issue that you find interesting. If nothing feels like a good fit for you, you can always reach out to us in our Slack or Matrix channels, we are there to help!\nHacktoberfest 2023 in Brussels If you would like to meet us in person, we are hosting a Hacktoberfest event in Brussels on October 19th. We will give a short presentation about open-source and how to contribute to your favorite projects. After that, we will have a few hours to hack together and help you get started with your first contribution whether on Kairos or any other project of your preference. We will also have some food and drinks to keep us going.\nThe event will take place in the Ring Twice office in Brussels, Belgium. The address is Cantersteen 10, 1000 Bruxelles. We will start at 18:00 and will end at 22:00.\nSign up at the event page https://events.mlh.io/events/10408-hacktoberfest-2023-in-brussels Keep in mind that we only have limited seats so don’t leave it to the last minute!\nIf you have any questions, please email mauro@kairos.io.\n","categories":"","description":"","excerpt":"Hacktoberfest 2023 is here, and we are excited to announce that Kairos …","ref":"/blog/2023/09/26/hacktoberfest-2023/","tags":"","title":"Hacktoberfest 2023"},{"body":" We’re thrilled to announce the release of version 2.4.0 of Kairos which brings a wealth of exciting improvements and essential bug fixes that we can’t wait to share with you.\nEnhanced Release Artifacts Naming One of the significant changes in this release is the revamp of our release artifacts naming. We’ve worked hard to make them more consistent and easier to identify, streamlining your experience in finding the right elements for your project.\nRaspberry Pi Enhancements For those using Raspberry Pi devices, we’ve addressed several issues, including serial console problems and auto expansion of the last partition, among others. This ensures a smoother experience when using our project on Raspberry Pi devices, making it more versatile and reliable than ever before. We’ve also split the RPI images into rpi3 and rpi4 versions due to partitioning issues, with support for GPT being exclusive to rpi4.\nConfiguration Centralization We’ve taken a step toward simplifying your configuration process by consolidating all settings to the kairos-config (/etc/elemental/config.yaml is no longer used). This change streamlines your configuration management, making it more straightforward and efficient.\nMerged Release Pipelines In an effort to provide a more unified experience, we’ve merged the kairos-io/kairos and kairos-io/provider-kairos release pipelines. Going forward, kairos-io/kairos will serve as the canonical location for all artifacts, both “standard” and “core,” making it easier than ever to access and manage your resources.\nK3S Releases Alignment To align more closely with upstream releases, we have limited the number of k3s releases bundled with our project to match the three most recent minor release versions. This ensures that you have access to the latest features and improvements while maintaining stability.\nStreamlined Package Management Kairos overlay files now come directly from luet packages, eliminating the need for them to be included in the repository. Similarly, K3S packages are now sourced directly from luet packages, providing better control over versions and ensuring a more consistent upgrade cadence.\nKairos Agent Updates (v2.2.11) Our Kairos agent has received significant updates in this release, bringing a host of improvements and new features:\nSingle Source for Configuration: We’ve simplified configuration management by merging two different files into one, the cloud-config, for configuring install, upgrade, and reset behaviors. Check out the full reference in our documentation to migrate your existing configurations.\nCreate Dirs in Rootfs: For situations where you need to create directories in the rootfs, we’ve introduced a new option in the configuration to facilitate this process, enhancing flexibility.\nUpgrade Workflow from Passive: We’ve improved the upgrade workflow when booting from the passive image, ensuring that only the active image is updated, preventing issues with a broken system.\nAuto Partition and Image Size: Say goodbye to fixed partition and image sizes. The agent now dynamically calculates optimal sizes based on the source for installation, creating partitions accordingly.\nOverridable Partition and Image Size: If you prefer to customize partition and image sizes, you can now do so directly via cloud config, allowing you to fine-tune the sizes for each action (install, upgrade, reset).\nImproved Unattended Reset: Unattended reset now works seamlessly in any terminal or service, automatically rebooting when needed. It offers greater flexibility and reliability across various scenarios.\nrun-stage Command: We’ve fixed several underlying issues related to cloud provider metadata, user creation, and partition layout, ensuring a smoother operation.\nUpgrade Recovery: Recovery upgrades are now possible with the introduction of flags to facilitate the process. Normal upgrades also include a warning to remind users to upgrade recovery when necessary.\nFor a comprehensive view of all the changes in this release, please refer to the full changelog (And be sure to check out the “Known issues” section for any potential hiccups.)\nThis release marks a significant milestone in the evolution of our project, and we want to extend our heartfelt thanks to everyone who contributed to this release. Whether through code contributions, reviews, bug reports, comments, debugging output, or simply joining our meetings, your support and engagement are invaluable.\nStay awesome, and keep the momentum going!\n","categories":"","description":"Kairos release v2.4","excerpt":"Kairos release v2.4","ref":"/blog/2023/09/19/kairos-release-v2.4/","tags":"","title":"Kairos release v2.4"},{"body":" We’re excited to roll out Kairos v2.3, the latest iteration of our project, packed with new features and enhancements. The most notable change in this version is the support for Federal Information Processing Standards (FIPS).\nFIPS, a set of standards governing document processing, encryption algorithms, and other IT standards for non-military government agencies and contractors, is now an integral part of Kairos. The addition of a FIPS generic and Ubuntu-specific framework image marks a significant milestone in our journey to offer a secure and reliable Linux distribution. We made sure to included examples to facilitate building FIPS from scratch on Ubuntu, Fedora, and RockyLinux.\nThis release also includes a series of updates to the diverse Kairos flavors. The Ubuntu core images now stand at version 23.04, AlmaLinux has joined our flavor offerings, and openSUSE Leap has been upgraded to version 15.5.\nSeveral bugs have been squashed to ensure a seamless experience for our users. This release contains bugfixes and general enhancements to the Kairos fundaments to improve stability.\nKairos v2.3 is a testament to our commitment to delivering a more secure, stable, and versatile experience for all users. We eagerly await your feedback and suggestions. Your insights are invaluable in our quest to make Kairos the best it can be. So, dive into Kairos v2.3 and share your thoughts!\nIf you are curious on what’s next, check out our Roadmap and feel free to engage with our community!\nFor a full list of changes, see the Changelog. We hope you find these updates useful and as always, let us know if you have any questions or feedback. Thanks for using Kairos!\n","categories":"","description":"Unveiling Kairos v2.3 - FIPS Compliance and More!","excerpt":"Unveiling Kairos v2.3 - FIPS Compliance and More!","ref":"/blog/2023/07/10/unveiling-kairos-v2.3-fips-compliance-and-more/","tags":"","title":"Unveiling Kairos v2.3 - FIPS Compliance and More!"},{"body":" Kairos is a cloud-native meta-Linux distribution that brings the power of public cloud to your on-premises environment. With Kairos, you can build your own cloud with complete control and no vendor lock-in. It allows you to easily spin up a Kubernetes cluster with the Linux distribution of your choice, and manage the entire cluster lifecycle with Kubernetes.\nWhy you should try Kairos: Kairos provides a wide range of use cases, from Kubernetes applications to appliances and more. You can provision nodes with your own image or use Kairos releases for added flexibility. Kairos also simplifies day-2 operations like node upgrades. It provides the benefits of a unified, cloud-native approach to OS management.\nWhat you can do with Kairos: With Kairos, you can create an immutable infrastructure that stays consistent and free of drift with atomic upgrades. You can manage your cluster’s entire lifecycle with Kubernetes, from building to upgrading. Kairos also allows you to automatically create multi-node, single clusters that span across regions for maximum flexibility and scalability.\nKairos 2.2.0 release Kairos 2.2.0 has just been released, and we are thrilled to share the latest updates and improvements to the Kairos project. This release is a major release as reflect changes to internal core components and enhanced support for the ARM architecture.\nWhat changed? This release brings updates to the Kairos core components, including a substantial refactor of internal components and several bugfixes.\nIn order to enhance the user experience and reduce maintenance efforts the elemental cli and the kairos-agent functionalities were merged together. The kairos-agent now is responsible for the lifecycle of Kairos. This includes upgrades, setup, and delegating configuration to the specific providers that implement the specific configuration (k3s, for instance). The cloud-init stages can be run directly with the kairos-agent, and there is a --debug flag now to ease out troubleshooting. We also have renamed and moved user-facing commands of the Kairos CLI to kairosctl.\nBesides, the Kairos agent now is capable to support multi-arch images, as such is now possible to specify container images during upgrades for different platforms within the same tag.\nWe consolidated our support for the ARM architecture, most notably:\nGeneric iso ARM images, that can be used for development or in VMs Support for Nvidia AGX Orin! RPI images now are shipping with LVM, and as such the oem partition is available as in the x86_64 flavors. Note: Reset is not working, we are addressing this into a patch release. Add ubuntu to the available RPI images We are working also on a pure, Alpine flavor, which is now independent from systemd. Stay tuned!\nIf you are curious on what’s next, check out our Roadmap and feel free to engage with our community!\nFor a full list of changes, see the Changelog. We hope you find these updates useful and as always, let us know if you have any questions or feedback. Thanks for using Kairos!\n","categories":"","description":"Kairos 2.2 release: ARM enhancements","excerpt":"Kairos 2.2 release: ARM enhancements","ref":"/blog/2023/06/15/kairos-release-v2.2/","tags":"","title":"Kairos release v2.2"},{"body":"The Kairos team is thrilled to announce the release of the Secure Edge-Native Architecture (SENA) whitepaper! You can download it here\nWhat is SENA? SENA stands for “Secure Edge-Native Architecture.” It is a comprehensive solution architecture that outlines the tools and practices to address the modern requirements for deploying and managing Kubernetes-based edge applications at scale. SENA’s objective is to establish a new industry standard in the form of a well-defined framework that leverages best-in-class security and other concepts, design principles and tools, bringing together the most innovative hardware and software security capabilities.\nSENA covers considerations across the full lifecycle of edge hardware and software to enable teams to efficiently deploy, provision, operate and manage edge environments at scale.\nKairos and SENA Kairos is a core foundation of SENA, providing capabilities in combination with other components across the following areas:\nWhen deploying hardware edge devices Ease of deployment: Kairos enables zero-touch provisioning through our Kubernetes Native API and locally with AuroraBoot. Self-coordinated deployment: Enable self-coordinated, fully autonomous deployments with Integrated Kairos P2P support. Flexible deployments: Kairos can be fully customized to meet your Infrastructure needs. Extend Kairos images easily, or build your own using the Kairos framework, even at scale by leveraging the power of Kubernetes. When provisioning the complete edge stack Ensuring the provenance of the image attestation before deployments and during upgrades via the Kubernetes control plane with kyverno. Instructions can be found here. Ensuring provenance of the artifacts and comply with SLSA: Kairos releases SBOM artifacts, and builds on Github Actions, allowing you to identify and track components included in the released images with cosign. When operating the edge application Immutable, read-only OS stack: Kairos is a single container image, immutable system which is read-only and cannot be modified during runtime. Ensuring the privacy of user data at rest and in use. You can encrypt data at rest using the TPM chip and with the Kairos Key Management Server (KMS) ‘kcrypt’.The KMS also accepts only hardware devices with a TPM chip, ensuring onboarding of trusted devices. Providing the ability for applications to execute in a Trusted Execution Environment (TEE) leveraging Gramine. A TEE is an environment where hardware mechanisms are used to ensure the integrity and privacy of process execution, protecting against privileged (root) processes and physical snooping of electrical signals or devices in the system. You can already run workloads in a TEE with Kairos. For instructions check out Confidential computing What’s next Here are some of the items in our roadmap:\nStatic and Dynamic measured boot: We are planning to have UKI-flavored variants to boot the full OS in a single file. This will enable measurement, signing, and verification, simplifying maintenance and management, and leading to true immutability with a reduced attack surface. Ensuring the provenance and integrity of the OS during boot and runtime. We plan to integrate measured boot and SecureBoot on top of UKI images, integrating with Keylime, enabling remote attestation of system integrity after boot Ensuring the provenance and integrity of the application stack in runtime. Integration with GSC, MarbleRun - to seamlessly run confidential applications in your Kubernetes cluster and running attestation of confidential workloads. Management of hardware at scale: OpenAMT - Offering ways to automatically register Kairos boxes to an OpenAMT-supported management platform. You can already benefit from the SENA Architecture today with Kairos and you can follow our roadmap to see what’s coming up in the next releases here.\nStay tuned! More to come!\n","categories":"","description":"Learn about how Kairos is now part of SENA, the Secure Edge-Native Architecture announced by Spectro Cloud and developed in collaboration with Intel, enabling organizations to securely deploy, provision, operate and manage at scale edge locations. Discover the benefits of SENA and what's coming up in the future roadmap of Kairos' secure edge computing solutions.","excerpt":"Learn about how Kairos is now part of SENA, the Secure Edge-Native …","ref":"/blog/2023/04/18/kairos-is-now-part-of-the-secure-edge-native-architecture-by-spectro-cloud-and-intel/","tags":"","title":"Kairos is now part of the Secure Edge-Native Architecture by Spectro Cloud and Intel"},{"body":" Kairos is a cloud-native meta-Linux distribution that brings the power of public cloud to your on-premises environment. With Kairos, you can build your own cloud with complete control and no vendor lock-in. It allows you to easily spin up a Kubernetes cluster with the Linux distribution of your choice, and manage the entire cluster lifecycle with Kubernetes.\nWhy you should try Kairos: Kairos provides a wide range of use cases, from Kubernetes applications to appliances and more. You can provision nodes with your own image or use Kairos releases for added flexibility. Kairos also simplifies day-2 operations like node upgrades. It provides the benefits of a unified, cloud-native approach to OS management.\nWhat you can do with Kairos: With Kairos, you can create an immutable infrastructure that stays consistent and free of drift with atomic upgrades. You can manage your cluster’s entire lifecycle with Kubernetes, from building to upgrading. Kairos also allows you to automatically create multi-node, single clusters that span across regions for maximum flexibility and scalability.\nKairos 2.0.0 release Kairos 2.0.0 has just been released, and we are thrilled to share the latest updates and improvements to the Kairos project. This release is a major release as reflect changes to internal core components.\nWhat changed? We replaced the former dracut modules (a set of bash scripts/dracut/systemd services), which were responsible for the immutability management of Kairos, with https://github.com/kairos-io/immucore , a self-contained binary which doesn’t have dependencies and can run without dracut and systemd. While changes shouldn’t be impactful for most of our users, as changes impacted only in internal components, we suggest to try the upgrade in a lab environment before upgrading from earlier versions (v1.x).\nThe 2.0 release allows us to:\nnot depend anymore on systemd while set up immutability on boot ( allowing us to unblock several stories, for instance create Alpine images with vanilla kernels ) have hybrid images, that boots both UKI as a single file image, and as well as pivoting (as we are doing currently) pave the way for things like SecureBoot, Static Measured boot and much more debug things more cleanly, have a better testbed, and allow to ease out maintenance of the codebase be a step closer to our Confidential computing roadmap, indeed now you can try out running Confidential computing workload. Besides, we have now full SBOM list attached to images, as part of the release process, and in-toto attestation, allowing you to verify attestation also of SBOM lists, and have full audit of images. We also have integrated grype and trivy in our pipelines, and as such now releases contains also CVE reports, and finally we upload the generated reports as sarif file to GitHub to have notifications and see with more ease the impact of CVEs to the images. See also our documentation on how to gate upgrades and allow only verified images to be used during the process.\nThere were also fixes to the Debian flavor (thanks to the community for reporting issues!) and now manual upgrades with private registries are supported, too.\nFinally, it is also now possible to specify custom bind mounts path to overlay on top of the persistent partition, allowing to easily specify paths that you want to be persistent in the system via the cloud config file: https://kairos.io/docs/advanced/customizing/#customizing-the-file-system-hierarchy-using-custom-mounts .\nIf you are curious on what’s next, check out our Roadmap and feel free to engage with our community!\nFor a full list of changes, see the Changelog. We hope you find these updates useful and as always, let us know if you have any questions or feedback. Thanks for using Kairos!\n","categories":"","description":"Introducing Kairos 2.0: long live UKI!","excerpt":"Introducing Kairos 2.0: long live UKI!","ref":"/blog/2023/04/13/kairos-release-v2.0/","tags":"","title":"Kairos release v2.0"},{"body":"The problem You got yourself a Raspberry Pi (or more), and you want to put them to good use. You decide to make a Kubernetes cluster out of them, so that you can utilise the resources better, use familiar tools and implement infrastructure-as-code.\nUp to this point, kudos to you for demanding no less than a real cloud from your home infra.\nLike a smart person you are, you probably used Kairos to create your cluster and it’s now up and running. It’s now time to run some workloads.\nHere is my list if you need some ideas:\nA self-hosted Dropbox alternative (e.g. Seafile, NextCloud or other) Pihole An mqtt broker for your IoT projects Your own Gitea instance Your own ChatGPT alternative (e.g. using lama-cli or serge) None of these workloads is intended for public access. There are ways to expose the cluster to the world (e.g. like I described in another post) but it would be better if only devices within a VPN would have access to it.\nOnce again, there are many VPN solutions out there, but for this blog post, we’ll go with Wireguard.\nSo here is the problem in one sentence:\n“How do we expose our (possibly behind NAT) cluster, to machines inside the same Wireguard VPN?”\n“NAT” is the main part of the problem because otherwise this would simply be a blog post on how to create a Wireguard VPN. There are many nice tutorials already out there for that.\nA Solution While trying to solve the problem, I learned 2 things about Wireguard that I didn’t know:\nWireguard doesn’t distinguish between a “server” and a “client”. All peers are made equal. Wireguard doesn’t provide a solution for NAT traversal. How you access nodes behind NAT, is up to you. So imagine you have your cluster behind your home router (NAT) and your mobile phone on another network (behind NAT too) trying to access a service on the cluster. That’s not possible, unless there is some public IP address that somehow forwards requests to the cluster.\nAnd that’s the idea this solution is based on.\nHigh level view The idea is almost similar to the one I described in another post. The only difference is, that this time we expose the cluster only to machines inside the VPN.\nPrerequisites:\nA VM with a public IP address and SSH access (as small as it gets, it’s good enough) kubectl access to the cluster we want to expose (it doesn’t have to be Kairos, even k3d and kind will do) A machine to test the result (a smartphone where Wireguard can be installed is fine) Step by step From this point on, we will use the IP address 1.2.3.4 as the public IP address of the VM in the cloud. Replace it with the one matching your VM. We also assume, that the user with SSH access is root. Replace if necessary.\nSetup the cloud VM SSH to the machine:\n$ ssh root@1.2.3.4 Create Wireguard keys:\n$ wg genkey | tee privatekey | wg pubkey \u003e publickey Create Wireguard config:\n$ cat \u003c\u003c EOF \u003e /etc/wireguard/wg0.conf [Interface] Address = 192.168.6.1/24 PrivateKey = $(cat privatekey) ListenPort = 41194 # Mobile client [Peer] PublicKey = \u003cpublic key from next step\u003e AllowedIPs = 192.168.6.2/32 EOF Start and enable the Wireguard service:\n$ sudo systemctl enable --now wg-quick@wg0 Allow binding non-loopback interfaces when creating an SSH reverse tunnel by setting GatewayPorts clientspecified in /etc/ssh/sshd_config.\nSetup the test machine (mobile?) On some computer with wg installed, generate the keys:\n$ wg genkey | tee privatekey | wg pubkey \u003e publickey Create the Wireguard configuration. Follow the instructions for your favorite application. For Android, you can use this: https://play.google.com/store/apps/details?id=com.wireguard.android\nIf setting up a Linux machine, you can create the configuration like this:\n$ cat \u003c\u003c EOF \u003e /etc/wireguard/wg0.conf [Interface] Address = 192.168.6.2/24 PrivateKey = $(cat privatekey) # The cloud VM [Peer] PublicKey = \u003cpublic key from the previous step\u003e AllowedIPs = 192.168.6.1/32 Endpoint = 1.2.3.4:41194 EOF Start and enable the Wireguard service. If on a Linux machine, something like this will do:\n$ sudo systemctl enable --now wg-quick@wg0 On a mobile, follow the instructions of your application.\nAfter a while, your client should be able to ping the IP address of the VM: 192.168.6.1. You may find the output of wg show useful, while waiting for the peers to connect.\nSetup the cluster Deploy the helper Pod. We will use an image created with this Dockerfile and published here. The image’s entrypoint works with a config described here. The image is not multiarch, but there is one suitable for RasberryPi 4 (see the comment in the file).\nIf you are are going to create a fresh Kairos cluster, you can use a config like the following to automatically set up the helper Pod (make sure you replace the id_rsa and id_rsa.pub keys). If you prefer to not have the keys stored on your Kairos host filesystem, you can simply create the same resources using kubectl apply -f after your cluster is up an running.\n#cloud-config users: - name: kairos passwd: kairos groups: - admin stages: after-install-chroot: - files: - path: /var/lib/rancher/k3s/server/manifests/rproxy-pod.yaml content: | --- apiVersion: v1 data: id_rsa: the_vms_private_key_in_base64 id_rsa.pub: the_vms_public_key_in_base64 kind: Secret metadata: name: jumpbox-ssh-key type: Opaque --- apiVersion: v1 kind: ConfigMap metadata: name: proxy-config data: config.json: | { \"services\": [ { \"bindIP\": \"192.168.6.1\", \"bindPort\": \"443\", \"proxyAddress\": \"traefik.kube-system.svc\", \"proxyPort\": \"443\" }, { \"bindIP\": \"192.168.6.1\", \"bindPort\": \"80\", \"proxyAddress\": \"traefik.kube-system.svc\", \"proxyPort\": \"80\" } ], \"jumpbox\": { \"url\": \"1.2.3.4\", \"user\": \"root\", \"sshKeyFile\": \"/ssh/id_rsa\" } } --- apiVersion: apps/v1 kind: Deployment metadata: annotations: name: nginx-ssh-reverse-proxy spec: replicas: 1 selector: matchLabels: app.kubernetes.io/instance: nginx-ssh-reverse-proxy app.kubernetes.io/name: nginx-ssh-reverse-proxy template: metadata: labels: app.kubernetes.io/instance: nginx-ssh-reverse-proxy app.kubernetes.io/name: nginx-ssh-reverse-proxy spec: containers: - name: proxy # Change to quay.io/jimmykarily/nginx-ssh-reverse-proxy-arm64:latest # if you are running on a RasberryPi 4 image: quay.io/jimmykarily/nginx-ssh-reverse-proxy:latest command: [\"/start.sh\", \"/proxy-config/config.json\"] imagePullPolicy: Always volumeMounts: - name: ssh-key mountPath: /ssh - name: config-volume mountPath: /proxy-config/ volumes: - name: ssh-key secret: secretName: jumpbox-ssh-key defaultMode: 0400 - name: proxy-config - name: config-volume configMap: name: proxy-config In a nutshell, the config above is creating a reverse SSH tunnel from the VM to the Pod. Inside the Pod, nginx redirects traffic to the traefik load balancer running on the cluster. This has the effect, that any request landing on the VM on ports 80 and 443 will eventually reach the Traefik instance inside the cluster on ports 80 and 443. As a result, you can point any domain you want to the VM and it will reach the corresponding Ingress defined on your cluster.\nNOTE: The SSH tunnel will only bind the IP address 192.168.6.1 on the VM, which means, anyone trying to access the VM using its public IP address, will not be able to access the cluster. Only machines that can talk to 192.168.6.1 have access, in other words, machines inside the VPN. Test the connection Try to access the cluster with the VPN IP address (should work). From your test peer, open http://192.168.6.1. You should see a 404 message from Traefik. You can also verify it is a response from Traefik in your cluster, by calling curl on the https endpoint (on a “default” k3s installation):\n$ curl -k -v https://192.168.6.1 2\u003e\u00261 | grep TRAEFIK * subject: CN=TRAEFIK DEFAULT CERT * issuer: CN=TRAEFIK DEFAULT CERT Try to access the cluster with domain pointing to the VPN IP address (should work) You can create a wildcard DNS record and point it to the VPN IP address if you want to make it easier for people to access the services you are running. E.g. by creating an A record like this: *.mydomainhere.org -\u003e 192.168.6.1 you will be able create Ingresses for your applications like: app1.mydomainhere.org, app2.mydomainhere.org.\nTry to access the cluster using the public IP address (should not work)\n$ curl http://1.2.3.4 This command should fail to connect to your cluster\nConclusion For non-critical workloads, when 100% uptime is not a hard requirement, the solution we described allows one to use services that would otherwise cost multiple times more by hosting those on their own hardware. It does so, without exposing the home network to the public.\nIf you liked this solution or if you have comments, questions or recommendations for improvements, please reach out!\nUseful links Kairos documentation WireGuard documentation ","categories":"","description":"","excerpt":"The problem You got yourself a Raspberry Pi (or more), and you want to …","ref":"/blog/2023/03/29/access-your-home-lab-kairos-cluster-over-a-wireguard-vpn/","tags":"","title":"Access your home-lab Kairos cluster over a Wireguard VPN"},{"body":" Updates 2025-01-10: Added project status for each solution in the comparison table For years, the traditional Linux operating system has been a top pick for its flexibility and ability to be customized. But as great as it is, there are use cases in which stricter security rules and higher reliability standards are needed. That’s where immutable Linux operating systems come in - offering a more secure and reliable option, especially in settings where security is paramount.\nAn illustration of a fortress surrounded by a moat and guarded by armored knights, with a banner flying the Linux penguin logo, medieval, fortified, secure, trending on Artstation. Author: Midjourney AI In this post, we’ll be addressing some common questions to help you understand the principles behind immutable operating systems. We’ll also be exploring the various solutions available and the challenges faced in this field. So, get ready to dive in!\nWhat is an Immutable Linux OS? Explaining the concept of an immutable Linux OS to a newcomer can often turn into a detailed discussion on system internals. However, we’ll simplify it here as much as possible, even for those unfamiliar with the concepts.\nFormally defined, an immutable Linux OS (also known as Immutable Infrastructure or Immutable Deployment) is an operating system designed to be unchangeable and read-only. This means that once the operating system has been installed, the system files and directories cannot be modified. Any changes made to the system are temporary and lost when the system is rebooted. Think of it as a snapshot of a standard Linux system that cannot be changed. Any updates or changes are made by creating a new instance of the OS, deploying it, and switching over to the new instance. You can also find a very good writeup by Adrian Hornsby here.\nIf you’re already a Linux user, you’ll know that as root (Administrator), you can write anywhere in the filesystem, potentially corrupting the OS portion responsible for booting or management. In an immutable OS, however, any command that attempts to modify the system files will fail, as those files are only accessible for reading.\nImmutable systems are particularly useful in environments where security is a top priority, such as cloud computing, embedded systems, kiosks, and container execution. Essentially, any environment that needs to scale can benefit from the security and reliability of an immutable OS.\n“But what does that really mean? And what problem are Immutable systems trying to solve?” There are several advantages to using immutable Linux systems over traditional Linux systems. Firstly, there is an additional layer of security as it’s not possible to tamper with the runtime OS. Changes, if accepted, are discarded on the next reboot. This means that if a hacker or a malicious actor gains access to the system, they cannot make permanent changes that could compromise the system’s security.\nSecondly, maintenance of immutable systems is easier because they do not require regular updates or patches at the atomic package level. Instead, the entire OS is updated, similar to how updates are handled on Android phones.\nFinally, because the system is read-only, it is more reliable and less prone to failure. A declarative configuration model is usually tied to it, simplifying the configuration of the OS when orchestrated with other tools such as Ansible, Terraform, or similar.\n“Right, but how do I manage upgrades?” Instead of upgrading the system in place, upgrades are typically handled by creating a new, updated image of the operating system and replacing the existing image, in an atomic operation. This process is commonly referred to as “image-based upgrade”. The image can also be delivered to the end system, but this differs depending on the implementation, and there is no building on the node side.\nIn contrast, standard Linux systems typically use package managers such as apt or yum to upgrade software packages in place. This can be a more complex process because the package manager must ensure that all dependencies are satisfied and that there are no conflicts between different software packages. Additionally, upgrades in standard Linux systems can sometimes cause issues if there are conflicts between different versions of software packages or if the upgrade process is interrupted.\nChallenges at scale In standard Linux systems, the package manager has a lot of responsibilities and interacts directly with the system to apply changes. It can install packages, upgrade packages, merge configurations, and generate additional data required for the package. This makes installing software, upgrading, and running a system easy as a couple of interactions away with the package manager.\nWhen it comes to upgrading an installed system, the package manager should take care of many aspects, such as: correctly ordering dependencies (which may require a solver), verifying which packages are installed or not, which new packages will be installed, and handling file transmission securely. However, as the complexity of the stack grows, conflicts between packages can arise, and the package manager may prompt the user to solve them. This is not ideal for scaling out deployments, upgrades, and cutting operational costs since it exposes the infrastructure to drift.\n_Huh, didn't we get rid of package conflicts already? ([screenshot](https://www.reddit.com/r/openSUSE/comments/z4ld75/this_seems_to_be_common_in_opensuse_should_i_wait/))_ Tools like Ansible, Salt, Puppet, or Chef can manage and control standard systems upgrade mechanisms without requiring any interaction with each system during high-scale upgrades. In the standard model, clients handle certain portions of upgrades and installations, such as updating configuration files, or regenerating the initramfs. However, these actions could eventually raise the infrastructure drift level, causing a configuration merging to block everything or cause damage to your infrastructure and interrupt services. To avoid such issues, preparing fallback or switching services connections after an upgrade has been rolled out is one way to approach it.\nTransactional upgrades, are a step toward making standard mutable Linux systems, act more similarly to image-based upgrades in immutable Linux systems. In a transactional upgrade, the new software packages are prepared, usually into a separate partition, and applied after the first boot, similar to how an image-based upgrade works. However, unlike an immutable system, the existing system files can still be modified during the upgrade process.\nOn the other hand, immutable OSes simplify managing the OS stack by not exposing the node to complexities during upgrades or installation. The image is built ahead of time, using a well-tested, reproducible recipe that does not modify the system itself. The package manager is responsible for preparing a new, pristine environment that the real system will boot into afterward. For instance, immutable Linux OSes that use A/B partitioning create a new image of the operating system with the updated software packages or configuration changes. The new image is deployed to a transitive partition, which then becomes the new active partition. If the upgrade fails, the system can simply boot on the passive partition.\nImmutable OS: a look at the current landscape Here are some popular Immutable OS solutions, although this list is not exhaustive. There are much better and updated ones you can find on Github. Each of the solutions was created to tackle its own set of challenges, and they differ in their implementation details depending on their target environments.\nThe following are some of the most popular Immutable OS solutions:\nCoreOS: A Linux-based operating system designed for containers and cloud computing, which uses an immutable file system called “Container Linux”. CoreOS has now merged with Red Hat Enterprise Linux. Project Atomic: A CentOS-based Linux distribution, that focuses on container deployment and management, using a layered approach that allows for easy rollbacks. Ubuntu Core: Ubuntu Core is a version of the Ubuntu operating system designed and engineered for IoT and embedded systems. It uses snap packages exclusively to create a confined and transaction-based system. It also updates itself and its applications automatically. RancherOS: - A Linux-based operating system that is designed to be minimal, lightweight, and optimized for running containers. RancherOS uses Docker for all system processes, and its file system is mounted read-only, making it immutable. Talos: An open-source Linux distribution designed to run Kubernetes, K3s, or other container orchestration systems. It features a highly secure, API-managed infrastructure with automated and scalable operations and is suitable for cloud, containers, and general-purpose environments. K3OS: A minimal Linux distribution designed specifically for running Kubernetes clusters. k3os is built around k3s, a lightweight Kubernetes distribution, and uses the immutable Container Linux file system with an A/B update model to ensure smooth and reliable updates. It is suitable for cloud and container environments. Flatcar: A Linux-based operating system that is based on CoreOS and is designed for use in containerized environments. Like CoreOS, Flatcar Container Linux uses an immutable file system to provide stability and security. Fedora Silverblue: A Fedora-based Linux distribution, that uses an immutable file system and a transactional update model, to provide a stable and secure environment. Fedora Silverblue is designed for use in desktop and containerized environments. A nice overview can be found here or here Photon OS: A Linux-based operating system developed by VMware, which is designed to run containerized workloads. Photon OS uses a minimal package set and an immutable file system for enhanced security and manageability. openSUSE MicroOS: An openSUSE-based Linux distributions with transaction-update and is designed to host container workloads with automated administration and patching. Install MicroOS to get a quick, small environment for deploying Containers, or any other workload that benefits from Transactional Updates. As a rolling release distribution, the software is always up-to-date. Server and Desktop have different naming like openSUSE MicroOS Desktop GNOME is Aeon and openSUSE MicroOS Desktop Plasma is Kalpa. Information about MicroOS can be found here and the iso can be downloaded here To simplify the comparison between the different Immutable OS solutions, the following table highlights their key differences and the environments they are targeted for:\nSolution Based on Update Model Target Environment Project Status CoreOS Gentoo Transactional Updates Cloud Actively Maintained Talos Nothing Container image update Cloud, Containers, General purpose Actively Maintained K3OS Alpine A/B Cloud, Containers Discontinued Project Atomic CentOS Layered Packages Containers Discontinued Ubuntu Core Ubuntu Transactional Updates IoT, Embedded Systems Actively Maintained RancherOS Linux Docker for System Processes Containers Discontinued Flatcar CoreOS Transactional Updates Cloud Actively Maintained \u0026 a CNCF Project Red Hat Atomic Host Red Hat Transactional Updates Cloud, optimized for running containers Discontinued SLE Micro SUSE Transactional Updates Containers, Cloud, Edge, General purpose Actively Maintained MicroOS openSUSE Transactional Updates Desktop, Containers, Cloud, Edge, General purpose Actively Maintained Fedora Silverblue Fedora Transactional Updates Desktop, Containers Actively Maintained Photon OS Linux Immutable File System Cloud Actively Maintained Kairos Any Linux distribution Immutable File System Cloud, Edge, General purpose Actively Maintained \u0026 a CNCF Project “So, what’s Kairos? What’s the challenges that Kairos tries to overcome?” How Kairos fits in the ecosystem Kairos is a great fit when you want to deploy a Linux system on real hardware at the Edge1 or in a datacenter, whether it’s in your cloud on-premises or in the Edge. Specifically, if you’re looking for:\nZero-touch configuration and high-scalable deployments. See how to perform automated installs or how to create custom appliances A single distribution center of upgrades across your infrastructure using container registries. See docs Strong security posture, including online data encryption at-rest via TPM, Supply chain verification and Service bill of material Good hardware support Simplified Kubernetes deployment with self-coordinated K3s Flexibility in customization, including fine-grained control over the OS layer (packages installed, versions), and complete support maintenance level by building images from scratch Complete control over your infrastructure A community-driven, open roadmap, office hours, and the opportunity to get involved Maintenance - One thing you may have noticed when comparing Kairos to other alternatives, is that it doesn’t tie you to a specific OS. Instead, Kairos is flexible and portable, supporting all the popular Linux distributions, such as Ubuntu, Debian, and Fedora, among others. This, unties you from typical vendor lock-in strategies, forcing you to choose a specific distribution only for the immutability aspect.\nThe design shines also for its support for long-term maintenance. Each framework image released by Kairos allows the conversion of any OS to the given Kairos version, which could potentially enable maintenance for as long as the base OS support model allows. You can learn more about it here.\nContainer based - Kairos treats every operating system (OS) as a set of packages and represents the OS with a standard container image that can be executed with tools such as podman, docker, and so on. This container image includes all the necessary components for booting. Kairos components manage all node lifecycle operations, such as upgrading, installing, and resetting. These components are packaged within the framework images, which can be overlaid while creating a standard container image. Unlike traditional Linux distributions, the kairos-agent handles upgrades by pulling new container images as systems to boot, instead of relying on the OS package manager.\nAll installation and upgrades are delivered exclusively through container images, which are overlaid at boot time, eliminating the need for a container engine at runtime. The container image used for booting includes the kernel, initrd, and all other required pieces. This allows for customization directly within a Dockerfile. The container being booted is the image itself, and there is no actual container runtime running the image. The container is used to construct an image internally, which is then used to boot the system in an A/B fashion, without adding any overhead.\nThis approach offers several benefits, including the ability to verify the image with security scans and treat it similarly to a standard application that can be distributed via a container registry.\nSeparation of concerns - The separation of concerns between the OS and the management interface is clear in Kairos. The OS is responsible for providing the booting components and packages necessary for its operation, while Kairos provides the framework for managing the node’s lifecycle and immutability interface. The relationship between the image and Kairos is governed by a contract, which enables package handling without vendor lock-in.\nThis separation of concerns simplifies the delegation of package maintenance, CVE monitoring, and security fixes to the OS layer. Upgrades to container images can be achieved by chaining Dockerfiles or manually committing changes to the image.\nAutomatic deployments - To further automate custom deployment models, the Kairos Kubernetes Native Extensions can be used to create customized configurations either directly from Kubernetes or via the command line interface (CLI).\nSelf co-ordinated: Configuring multiple nodes at the Edge to form a single cluster can present challenges at various levels, from the network stack (such as assigning IPs to machines) to the configuration of the cluster topology (such as determining which machine will be the master). However, Kairos enables completely self-coordinated deployments, including for high availability (HA), eliminating the need for any configuration templating mechanism or specific role assignments for nodes.\nConclusion In conclusion, an immutable Linux OS, provides a more secure and reliable environment than a standard Linux system. However, it may not be suitable for all use cases, such as those that require frequent updates or modifications to the system. Upgrades in immutable systems are handled differently from standard Linux systems, using an image-based approach rather than package-based upgrades. While transactional upgrades in standard mutable Linux systems offer some benefits over traditional package-based upgrades, they still do not provide the same level of security and reliability as image-based upgrades in immutable Linux systems. Overall, the decision to use an immutable Linux system should be based on the specific requirements of the use case, and the benefits and limitations should be carefully considered, something that we can’t just let ChatGPT decide 😉\nImmutable Linux OSes offer a higher degree of reliability, security, and fault tolerance compared to traditional Linux systems. By using read-only file systems, separate update partitions and A/B partitioning, Immutable Linux OSes provide a safe, reliable way to update the system without downtime or the risk of breaking the system. Immutable Linux OSes are particularly well-suited for critical systems such as cloud container platforms, embedded systems, or IoT devices, where stability, security and scalability are of the utmost importance.\nFootnotes To put it simply, Kairos can be deployed on bare-metal hardware, and it provides robust support for hardware deployment.\n(Author note) As I dislike marketing buzzwords, I prefer to describe the Edge as the last-mile of computing. It involves a dedicated hardware that needs to be controlled by the Cloud in some way, such as a small server running Kubernetes, performing measurements and communicating with the Cloud. The term “Edge” is a broad, generic term that encompasses various computing scenarios, such as near-edge and far-edge computing, each with its own specialized deployment solution. ↩︎\n","categories":"","description":"In this post we are trying to answer some of the typical questions that help understanding Immutable OSes principles and we will dive a bit in what solutions are out there, and what are the challenges in the field","excerpt":"In this post we are trying to answer some of the typical questions …","ref":"/blog/2023/03/22/understanding-immutable-linux-os-benefits-architecture-and-challenges/","tags":"","title":"Understanding Immutable Linux OS: Benefits, Architecture, and Challenges"},{"body":"Hello Kairos community!\nWe are thrilled to announce that Kairos has been used as a key building block for a revolutionary Telco Radio Edge solution developed in collaboration with Spectro Cloud and Canonical. This cutting-edge solution showcases the latest advances in OpenRAN automation and distributed compute management, and took center stage at this year’s Mobile World Congress.\nThe Telco Radio Edge solution leverages the power of Kairos, Ubuntu Pro 22.04 LTS with RT kernel, and MicroK8s CAPI provider to deliver highly distributed edge node onboarding, secure deployment, and substrate provisioning. With this innovative technology stack, we’ve enabled OpenRAN o-DU service orchestration at scale, while optimizing performance, scalability, reliability, and security.\nThis is an exciting collaboration between the Kairos project, Spectro Cloud and Canonical to develop a solution that is highly performant, efficient, and scalable. The demos that have been presented at MWC showcase the advanced capabilities of the MicroK8s CAPI provider, and highlight the power of Kairos as a building block for distributed infrastructure substrates that can host even the most demanding modern OpenRAN o-DU, 5G UPF or AI/ML use-cases at scale.\nThis is a true testament to the power of open-source technologies and community collaboration, and we can’t wait to see what new possibilities this partnership will bring.\nThank you for your continued support and enthusiasm for Kairos!\nDetails: You can learn more about the Talco Radio Edge solution on the Ubuntu blog https://ubuntu.com/blog/meet-canonical-at-mwc-barcelona-2023 or at https://ubuntu.com/blog/canonical-at-mwc and watch it in action here: https://www.youtube.com/watch?v=wUCSK0O8Ro4\n","categories":"","description":"Kairos, the open-source distributed infrastructure platform, has collaborated with SpectroCloud and Canonical to develop a revolutionary Telco Radio Edge solution. The solution leverages the latest advances in OpenRAN automation and distributed compute management, and is set to take center stage at this year's Mobile World Congress.","excerpt":"Kairos, the open-source distributed infrastructure platform, has …","ref":"/blog/2023/03/13/kairos-spectrocloud-and-canonical-collaborate-to-deliver-revolutionary-telco-radio-edge-solution/","tags":"","title":"Kairos, SpectroCloud, and Canonical Collaborate to Deliver Revolutionary Telco Radio Edge Solution"},{"body":"We recently had the opportunity to sponsor two Kubernetes events, KCD Amsterdam and KCD Paris. This blog post, is a summary about my personal experience attending them.\nLet me start by saying, that I’m fairly new to Kubernetes and its community 👋. I know this project is big and that there are many companies building products and services around it, or have an interest in adopting it. So, I was very curious to see what kind of people I was going to meet and understand how Kairos could help them.\nMost attendees that approached us at the Kairos booths, were hearing about Kairos for the first time, and genuinely wanted to know what the project was about. I feel confident to say this, because we didn’t bring fancy prizes to give away and yet most of them would happily stay with us for 5, 10 and up to 15 minutes hearing about our features and engaging in conversation.\nIf you’re reading this and would like to know about those cool features I’d recommend going checking out the Getting Started, Web UI, P2P Network and AuroraBoot\nWhen you’re in the trenches building a product, talking to users or potential users is super valuable because it lets you see first hand, what kind of issues they are trying to solve. I don’t like building projects just because they are cool. To me, it’s important that they make people’s life easier. Some of the folks who reached to us, had clear problems in mind, and they didn’t shy to make hard questions about the internals of Kairos, our project’s governance and beyond. I’m very pleased to say that some of them left the booth with a smile on their face, because they might have found a good fit.\nWhile I didn’t get to attend any of the talks, I saw some really interesting topics, some of them from fantastic organizations like CERN! However, what I did do a bit, was to speak to some of the folks in the other booths, just to see what they were up to 🔍 and most importantly to see if there were chances our different projects could leverage each other out 🙌.\nLast but not least, let me thank everyone 🙇 who attended our booth for your valuable time and feedback. I think every one of my colleagues will agree that we’re committed to building a great product, that solves real world problems and we plan to use that feedback accordingly. We have a passion for open-source and we understand that this means much more than just great engineering and best practices. It also means being there for you, the community.\n","categories":"","description":"","excerpt":"We recently had the opportunity to sponsor two Kubernetes events, KCD …","ref":"/blog/2023/03/09/kairos-at-the-kcd-amsterdam-and-paris-2023/","tags":"","title":"Kairos at the KCD Amsterdam and Paris 2023"},{"body":" Kairos is a cloud-native meta-Linux distribution that brings the power of public cloud to your on-premises environment. With Kairos, you can build your own cloud with complete control and no vendor lock-in. It allows you to easily spin up a Kubernetes cluster with the Linux distribution of your choice, and manage the entire cluster lifecycle with Kubernetes.\nWhy you should try Kairos: Kairos provides a wide range of use cases, from Kubernetes applications to appliances and more. You can provision nodes with your own image or use Kairos releases for added flexibility. Kairos also simplifies day-2 operations like node upgrades. It provides the benefits of a unified, cloud-native approach to OS management.\nWhat you can do with Kairos: With Kairos, you can create an immutable infrastructure that stays consistent and free of drift with atomic upgrades. You can manage your cluster’s entire lifecycle with Kubernetes, from building to upgrading. Kairos also allows you to automatically create multi-node, single clusters that span across regions for maximum flexibility and scalability.\nKairos 1.6.0 release Kairos 1.6.0 has just been released, and we are thrilled to share the latest updates and improvements to the Kairos project. This release includes bug fixes, small improvements to the Kairos core codebase, and the introduction of AuroraBoot, a tool that simplifies bootstrapping of Kairos nodes. In this post, we will explore how AuroraBoot works and its benefits for users deploying Kairos.\nWhat is AuroraBoot? AuroraBoot is a tool designed to make the process of bootstrapping Kairos machines quick, simple, and efficient. It is specifically designed for the Kairos operating system and provides a comprehensive solution for downloading required artifacts and provisioning a machine, both from network or manually via flashing to USB stick.\nAuroraBoot simplifies the bootstrapping process by automating several steps, such as downloading required files, verifying their authenticity, and providing a streamlined interface for customizing the installation media. With AuroraBoot, users can prepare the environment for network-based bootstrapping, download the necessary release assets, and also customize the installation media for USB-based mass-installations.\nThe Benefits of AuroraBoot With AuroraBoot, users can prepare multiple nodes in a lab before shipment or deploy Kairos nodes in a network segment where workload can already be sent to (running AuroraBoot in an already-existing downstream cluster). Additionally, AuroraBoot offers a simple, intuitive, and streamlined way to deploy Kairos automatically and manually. It makes the deployment process faster, more efficient, and less error-prone. Besides, it does leverage the DHCP server already existing in the network for booting, requiring zero-configuration.\nYou can see AuroraBoot in action here, with a full e2e example on how to use it with p2p in Kairos, and in the video below:\nImprovements to the WebUI for a simplified user experience The WebUI got several improvements, we have integrated the documentation inside the web interface, and now can be accessed also offline. The configuration schema is validated and a message is displayed if the configuration is incorrect. You can see how it works here\nScreencast from 2023-02-21 15-24-59.webm\nOther Improvements in Kairos 1.6.0 Aside from AuroraBoot, Kairos 1.6.0 includes several improvements and bugfixes, including:\nIntegration of documentation into the Web UI Initial support for schema validation in the WebUI and the installer Support for Rocky Linux in provider builds Renaming of kairos-agent and addition of SHA256 signatures Addition of custom mounts Fix for DHCP hostname issues Fix for encryption reset failures Fix for systemd-networkd hostname settings Fix for Tumbleweed ISO You can check the full changelog at: https://github.com/kairos-io/kairos/releases/tag/v1.6.0\nConclusion Kairos 1.6.0 is a significant step forward in simplifying the deployment process of Kairos nodes. With AuroraBoot, users can deploy Kairos faster, more efficiently, and with less risk of error. Additionally, the bug fixes and improvements in this release demonstrate Kairos’ commitment to providing a reliable and robust operating system for users. We invite you to download and try Kairos 1.6.0 and experience the benefits of AuroraBoot for yourself.\nFor a full list of changes, see the Changelog. We hope you find these updates useful and as always, let us know if you have any questions or feedback. Thanks for using Kairos!\n","categories":"","description":"Introducing Kairos 1.6: Get ready to boot with AuroraBoot!","excerpt":"Introducing Kairos 1.6: Get ready to boot with AuroraBoot!","ref":"/blog/2023/02/26/kairos-release-v1.6/","tags":"","title":"Kairos release v1.6"},{"body":"I recently had the opportunity to attend FOSDEM 2023 and share a bit about the Kairos project. In this post I want to summarize what I presented and share other interesting presentations I attended, which I believe are relevant for Kairos and our community.\nHow we build and maintain Kairos I had the opportunity to share about How we build and maintain Kairos. In first half of the presentation, I introduce the different elements that make Kairos a great OS for Edge Kubernetes. During the second half of the presentation you will get an overview of how the Kairos Factory works, starting from those different Linux distributions all the way up to producing Kairos core and standard images. Because my presentation took place in the Distributions Devroom, I put some extra emphasis on the challenges we have to be distribution agnostic.\nThe talk is intended to newcomers, so I made an effort to describe things in a simple and welcoming language. However, I think it can also be interesting for those who might already know about Kairos but wonder how to extend the core and standard images, or simply have a better understanding of how all the pieces interconnect.\nLike I mentioned, the presentation took place in the Distributions Devroom and we’re very thankful to them for hosting us. While it was a great experience and the talk seemed to have a good reception, I now realize that the topic is probably more relevant for a different devroom, for example, the Image-based Linux and Secure Measured Boot devroom , which I’ll make sure to send proposals next year.\nOther talks which are relevant to Kairos There were other interesting presentations I had the opportunity to attend, which I think are also relevant to Kairos and our community. These would be my top picks:\nIf you’re completely new to the concepts of Image-Based Linux, Unified Kernel Image or Discoverable Disk Image, I’d recommend checking Luca Bocassi’s talk Introducing and decoding image-based Linux terminology and concepts. As someone who very recently joined the Kairos project, I still get a bit lost with all the different technologies used in Image-Based Linux. The presenter made a good job clarifying some of these technologies and how they work together.\nOne of the key presentations in my opinion was Lennart Poettering’s, Measured Boot, Protecting Secrets and you, where he talks about Trusted Plataform Modules and upcoming functionality in systemd. I’m pretty sure there will be some of these features which will be relevant for Kairos sooner rather than later.\nLast but not least, there was an interesting talk by Gabriel Kerneis about User-friendly Lightweight TPM Remote Attestation over Bluetooth. My guess is that we will continue seeing different methods to do and simplify attestation and because one of our goals at the Kairos project is to be as friendly as we can to our user base, then I can only imagine we will end up introducing some sort of remote attestation technologies like Ultrablue in the future.\nConclusion FOSDEM is a very important conference when it comes to free and open source software and I’m very happy that Kairos was present. First of all because I think the work we’re doing with Kairos is helping solve some of the most challenging issues of running cloud native applications on the edge, but also because as an open source project, it was nice to introduce ourselves to the community there and start a conversation. Expect us to keep engaging with you in further editions of FOSDEM and other conferences!\n","categories":"","description":"","excerpt":"I recently had the opportunity to attend FOSDEM 2023 and share a bit …","ref":"/blog/2023/02/07/kairos-at-fosdem-2023/","tags":"","title":"Kairos at FOSDEM 2023"},{"body":" We are thrilled to announce the release of Kairos version 1.5, a major update that brings significant improvements to user experience and security. With this release, we have made it even easier for you to install and set up Kairos, as well as better protect your user data. Our community has been an invaluable source of feedback, bug reports, and contributions, and we are grateful for their support.\nYou can find Kairos core images at https://github.com/kairos-io/kairos/releases/tag/v1.5.0 and images with k3s pre-bundled here: https://github.com/kairos-io/provider-kairos/releases/tag/v1.5.1.\nEffortless Installation with the WebUI Installer Gone are the days of complicated command-line instructions. With the new WebUI installer, installation and setup are a breeze. Simply follow the steps on the web page, and you’ll be up and running in no time. You can also use our core images as an installer. Take a look at this gif to see the WebUI installer in action:\nProtect Your Data with User Data Encryption at the Edge Kairos 1.5 now allows you to encrypt your user data with ease, keeping it secure from prying eyes. Encryption is done via TPM and optionally with the Kairos KMS (Key Management Server) for external authentication and management of encrypted secrets. Check out our documentation for more information on partition encryption.\nOS updates We’ve added RockyLinux and Debian to our list of supported releases, giving you more options to run Kairos on both stable and feature-rich operating systems. We’ve also updated our Alpine support, so you can now run Kairos on the latest version of Alpine Linux.\nExtend Kairos with Custom Deployment Models (bundles) Kairos 1.5 allows you to extend the configuration of your node with custom, container-based deployment models defined as bundles. Check out our documentation and examples to see how to deploy MetaLB. Kubevirt and MetalLB bundles are also availble in the community-bundles repository.\nFor a full list of changes, see the Changelog. We hope you find these updates useful and as always, let us know if you have any questions or feedback. Thanks for using Kairos!\n","categories":"","description":"Introducing Kairos 1.5: A Smarter, More Secure Way to Manage Your Infrastructure","excerpt":"Introducing Kairos 1.5: A Smarter, More Secure Way to Manage Your …","ref":"/blog/2023/01/27/kairos-release-v1.5/","tags":"","title":"Kairos release v1.5"},{"body":"We’ve added a new media section so it’s easy to find the different videos and articles about Kairos. To access it, go to the Documentation and at the bottom of the left menu, you will find a link called Media.\nYou can also click here to go check it out.\n","categories":"","description":"","excerpt":"We’ve added a new media section so it’s easy to find the different …","ref":"/blog/2023/01/04/media-section/","tags":"","title":"Media Section"},{"body":"","categories":"","description":"","excerpt":"","ref":"/index.json","tags":"","title":""},{"body":"Kairos is all about giving you the power to customize your operating system just the way you need it—declaratively, reproducibly, and predictably. Today, we’re walking through how to build and boot a Kairos image using the provider-kubeadm to set up a Kubernetes cluster with kubeadm.\nThis guide is focused on a simple use case: booting a single-node Kubernetes cluster with role init, version v1.30.0.\n🧱 What Is provider-kubeadm? The provider-kubeadm is a binary for Kairos that integrates with Kubernetes’ kubeadm bootstrap process. It translates the familiar kubeadm configuration into a Kairos-compatible cloud-init YAML, wrapping everything in a reproducible and declarative boot process.\nWith this provider, we can fully define Kubernetes cluster parameters—including API server args, scheduler, networking, and etcd configuration—right inside a Kairos #cloud-config block.\n🔧 Building the Image We start with a Kairos base image—here, Ubuntu 24.04 Core—and layer on everything kubeadm needs: containerd, kubelet, kubectl, and the kubeadm binary. Also we will download and set the agent-provide-kubeadm to handle the kubeadm configuration.\nHere’s the Dockerfile used to construct the image:\nFROM quay.io/kairos/ubuntu:24.04-core-amd64-generic-v3.4.2 # Add Kubernetes apt repository RUN curl -fsSL https://pkgs.k8s.io/core:/stable:/v1.31/deb/Release.key | gpg --dearmor -o /etc/apt/keyrings/kubernetes-apt-keyring.gpg RUN echo 'deb [signed-by=/etc/apt/keyrings/kubernetes-apt-keyring.gpg] https://pkgs.k8s.io/core:/stable:/v1.31/deb/ /' | tee /etc/apt/sources.list.d/kubernetes.list # Install required packages RUN apt-get update \u0026\u0026 apt-get install -y --no-install-recommends \\ socat \\ conntrack \\ containerd \\ runc \\ kubelet \\ kubeadm \\ kubectl \\ \u0026\u0026 apt-get clean \u0026\u0026 rm -rf /var/lib/apt/lists/* # Copy the provider into place RUN mkdir -p /system/providers \u0026\u0026 curl -L https://github.com/kairos-io/provider-kubeadm/releases/download/v4.7.0-rc.4/agent-provider-kubeadm-v4.7.0-rc.4-linux-amd64.tar.gz | tar -xz -C /system/providers/ Or with the modern Kairos Factory method:\nFROM quay.io/kairos/kairos-init:v0.5.20 AS kairos-init FROM ubuntu:24.04 ARG VERSION=1.0.0 RUN --mount=type=bind,from=kairos-init,src=/kairos-init,dst=/kairos-init /kairos-init --version \"${VERSION}\" RUN curl -fsSL https://pkgs.k8s.io/core:/stable:/v1.31/deb/Release.key | gpg --dearmor -o /etc/apt/keyrings/kubernetes-apt-keyring.gpg RUN echo 'deb [signed-by=/etc/apt/keyrings/kubernetes-apt-keyring.gpg] https://pkgs.k8s.io/core:/stable:/v1.31/deb/ /' | tee /etc/apt/sources.list.d/kubernetes.list # Install required packages RUN apt-get update \u0026\u0026 apt-get install -y --no-install-recommends \\ socat \\ conntrack \\ containerd \\ runc \\ kubelet \\ kubeadm \\ kubectl \\ \u0026\u0026 apt-get clean \u0026\u0026 rm -rf /var/lib/apt/lists/* RUN mkdir -p /system/providers \u0026\u0026 curl -L https://github.com/kairos-io/provider-kubeadm/releases/download/v4.7.0-rc.4/agent-provider-kubeadm-v4.7.0-rc.4-linux-amd64.tar.gz | tar -xz -C /system/providers/ ☁️ The Cloud-Config Here’s the heart of the system—a Kairos-compatible #cloud-config YAML. This config installs the OS, sets up kernel and containerd parameters, and passes the full kubeadm configuration block to the provider-kubeadm.\n#cloud-config install: device: auto auto: true reboot: true cluster: cluster_token: \"random_token\" control_plane_host: \"1.1.1.1\" role: init config: | clusterConfiguration: apiServer: extraArgs: advertise-address: 0.0.0.0 anonymous-auth: \"true\" audit-log-maxage: \"30\" audit-log-maxbackup: \"10\" audit-log-maxsize: \"100\" audit-log-path: /var/log/apiserver/audit.log authorization-mode: RBAC,Node default-not-ready-toleration-seconds: \"60\" default-unreachable-toleration-seconds: \"60\" disable-admission-plugins: AlwaysAdmit enable-admission-plugins: AlwaysPullImages,NamespaceLifecycle,ServiceAccount,NodeRestriction profiling: \"false\" secure-port: \"6443\" tls-cipher-suites: TLS_ECDHE_ECDSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256,... extraVolumes: - hostPath: /var/log/apiserver mountPath: /var/log/apiserver name: audit-log pathType: DirectoryOrCreate timeoutForControlPlane: 10m0s controllerManager: extraArgs: feature-gates: RotateKubeletServerCertificate=true profiling: \"false\" terminated-pod-gc-threshold: \"25\" use-service-account-credentials: \"true\" etcd: local: dataDir: /etc/kubernetes/etcd extraArgs: listen-client-urls: https://0.0.0.0:2379 kubernetesVersion: v1.30.0 networking: podSubnet: 192.168.0.0/16 serviceSubnet: 192.169.0.0/16 initConfiguration: nodeRegistration: kubeletExtraArgs: event-qps: \"0\" feature-gates: RotateKubeletServerCertificate=true protect-kernel-defaults: \"true\" read-only-port: \"0\" joinConfiguration: nodeRegistration: kubeletExtraArgs: event-qps: \"0\" feature-gates: RotateKubeletServerCertificate=true protect-kernel-defaults: \"true\" read-only-port: \"0\" kubeletConfiguration: authentication: anonymous: {} webhook: { cacheTTL: 0s } x509: {} authorization: webhook: cacheAuthorizedTTL: 0s cacheUnauthorizedTTL: 0s cpuManagerReconcilePeriod: 0s logging: flushFrequency: 0 options: json: infoBufferSize: \"0\" verbosity: 0 stages: initramfs: - name: pre-kubeadm sysctl: net.ipv4.conf.default.rp_filter: 0 net.ipv4.conf.all.rp_filter: 0 net.bridge.bridge-nf-call-ip6tables: 1 net.bridge.bridge-nf-call-iptables: 1 net.ipv4.ip_forward: 1 kernel.panic: \"10\" kernel.panic_on_oops: \"1\" vm.overcommit_memory: \"1\" modules: - br_netfilter - overlay files: - path: /etc/containerd/config.toml permissions: \"0644\" content: | [plugins.\"io.containerd.grpc.v1.cri\".containerd.runtimes.runc.options] SystemdCgroup = true [plugins.\"io.containerd.grpc.v1.cri\"] sandbox_image = \"registry.k8s.io/pause:3.8\" - path: /etc/hosts permissions: \"0644\" content: | 127.0.0.1 localhost users: kairos: passwd: kairos groups: - sudo commands: - ln -s /etc/kubernetes/admin.conf /run/kubeconfig - mkdir -p /etc/kubernetes/manifests cluster.role is set to init, so this node bootstraps the control plane. For a multi-node setup, change this to join and provide discovery options. 🔄 What’s Next? This is a minimal setup, but it lays the groundwork for more advanced clusters. From here, you can:\nCustomize the CNI (via the containerd config or an additional stage), Inject manifests into /etc/kubernetes/manifests, Scale to multiple nodes with join configurations and token-based discovery. And, of course, all of this benefits from the immutability and reproducibility that Kairos brings to the OS layer.\nIf you want to see more examples or contribute to the provider-kubeadm, check out the GitHub repo or hop into our community channels.\nLet us know how you’re bootstrapping Kubernetes with Kairos—we’d love to feature your use case!\n","categories":"","description":"Learn how to build a Kairos image for a single-node Kubernetes cluster using the provider-kubeadm.","excerpt":"Learn how to build a Kairos image for a single-node Kubernetes cluster …","ref":"/docs/examples/kubeadm-provider/","tags":"","title":"A Minimal Single-Node Kubernetes with Kubeadm"},{"body":"Drop a file under /oem/91_paths.yaml with the following content:\nstages: rootfs: - name: \"Custom mounts\" environment_file: /run/cos/cos-layout.env environment: CUSTOM_BIND_MOUNTS: \"/var/lib/path1 /var/lib/path2 /var/lib/path3\" This will indicate to Kairos to bind mount the paths /var/lib/path1, /var/lib/path2, and /var/lib/path3 to the persistent partition under /usr/local/.state after the next reboot.\nNote The example mentions /oem/91_paths.yaml, but you can use any file name as long as it is under /oem/ and has a .yaml extension. The file will be processed by Kairos during the next boot. ","categories":"","description":"This section describes examples on how to add persistent paths after install","excerpt":"This section describes examples on how to add persistent paths after …","ref":"/docs/examples/extra_persistent_paths_after_install/","tags":"","title":"Adding persistent paths after install"},{"body":"If you want to create an airgap K3s installation, Kairos provides a convenient way to do so using AuroraBoot. In this guide, we will go through the process of creating a custom ISO of Kairos that contains a configuration file and a bundle that executes preparatory steps after installation. The bundle will overlay new files in the system and prepare the node for having an airgapped K3s installation.\nNote If you already have a Kubernetes cluster, you can use the osbuilder controller to generate container images with your additional files already inside. Prerequisites Docker running in the host\nCreating the Bundle First, we need to create a bundle that contains the K3s images used for the airgap installation. The bundle will place the images in the /var/lib/rancher/k3s/agent/images directory. The /var/lib/rancher is already configured as persistent by Kairos defaults and every change to that directory persist reboots. You can add additional persistent paths in the system with the cloud config\nCreate a new directory named images-bundle, and create a new file inside it called Dockerfile. Paste the following code into the Dockerfile: FROM alpine AS alpine WORKDIR /build RUN wget https://github.com/k3s-io/k3s/releases/download/v1.23.16%2Bk3s1/k3s-airgap-images-amd64.tar.gz FROM scratch COPY ./run.sh / COPY --from=alpine /build/k3s-airgap-images-amd64.tar.gz /assets/ Create a new file called run.sh inside the images-bundle directory, and paste the following code: #!/bin/bash mkdir -p /usr/local/.state/var-lib-rancher.bind/k3s/agent/images/ cp -rfv assets/k3s-airgap-images-amd64.tar.gz /usr/local/.state/var-lib-rancher.bind/k3s/agent/images/ Make the run.sh file executable by running the following command: chmod +x run.sh Build the container image by running the following command inside the images-bundle directory. This will save the image as data/bundle.tar: docker build -t images-bundle . Save the bundle: $ ls images-bundle # create a directory $ mkdir data $ docker save images-bundle -o data/bundle.tar Building the Offline ISO for Airgap Now that we have created the bundle, we can use it to build an offline ISO for the airgap installation.\nCreate a cloud config for the ISO and save it as config.yaml. The config.yaml file should contain your cloud configuration for Kairos and is used to set up the system when it is installed. An example can be: #cloud-config install: auto: true device: \"auto\" reboot: true bundles: # This bundle needs to run after-install as it consumes assets from the LiveCD # which is not accessible otherwise at the first boot (there is no live-cd with any bundle.tar) - targets: - run:///run/initramfs/live/bundle.tar local_file: true fail_on_bundles_errors: true # Define the user accounts on the node. users: - name: \"kairos\" # The username for the user. passwd: \"kairos\" # The password for the user. ssh_authorized_keys: # A list of SSH keys to add to the user's authorized keys. - github:mudler # A key from the user's GitHub account. k3s: enabled: true Build the ISO with AuroraBoot by running the following command: IMAGE=quay.io/kairos/@flavor:@flavorRelease-standard-amd64-generic-master-k3sv1.33.4-k3s1 docker pull $IMAGE docker run -v $PWD/config.yaml:/config.yaml \\ -v $PWD/build:/tmp/auroraboot \\ -v /var/run/docker.sock:/var/run/docker.sock \\ -v $PWD/data:/tmp/data \\ --rm -ti quay.io/kairos/auroraboot:v0.13.0 \\ --set \"disable_http_server=true\" \\ --set \"disable_netboot=true\" \\ --set \"container_image=docker://$IMAGE\" \\ --set \"iso.data=/tmp/data\" \\ --cloud-config /config.yaml \\ --set \"state_dir=/tmp/auroraboot\" The resulting ISO should be available at: build/iso/kairos.iso\nThis example is also available in the AuroraBoot repository in the examples/airgap directory, where you can run build_docker.sh to reproduce the example.\nSee also Customize the OS image Create ISOs with Kubernetes Bundles reference System extensions ","categories":"","description":"This section describe examples on how to use AuroraBoot and Kairos bundles to create ISOs for airgapped installs","excerpt":"This section describe examples on how to use AuroraBoot and Kairos …","ref":"/docs/examples/airgap/","tags":"","title":"How to Create an Airgap K3s Installation with Kairos"},{"body":"This page demonstrates the styling and colors of alerts in the Kairos Docs.\nThe goal will be to standardize alerts between notes, info, etc. across different pages.\nAlerts Note Lorem ipsum dolor sit amet, consectetur adipiscing elit, sed do eiusmod tempor incididunt ut labore et dolore magna aliqua. Ut enim ad minim veniam, quis nostrud exercitation ullamco laboris nisi ut aliquip ex ea commodo consequat. Duis aute irure dolor in reprehenderit in voluptate velit esse cillum dolore eu fugiat nulla pariatur. Excepteur sint occaecat cupidatat non proident, sunt in culpa qui officia deserunt mollit anim id est laborum. Info Lorem ipsum dolor sit amet, consectetur adipiscing elit, sed do eiusmod tempor incididunt ut labore et dolore magna aliqua. Ut enim ad minim veniam, quis nostrud exercitation ullamco laboris nisi ut aliquip ex ea commodo consequat. Duis aute irure dolor in reprehenderit in voluptate velit esse cillum dolore eu fugiat nulla pariatur. Excepteur sint occaecat cupidatat non proident, sunt in culpa qui officia deserunt mollit anim id est laborum. Warning Lorem ipsum dolor sit amet, consectetur adipiscing elit, sed do eiusmod tempor incididunt ut labore et dolore magna aliqua. Ut enim ad minim veniam, quis nostrud exercitation ullamco laboris nisi ut aliquip ex ea commodo consequat. Duis aute irure dolor in reprehenderit in voluptate velit esse cillum dolore eu fugiat nulla pariatur. Excepteur sint occaecat cupidatat non proident, sunt in culpa qui officia deserunt mollit anim id est laborum. Secondary Lorem ipsum dolor sit amet, consectetur adipiscing elit, sed do eiusmod tempor incididunt ut labore et dolore magna aliqua. Ut enim ad minim veniam, quis nostrud exercitation ullamco laboris nisi ut aliquip ex ea commodo consequat. Duis aute irure dolor in reprehenderit in voluptate velit esse cillum dolore eu fugiat nulla pariatur. Excepteur sint occaecat cupidatat non proident, sunt in culpa qui officia deserunt mollit anim id est laborum. ","categories":"","description":"","excerpt":"This page demonstrates the styling and colors of alerts in the Kairos …","ref":"/docs/reference/alerts/","tags":"","title":"Alerts Styling"},{"body":" Info This tutorial demonstrates how to optimize bandwidth usage during OS upgrades using distributed caching solutions like embedded registries. Introduction Nodes in edge clusters often have poor networking capabilities, and Kairos users may create custom images that are significantly larger (e.g., by including many kernel drivers). The current issue is that during upgrades, each node in the cluster must re-download the entire image from scratch before applying the upgrade.\nThis documentation explores solutions to optimize bandwidth usage during upgrades by implementing distributed caching mechanisms.\nProblem Statement Large Images: Custom images can be very large due to additional drivers, tools, or configurations Poor Network: Edge nodes often have limited or unreliable network connectivity Redundant Downloads: Each node downloads the same upgrade image independently Bandwidth Waste: Multiple nodes downloading identical images consumes unnecessary bandwidth Solutions Note This documentation covers bandwidth optimization solutions for Kairos images that include a Kubernetes distribution (K3s, K0s, or kubeadm via provider-kubeadm). K3s with Embedded Registry (Spegel) K3s integrates with Spegel, a distributed registry that enables efficient image caching across your cluster. This integration allows nodes to share container images locally, reducing bandwidth usage and improving deployment speed.\nInfo For detailed information about k3s embedded registry configuration, see the official k3s documentation. Manual Setup Master Node Configuration #cloud-config hostname: metal-{{ trunc 4 .MachineID }} users: - name: kairos # Change to your own user passwd: kairos # Change to your own password groups: - admin # This user needs to be part of the admin group install: reboot: true k3s: enabled: true embedded_registry: true stages: boot: - name: \"Add registries configuration for k3s/spegel\" files: - path: /etc/rancher/k3s/registries.yaml content: | mirrors: \"*\": Worker Node Configuration #cloud-config hostname: metal-{{ trunc 4 .MachineID }} users: - name: kairos # Change to your own user passwd: kairos # Change to your own password groups: - admin # This user needs to be part of the admin group k3s-agent: # Warning: the key is different from the master node one enabled: true args: - --with-node-id # will configure the agent to use the node ID to communicate with the master node env: K3S_TOKEN: \"YOUR_K3S_TOKEN_HERE\" # Replace with the actual token from your master node K3S_URL: https://YOUR_MASTER_NODE_IP:6443 # Replace with your master node's IP address stages: boot: - name: \"Add registries configuration for k3s/spegel\" files: - path: /etc/rancher/k3s/registries.yaml content: | mirrors: \"*\": Important Notes:\nReplace YOUR_K3S_TOKEN_HERE with the actual token from your master node Replace YOUR_MASTER_NODE_IP with your master node’s IP address The embedded_registry: true setting enables Spegel integration Auto Configuration For automated cluster setup with P2P coordination:\n#cloud-config hostname: metal-{{ trunc 4 .MachineID }} users: - name: kairos # Change to your own user passwd: kairos # Change to your own password groups: - admin # This user needs to be part of the admin group k3s: embedded_registry: true p2p: # Disabling DHT makes co-ordination to discover nodes only in the local network disable_dht: true #Enabled by default # network_token is the shared secret used by the nodes to co-ordinate with p2p. # Setting a network token implies auto.enable = true. # To disable, just set auto.enable = false network_token: \"YOUR_P2P_NETWORK_TOKEN_HERE\" # Replace with your P2P network token stages: boot: - name: \"Add registries configuration for k3s/spegel\" files: - path: /etc/rancher/k3s/registries.yaml content: | mirrors: \"*\": Upgrade Process with Distributed Caching When performing upgrades, the embedded registry provides significant benefits:\nMaster Node First: The upgrade starts on the master node, which pulls the upgrade image Distributed Caching: The image is cached in the embedded registry Worker Nodes: When worker nodes start their upgrade, they fetch the image from the local registry instead of pulling from remote Upgrade Configuration apiVersion: operator.kairos.io/v1alpha1 kind: NodeOpUpgrade metadata: name: kairos-upgrade namespace: default spec: # The container image containing the new Kairos version image: quay.io/kairos/opensuse:tumbleweed-latest-standard-amd64-generic-v3.5.0-k3s-v1.33.2-k3s1 # NodeSelector to target specific nodes (optional) nodeSelector: matchLabels: kairos.io/managed: \"true\" # Maximum number of nodes that can run the upgrade simultaneously # 0 means run on all nodes at once concurrency: 1 # Whether to stop creating new jobs when a job fails # Useful for canary deployments stopOnFailure: true Upgrade Process Flow Master Node Upgrade: The master node pulls the upgrade image from the remote registry Registry Caching: The image is automatically cached in the embedded Spegel registry Worker Node Upgrades: Worker nodes fetch the image from the local registry, avoiding duplicate downloads Bandwidth Efficiency: Only one node downloads the image from remote, others use the local cache K0s with Spegel K0s can be configured with Spegel for distributed image caching, following the official Spegel documentation for k0s. This setup requires specific containerd configuration that must be explicitly created in the cloud-config.\nManual Setup Master Node Configuration #cloud-config hostname: metal-{{ trunc 4 .MachineID }} users: - name: kairos # Change to your own user passwd: kairos # Change to your own password groups: - admin # This user needs to be part of the admin group k0s: enabled: true Worker Node Configuration #cloud-config hostname: metal-{{ trunc 4 .MachineID }} users: - name: kairos # Change to your own user passwd: kairos # Change to your own password groups: - admin # This user needs to be part of the admin group k0s-worker: enabled: true args: - --token-file /etc/k0s/token write_files: - path: /etc/k0s/token permissions: 0644 content: | \u003cTOKEN\u003e # generate it on your master node by running `k0s token create --role=worker` - path: /etc/k0s/containerd.d/spegel.toml permissions: 0644 content: | [plugins.\"io.containerd.grpc.v1.cri\".registry] config_path = \"/etc/containerd/certs.d\" [plugins.\"io.containerd.grpc.v1.cri\".containerd] discard_unpacked_layers = false Important Notes:\nReplace \u003cTOKEN\u003e with the actual token from your master node (generate it by running k0s token create --role=worker) The containerd configuration file /etc/k0s/containerd.d/spegel.toml must be explicitly created to enable Spegel compatibility This configuration follows the official Spegel k0s documentation Installing Spegel After your k0s cluster is running, install Spegel using the Helm chart with k0s-specific paths:\nhelm upgrade --create-namespace --namespace spegel --install spegel oci://ghcr.io/spegel-org/helm-charts/spegel \\ --set spegel.containerdSock=/run/k0s/containerd.sock \\ --set spegel.containerdContentPath=/var/lib/k0s/containerd/io.containerd.content.v1.content Upgrade Process apiVersion: operator.kairos.io/v1alpha1 kind: NodeOpUpgrade metadata: name: kairos-upgrade namespace: default spec: # The container image containing the new Kairos version image: quay.io/kairos/opensuse:leap-15.6-standard-amd64-generic-v3.5.0-k0s-v1.33.3-k0s.0 # NodeSelector to target specific nodes (optional) nodeSelector: matchLabels: kairos.io/managed: \"true\" # Maximum number of nodes that can run the upgrade simultaneously # 0 means run on all nodes at once concurrency: 1 # Whether to stop creating new jobs when a job fails # Useful for canary deployments stopOnFailure: true Upgrade Process Flow First Node Upgrade: The first node pulls the upgrade image from the remote registry Spegel Caching: The image is automatically cached in the Spegel distributed registry Subsequent Node Upgrades: When the second and subsequent nodes start their upgrade, they fetch the image from the local Spegel registry instead of pulling from remote Bandwidth Efficiency: Only the first node downloads the image from remote, others use the local cache Provider-kubeadm with Spegel Provider-kubeadm enables Kairos to use kubeadm for Kubernetes cluster management. With Spegel integration, you can achieve bandwidth-optimized upgrades by leveraging distributed image caching across your kubeadm-managed cluster.\nPrerequisites To use provider-kubeadm with Spegel, you need:\nA custom Kairos image built with provider-kubeadm (build instructions) Kubernetes version compatibility between your image and configuration Containerd runtime configured for Spegel integration Configuration Examples For complete, up-to-date configuration examples, refer to the provider-kubeadm repository where you’ll find two example configurations at the root. The examples include:\nMaster node configuration with containerd setup for Spegel Worker node configuration with proper registry mirroring Spegel deployment manifests Upgrade procedures with bandwidth optimization Key Configuration Components The provider-kubeadm Spegel integration requires specific containerd configuration:\n#cloud-config # Essential containerd configuration for Spegel stages: initramfs: - name: \"Setup containerd for Spegel\" files: - path: /etc/containerd/config.toml content: | version = 2 [plugins.\"io.containerd.grpc.v1.cri\".registry] config_path = \"/etc/containerd/certs.d\" [plugins.\"io.containerd.grpc.v1.cri\".containerd] discard_unpacked_layers = false Spegel Deployment After your kubeadm cluster is running, deploy Spegel:\n# Install Spegel using the Helm chart helm install --create-namespace --namespace spegel spegel oci://ghcr.io/spegel-org/helm-charts/spegel --set spegel.containerdSock=/run/containerd/containerd.sock --set spegel.containerdContentPath=/opt/containerd/io.containerd.content.v1.content --set spegel.containerdRegistryConfigPath=/etc/containerd/certs.d Upgrade Process with Provider-kubeadm The upgrade process follows the same bandwidth-efficient pattern:\napiVersion: operator.kairos.io/v1alpha1 kind: NodeOpUpgrade metadata: name: kairos-kubeadm-upgrade namespace: default spec: # Custom Kairos image with provider-kubeadm image: your-registry/kairos-kubeadm:v1.32.0-latest nodeSelector: matchLabels: kairos.io/managed: \"true\" # Sequential upgrades to maximize cache utilization concurrency: 1 stopOnFailure: true Upgrade Process Flow First Node Upgrade: Downloads the upgrade image from the remote registry Spegel Caching: The image is cached in the Spegel distributed registry Subsequent Nodes: Fetch the image from the local Spegel cache Bandwidth Savings: Only one download from remote, all others use local cache If you need to verify that spegel is working, you can also check the upstream Spegel documenteation here: https://spegel.dev/docs/faq/#how-do-i-know-that-spegel-is-working\nImportant Considerations Image Compatibility: Ensure your custom provider-kubeadm image includes the correct Kubernetes version that matches your kubernetesVersion configuration Containerd Configuration: The containerd setup is critical for Spegel functionality with provider-kubeadm Network Policies: Ensure Spegel can communicate between nodes (typically requires port 5001) For the most current examples and detailed configurations, always refer to the provider-kubeadm repository which contains tested configurations updated for the latest versions.\nRelated Documentation K3s Stages - Running stages with k3s Multi-node Setup - Setting up multi-node clusters P2P Examples - P2P coordination examples Provider-kubeadm Repository - Complete examples and build instructions Spegel Documentation - Official Spegel distributed registry documentation ","categories":"","description":"This section describes how to optimize bandwidth usage during OS upgrades using distributed caching solutions.","excerpt":"This section describes how to optimize bandwidth usage during OS …","ref":"/docs/examples/bandwidth-optimized-upgrades/","tags":"","title":"Bandwidth Optimized Upgrades"},{"body":"","categories":"","description":"","excerpt":"","ref":"/blog/","tags":"","title":"Blog"},{"body":"Welcome to the guide on setting up MetalLB on a Kairos cluster with K3s! This tutorial will walk you through the steps of using a Kairos bundle to automatically configure MetalLB on your local network with an IP range of 192.168.1.10-192.168.1.20. Check out the MetalLB example to configure it without a bundle.\nFor those unfamiliar with MetalLB, it is an open-source load balancer implementation for bare metal Kubernetes clusters that utilizes standard routing protocols. When used with K3s on Kairos, it provides load balancing capabilities and helps manage IP addresses within a cluster.\nPrerequisites Before we begin, you will need to have the following:\nA Kairos standard image which includes K3s A baremetal node to run the installation Installation Follow the Installation documentation for Kairos. Use the following cloud configuration file when setting up Kairos: #cloud-config hostname: metal-{{ trunc 4 .MachineID }} users: - name: kairos # Change to your pass here passwd: kairos groups: - admin ssh_authorized_keys: # Replace with your github user and un-comment the line below: # - github:mudler k3s: enabled: true args: - --disable=traefik,servicelb # Specify the bundle to use bundles: - targets: - run://quay.io/kairos/community-bundles:metallb_latest # Specify metallb settings, available only with the bundle. metallb: version: 0.13.7 address_pool: 192.168.1.10-192.168.1.20 There are a few key points to note in the configuration file:\nThe metallb block is provided by the MetalLB bundle and allows us to specify the version of MetalLB that we want to deploy, as well as the address_pool available for our services. The bundles block enables the run bundle type. The bundle we are using is part of the community-bundles repository. And that’s it! With these steps, you should now have MetalLB configured and ready to use on your Kairos cluster. If you have any questions or run into any issues, don’t hesitate to check out the bundle documentation or reach out to the community for support.\n","categories":"","description":"This section describe examples on how to use a Kairos bundle to deploy MetalLB on top of K3s","excerpt":"This section describe examples on how to use a Kairos bundle to deploy …","ref":"/docs/examples/bundles/","tags":"","title":"Bundles"},{"body":"","categories":"","description":"","excerpt":"","ref":"/community/","tags":"","title":"Community"},{"body":"Deploying Ubuntu kernel firmware via systemd‑sysext under Trusted Boot (Kairos) This hands‑on example shows how to keep your Ubuntu‑based Kairos image slim by removing firmware from the base OS, packaging the firmware as a signed system extension (sysext), and loading it under Trusted Boot (UKI)—with notes on early‑boot firmware availability.\nWhy this pattern?\n• Avoid oversized UKIs: firmware blobs can bloat the UKI and even trigger allocation errors on certain platforms.\n• Stay verifiable: sysexts can be signed and verified under Trusted Boot.\n• Swap/iterate fast: update firmware by swapping the sysext without rebuilding the whole OS.\nFor more info on Kairos sysexts, see the sysext documentation.\nPrerequisites A workstation with Docker/Podman. Secure Boot/Trusted Boot keys (DB key + certificate) you already use for your Kairos UKIs: db.key and db.pem. A Kairos Ubuntu base you control (we’ll build a minimal one with kairos-init). AuroraBoot container image (quay.io/kairos/auroraboot, v0.9.0+ recommended). A machine that will boot Kairos with Trusted Boot. Terminology quickies\nsysext: a signed+verity system extension image that overlays /usr (and optionally /opt) at boot. UKI: Unified Kernel Image (*.efi) that systemd‑boot loads. Under Trusted Boot, the boot chain and optional sysext payloads get measured in TPM PCRs. Step 1 — Build a slim Ubuntu base without firmware Create Dockerfile.kairos-ubuntu-slim that “Kairosifies” Ubuntu and strips firmware from the rootfs:\n# Stage with kairos-init FROM quay.io/kairos/kairos-init:v0.5.19 AS kairos-init # Your Ubuntu base FROM ubuntu:24.04 # Run kairos-init to turn this into a Kairos-ready base RUN --mount=type=bind,from=kairos-init,src=/kairos-init,dst=/kairos-init \\ /kairos-init -l debug -t true --version 1.0.0 \u0026\u0026 /kairos-init validate -t true # Ensure the base rootfs contains NO firmware # (Kernel firmware will be provided by a sysext at boot.) RUN apt-get remove -y linux-firmware Build and tag the image:\ndocker build -f Dockerfile.kairos-ubuntu-slim -t kairos-ubuntu:1.0.0 . Step 2 — Create a firmware sysext with AuroraBoot We’ll craft a tiny OCI image that contains just the firmware files under /usr/lib/firmware, then let AuroraBoot convert/sign it into *.sysext.raw.\nCreate a minimal Dockerfile that collects only the firmware you need: # Dockerfile.firmware # Use the slim Kairos Ubuntu base so everything matches FROM kairos-ubuntu:1.0.0 RUN apt-get update \u0026\u0026 \\ apt-get install -y --no-install-recommends linux-firmware \u0026\u0026 \\ rm -rf /var/lib/apt/lists/* Build it locally:\ndocker build -f Dockerfile.firmware -t firmware:ubuntu-24.04 . Convert and sign as a sysext with AuroraBoot (uses your Secure Boot DB key): # Create a signed+verity sysext from the LAST layer of the OCI image # (AuroraBoot will autogenerate the extension-release metadata.) docker run --rm -ti \\ -v \"$PWD\":/build \\ -v \"$PWD/keys\":/keys \\ -v /var/run/docker.sock:/var/run/docker.sock \\ quay.io/kairos/auroraboot \\ sysext \\ --private-key=/keys/db.key \\ --certificate=/keys/db.pem \\ --output=/build \\ firmware-ubuntu-2404 firmware:ubuntu-24.04 # Result: firmware-ubuntu-2404.sysext.raw 2025-09-04T12:23:05Z INF [1] 🚀 Start sysext creation 2025-09-04T12:23:05Z DBG creating directory dir=/tmp/auroraboot-sysext-3867492028 2025-09-04T12:23:05Z INF [1] 💿 Getting image info 2025-09-04T12:23:05Z INF [1] 📤 Extracting archives from image layer 2025-09-04T12:23:17Z INF 📦 Packing sysext into raw image 2025-09-04T12:23:18Z INF 🎉 Done sysext creation output=/build/firmware-ubuntu-2404.sysext.raw (Optional) Inspect the result: sudo systemd-dissect firmware-ubuntu-2404.sysext.raw File Name: firmware-ubuntu-2404.sysext.raw Size: 519.9M Sec. Size: 512 Arch.: x86-64 Image Name: firmware-ubuntu-2404 Image UUID: 60f29b0d-f685-4878-b529-4ef35c3f1196 sysext R.: ID=_any ARCHITECTURE=x86-64 Use As: ✗ bootable system for UEFI ✗ bootable system for container ✗ portable service ✗ initrd ✓ sysext for system ✓ sysext for portable service ✗ sysext for initrd ✗ confext for system ✗ confext for portable service ✗ confext for initrd RW DESIGNATOR PARTITION UUID PARTITION LABEL FSTYPE AR\u003e ro root 12492769-aef0-605b-0df7-b48fa9d8fab8 root-x86-64 erofs x8\u003e ro root-verity 645436ca-6eb4-af91-25b0-e93b3601607b root-x86-64-verity DM_verity_hash x8\u003e ro root-verity-sig ba282899-118d-4b1c-ba8c-5e1af8e37c81 root-x86-64-verity-sig verity_hash_signature x8\u003e Step 3 — Deliver the sysext Via kairos-agent after the install has been done and we have booted to the system:\n$ kairos-agent sysext install https://example.org/firmware-ubuntu-2404.sysext.raw $ kairos-agent sysext enable --common --now firmware-ubuntu-2404 You can also scp the file onto the node and enable it locally.\n$ kairos-agent sysext install file:/tmp/firmware-ubuntu-2404.sysext.raw $ kairos-agent sysext enable --common --now firmware-ubuntu-2404 Step 4 — Trusted Boot specifics and signatures Sign sysexts with the same key/cert used for your UKI (DB key). Kairos verifies sysext signatures under Trusted Boot and will ignore unsigned/mismatched ones. Keep the sysext filename versioned (e.g. firmware‑ubuntu‑2404‑YYYYMMDD.sysext.raw) so systemd can order and upgrade cleanly. Optional: make firmware available to the initramfs itself Some hardware needs firmware before the real root is mounted (e.g., early GPU, NIC, or storage). You can workaround this by embedding a minimal subset directly into the image:\n- During your UKI build step, copy only the critical blobs into `/usr/lib/firmware` - Keep the full set in the sysext for post‑switch use. Step 5 — Verify at runtime After first boot on a node:\n# See which extensions are installed and active for this profile kairos-agent sysext list --active # systemd view of merged overlays systemd-sysext status # Kernel firmware requests dmesg | grep -i firmware Upgrading the firmware Ship a new .sysext.raw and enable it atomically:\nkairos-agent sysext install https://example.org/firmware-ubuntu-2404-2025.09.01.sysext.raw kairos-agent sysext enable --common --now firmware-ubuntu-2404-2025.09.01 # Optionally remove the old image after a soak period kairos-agent sysext remove firmware-ubuntu-2404-2025.06.01 Under Trusted Boot, the new sysext must be signed with the same key/cert as your UKI.\nTroubleshooting \u0026 known gotchas Sysext filename must end with .sysext.raw for Kairos/immucore to find it. Only /usr (and optionally /opt) is overlayed. Ensure firmware lives under /usr/lib/firmware. Order matters: multiple sysexts are applied in version‑sorted order; keep names properly versioned. Unsigned / wrong‑key sysext: will be ignored in Trusted Boot—check logs under /run/immucore/. Appendix — Reloading devices after firmware becomes available After the firmware sysext is active, some devices that probed before the overlay may still be missing firmware. Use a short-lived service to retrigger or reload the affected drivers.\nOne-shot service (generic) Create /etc/systemd/system/reprobe-after-firmware.service:\n[Unit] Description=Re-probe devices once firmware sysext is available After=systemd-sysext.service ConditionDirectoryNotEmpty=/usr/lib/firmware [Service] Type=oneshot # Reload udev rules and re-emit add/change events ExecStart=/usr/bin/udevadm control --reload ExecStart=/usr/bin/udevadm trigger --action=add --subsystem-match=pci # or --subsystem-match=usb/net/sound/input etc... ExecStart=/usr/bin/udevadm trigger --action=change --subsystem-match=pci # or --subsystem-match=usb/net/sound/input etc... # (Optional) Reload common drivers that usually need firmware # Adjust to your hardware; safe examples: ExecStart=/usr/sbin/modprobe -r iwlmvm iwlwifi || true ExecStart=/usr/sbin/modprobe iwlwifi iwlmvm || true ExecStart=/usr/sbin/modprobe -r e1000e || true ExecStart=/usr/sbin/modprobe e1000e || true ExecStart=/usr/sbin/modprobe -r rtw_8821au || true ExecStart=/usr/sbin/modprobe rtw_8821au || true [Install] WantedBy=multi-user.target Enable it:\nsudo systemctl daemon-reload sudo systemctl enable --now reprobe-after-firmware.service Binding trick (when unloading a module isn’t safe) For GPUs or storage controllers backing the root console, prefer unbind/bind over modprobe -r.\nReplace the PCI BDF and driver to match your device:\n# Example: rebind an Intel iGPU without unloading the module BDF=\"0000:00:02.0\" DRV=\"i915\" echo \"$BDF\" | sudo tee /sys/bus/pci/drivers/$DRV/unbind echo \"$BDF\" | sudo tee /sys/bus/pci/drivers/$DRV/bind To automate, drop a helper at /usr/local/bin/rebind-pci.sh:\n#!/usr/bin/env bash set -euo pipefail BDF=\"$1\" DRV=\"$(basename \"$(readlink -f /sys/bus/pci/devices/$BDF/driver)\")\" echo \"$BDF\" \u003e\"/sys/bus/pci/drivers/$DRV/unbind\" echo \"$BDF\" \u003e\"/sys/bus/pci/drivers/$DRV/bind\" …and call it from a tiny unit that has the After=systemd-sysext.service stanza.\nYou can do the same trick with usb devices by sending the device ID to /sys/bus/usb/drivers/usb/bind and /sys/bus/usb/drivers/usb/unbind, like echo \"1-1.2\" | sudo tee /sys/bus/usb/drivers/usb/unbind.\n","categories":"","description":"This section describes an examples on how to deploy the kernel firmware via sysext on Trusted Boot","excerpt":"This section describes an examples on how to deploy the kernel …","ref":"/docs/examples/trusted-boot-firmware-sysext/","tags":"","title":"Deploying kernel firmware via sysext on Trusted Boot"},{"body":" Info This tutorial is based on Opensuse Leap. Kdump configs vary over distributions and we are not able to test them all but they should be easily adaptable from this tutorial. Introduction kdump is a feature of the Linux kernel that creates crash dumps in the event of a kernel crash. When triggered, kdump exports a memory image (also known as vmcore) that can be analyzed for the purposes of debugging and determining the cause of a crash.\nIn the event of a kernel crash, kdump preserves system consistency by booting another Linux kernel, which is known as the dump-capture kernel, and using it to export and save a memory dump. As a result, the system boots into a clean and reliable environment instead of relying on an already crashed kernel that may cause various issues, such as causing file system corruption while writing a memory dump file\nThis is why we need a clean initramfs that disables most of the modules and mounts the persistent partition into the /var/crash route in order to store the dumps over reboots\nRequirements A custom image that builds a simple,small initrd with the kdump module and that mounts persistent to store the dump A service override to skip the kdump service rebuilding the initrd on an immutable system Steps Build the custom derivative artifact Build an iso from that artifact Install the iso Check that kdump is enabled and works Building the custom derivative We will keep this short as there is more docs about building your own derivatives than what we can go in this tutorial like the Customizing page\nThe main step is to build a clean initrd that has the kdump module and can mount persistent to store the kernel dump.\nFROM quay.io/kairos/opensuse:leap-15.6-core-amd64-generic-v3.1.2 # Install kdump RUN zypper ref \u0026\u0026 zypper in -y kdump kexec-tools makedumpfile # Build initramfs with kdump module on it RUN kernel=$(ls /lib/modules | head -n1) \u0026\u0026 depmod -a \"${kernel}\" \u0026\u0026 \\ dracut -N -f /var/lib/kdump/initrd \"${kernel}\" -a kdump \\ --omit \"plymouth resume usrmount zz-fadumpinit immucore kairos-network kairos-sysext\" --compress \"xz -0 --check=crc32\" \\ --mount \"/dev/disk/by-label/COS_PERSISTENT /kdump/mnt/var/crash ext4 rw,relatime\" # On opensuse the path to the kernel is hardcoded so we need to soft link it to our kernel RUN ln -s /boot/vmlinuz /var/lib/kdump/kernel # Enable services. early service is the one on the initramfs RUN systemctl enable kdump-early \u0026\u0026 systemctl enable kdump We are generating a new initrd and storing it on /var/lib/kdump/initrd as the kdump service will look into that directory to find the kernel and initrd needed to exec into. We are using the following options to generate a simple, clean initrd:\n-a kdump: adds the kdump module explicitly to the initramfs. --omit: omits modules from initrd. This is to have a clean, simple initramfs. --compress: Compresses the initrd to keep it small. --mount: Explicitly mount the PERSISTENT partition into /kdump/mnt/var/crash so kdump can store the dump Then we generate a new artifact using that dockerfile:\n$ docker build -t kdump-kairos . [+] Building 0.6s (9/9) FINISHED docker:default =\u003e [internal] load build definition from Dockerfile 0.0s =\u003e =\u003e transferring dockerfile: 853B 0.0s =\u003e [internal] load metadata for quay.io/kairos/opensuse:leap-15.6-core-amd64-generic-v3.1.2 0.6s =\u003e [internal] load .dockerignore 0.0s =\u003e =\u003e transferring context: 2B 0.0s =\u003e [1/5] FROM quay.io/kairos/opensuse:leap-15.6-core-amd64-generic-v3.1.2@sha256:a85cf92ea9ed5a0 0.0s =\u003e CACHED [2/5] RUN zypper ref \u0026\u0026 zypper in -y kdump kexec-tools makedumpfile 0.0s =\u003e CACHED [3/5] RUN kernel=$(ls /lib/modules | head -n1) \u0026\u0026 depmod -a \"${kernel}\" \u0026\u0026 0.0s =\u003e CACHED [4/5] RUN ln -s /boot/vmlinuz /var/lib/kdump/kernel 0.0s =\u003e CACHED [5/5] RUN systemctl enable kdump-early \u0026\u0026 systemctl enable kdump 0.0s =\u003e exporting to image 0.0s =\u003e =\u003e exporting layers 0.0s =\u003e =\u003e writing image sha256:a7ed8f51a32648f8e97cae1eb253a309b696dc3243ba366b489a76150721f403 0.0s =\u003e =\u003e naming to docker.io/library/kdump-kairos Build an iso from that artifact Again, this tutorial does not cover this part deeply as there are docs providing a deep insight onto this like the AuroraBoot page\n$ docker run -v \"$PWD\"/build-iso:/tmp/auroraboot -v /var/run/docker.sock:/var/run/docker.sock --rm -ti quay.io/kairos/auroraboot --set container_image=\"docker://kdump-kairos\" --set \"disable_http_server=true\" --set \"disable_netboot=true\" --set \"state_dir=/tmp/auroraboot\" 2:38PM INF Pulling container image 'kdump-kairos' to '/tmp/auroraboot/temp-rootfs' (local: true) 2:38PM INF Generating iso 'kairos' from '/tmp/auroraboot/temp-rootfs' to '/tmp/auroraboot/build' Install the iso Then we burn the resulting ISO to a dvd or usb stick and boot it normally.\nIn order to have kdump working properly, we need to reserve a chunk of memory from the system so it can dump correctly. Several tools exist for this like kdumptool which will give us some approximated values to reserve if running on the machine. A safe value might be 512M high and 72M low\nThis values need to be passed to the kernel in the cmdline so kdump knows what memory it has to work with. The easiest way is to set the install.grub_options.extra_cmdline value in the cloud-config during install.\n#cloud-config install: auto: true reboot: true grub_options: extra_cmdline: \"crashkernel=512M,high crashkernel=72M,low\" stages: initramfs: - name: \"Set user and password\" users: kairos: passwd: \"kairos\" - name: \"Set hostname\" hostname: kairos-{{ trunc 4 .Random }} Check that kdump is enabled and works Once the system has been installed there is 2 services that can be checked to see if kdump is correctly enabled. kdump-early and kdump services run in initrafms and userspace respectively and both of them should be in status Active after booting:\n$ systemctl status kdump-early * kdump-early.service - Load kdump kernel early on startup Loaded: loaded (/usr/lib/systemd/system/kdump-early.service; enabled; preset: disabled) Active: active (exited) since Mon 2024-09-09 14:41:02 UTC; 19min ago Main PID: 1556 (code=exited, status=0/SUCCESS) CPU: 68ms Sep 09 14:41:02 kairos-cfig systemd[1]: Starting Load kdump kernel early on startup... Sep 09 14:41:02 kairos-cfig systemd[1]: Finished Load kdump kernel early on startup. $ systemctl status kdump * kdump.service - Load kdump kernel and initrd Loaded: loaded (/usr/lib/systemd/system/kdump.service; enabled; preset: disabled) Active: active (exited) since Mon 2024-09-09 14:41:03 UTC; 20min ago Main PID: 1672 (code=exited, status=0/SUCCESS) CPU: 64ms Sep 09 14:41:03 kairos-cfig systemd[1]: Starting Load kdump kernel and initrd... Sep 09 14:41:03 kairos-cfig systemd[1]: Finished Load kdump kernel and initrd. Now we know that our systems is kdump ready and in case of a kernel crash it will dump the crash allowing us to troubleshoot it.\nThe dumps will be stored in /usr/local/DATE and will survive reboots\n$ ls -ltra /usr/local/2024-09-09-15-03/ total 79488 drwxr-xr-x 9 root root 4096 Sep 9 15:03 .. -rw-r--r-- 1 root root 74108 Sep 9 15:03 dmesg drwxr-xr-x 2 root root 4096 Sep 9 15:03 . -rw-r--r-- 1 root root 81305093 Sep 9 15:03 vmcore -rw-r--r-- 1 root root 320 Sep 9 15:03 README.txt Warning You can manually trigger a crash by running echo c \u003e /proc/sysrq-trigger\nNote that this will immediately crash your machine, dump the kernel and restart so make sure that everything is ready for the sudden crash.\n","categories":"","description":"This section describe examples on how to enable kdump in Kairos derivatives","excerpt":"This section describe examples on how to enable kdump in Kairos …","ref":"/docs/examples/kdump/","tags":"","title":"Enabling kdump"},{"body":"If Kairos is installed on a device with an Intel AMT device, the device can be automatically registered with an MPS server. The registration will only run during installation. Devices with Kairos already installed will not be affected.\nConfiguration To configure this bundle, it must be referenced in the install bundles section. Additional configuration can be included under the amt section. To see all configuration options see the openamt repository.\n#cloud-config install: bundles: - run://quay.io/kairos/community-bundles:openamt_latest amt: server_address: wss://mps.contoso.com/activate profile: myprofile ","categories":"","description":"This bundle configures Intel AMT devices during Kairos installation.","excerpt":"This bundle configures Intel AMT devices during Kairos installation.","ref":"/docs/examples/openamt/","tags":"","title":"Intel Open AMT Registration"},{"body":"This example how to use Keylime with Kairos in order to provide verified measurement for runtime binaries or files in a Kairos system. This, for instance involve having measurements for specific files in the persistent portion of the disk, or the configuration directories.\nMost of the steps are already covered in the Keylime documentation. Here we will cover the steps that are specific to Kairos.\nExtend Kairos First of all we need to create a Kairos derivative with the keylime agent, in order to do this we use the Kairos factory process where we will build our own OS derivative to bundle keylime-agent:\n# Build the keylime agent FROM ubuntu:24.04 AS keylime-build RUN apt-get update \u0026\u0026 apt-get install -y --no-install-recommends ca-certificates git gcc libclang-dev libssl-dev libtss2-dev libzmq3-dev pkg-config rustup make \u0026\u0026 rm -rf /var/lib/apt/lists/* WORKDIR /keylime RUN git clone --depth 1 --branch v0.2.7 https://github.com/keylime/rust-keylime.git /keylime # Set up install destination dir so we can reuse the build artifacts for our Kairos image ENV DESTDIR=/keylime-output RUN rustup default stable \u0026\u0026 make \u0026\u0026 make install # Build the final image with the keylime agent copied over, only systemd supported for now FROM quay.io/kairos/rockylinux:9-core-amd64-generic-v3.4.1 AS default COPY --from=keylime-build /keylime-output/ / Then you can build your image with the agent on it:\ndocker build -t kairos-keylime . That will generate an artifact based on the Kairos image with the keylime-agent installed.\nThen you need at a minimum the follow configuration in your cloud config:\n#cloud-config install: auto: true reboot: true device: /dev/vda bind_mounts: - /var/lib/keylime grub_options: extra_cmdline: \"ima_appraise=fix ima_template=ima-ng ima_policy=custom\" # custom policy will load it from /etc/ima/ima-policy stages: initramfs: - name: \"Set user and password\" users: kairos: passwd: \"kairos\" groups: - \"admin\" keylime: groups: - \"tss\" hostname: \"kairos-keylime\" - files: - name: Set default IMA policy # In initramfs as the kernel will load it on rootfs pivot path: /etc/ima/ima-policy permissions: 0644 content: | # PROC_SUPER_MAGIC dont_measure fsmagic=0x9fa0 # SYSFS_MAGIC dont_measure fsmagic=0x62656572 # DEBUGFS_MAGIC dont_measure fsmagic=0x64626720 # TMPFS_MAGIC dont_measure fsmagic=0x01021994 # RAMFS_MAGIC dont_measure fsmagic=0x858458f6 # SECURITYFS_MAGIC dont_measure fsmagic=0x73636673 # SELINUX_MAGIC dont_measure fsmagic=0xf97cff8c # CGROUP_SUPER_MAGIC dont_measure fsmagic=0x27e0eb # OVERLAYFS_MAGIC # when containers are used we almost always want to ignore them dont_measure fsmagic=0x794c7630 # MEASUREMENTS # This covers regular binary execution (e.g., running an ELF file with execve). measure func=BPRM_CHECK # Captures JIT-executed or interpreted code and shared libraries mapped into memory with executable permission. measure func=FILE_MMAP mask=MAY_EXEC # Tracks all kernel modules loaded by root, which is crucial for maintaining kernel integrity. measure func=MODULE_CHECK uid=0 # Measures all files that are read, written, or appended to. measure func=FILE_CHECK mask=MAY_READ measure func=FILE_CHECK mask=MAY_WRITE measure func=FILE_CHECK mask=MAY_APPEND boot: - name: \"Set Keylime config\" files: - path: /var/lib/keylime/cv_ca/cacert.crt # This is the cert from the Keylime registrar/tenant server. The clients need to trust it. content: | -----BEGIN CERTIFICATE----- MIID8zCCAtugAwIBAgIBATANBgkqhkiG9w0BAQsFADBzMQswCQYDVQQGEwJVUzEm MCQGA1UEAwwdS2V5bGltZSBDZXJ0aWZpY2F0ZSBBdXRob3JpdHkxCzAJBgNVBAgM Ak1BMRIwEAYDVQQHDAlMZXhpbmd0b24xDjAMBgNVBAoMBU1JVExMMQswCQYDVQQL DAI1MzAeFw0yNTA3MDQwODIxMThaFw0zNTA3MDIwODIxMThaMHMxCzAJBgNVBAYT AlVTMSYwJAYDVQQDDB1LZXlsaW1lIENlcnRpZmljYXRlIEF1dGhvcml0eTELMAkG A1UECAwCTUExEjAQBgNVBAcMCUxleGluZ3RvbjEOMAwGA1UECgwFTUlUTEwxCzAJ BgNVBAsMAjUzMIIBIjANBgkqhkiG9w0BAQEFAAOCAQ8AMIIBCgKCAQEAqdMRFikV w65U0B213Zscps3ODEXPkZgFFf5Uyv9Md9kV5zMSWk0Em/HTUUiixVz8o7+soK7f SZezKg8Je/Sy1eZlRLR7ijHQyHmFByMcHiBry8FhaHelP1bfUNVHY9PkTYX1i7Cb yXiSwD2x467Ao8KwZWNR01d9rDMwWSV73scddRt9hLaI8BWaTptpaC3tpQhvo4K9 LrYsOxpgoFGyMU09Ds5BOqt5IaU3DkY2bfkSy6D9W4GVzk56u0RHevy6kTA6DARK wDYlo+z/mgylLZsxD+r5VwxLjoXT1e+M2T4H/F2T/FPh5BLNngXOfaJl0YP9amLM oopARJ04qKO5pwIDAQABo4GRMIGOMA8GA1UdEwEB/wQFMAMBAf8wHQYDVR0OBBYE FE3BrSsveBPSY5ct2MLvIobGDDtSMCsGA1UdHwQkMCIwIKAeoByGGmh0dHA6Ly9s b2NhbGhvc3Q6MzgwODAvY3JsMA4GA1UdDwEB/wQEAwIBBjAfBgNVHSMEGDAWgBRN wa0rL3gT0mOXLdjC7yKGxgw7UjANBgkqhkiG9w0BAQsFAAOCAQEACk//H5DDy/F6 fqp7Ar3tWtTsrPiDmNo3P1w6u8dstuyfcfbHDi8HaN0YN2a837ya0eF4WzLIpo/b Ft4hNboGTIBQbcVsQ6sm6JY7ioI/eEiaHO0FeyIFfTy/oj00if5SZ1AT/cpumTCm NapygE7EZYzQU8VW1wLKZlOb0KvuBaLbfsfpFB05ZB1PYyxETTuZvs93vEs/huay HolE8pIl1eHE6fFcEeBtmnOGOMHHcITlmyjdzSuL/Z3aDmXxuQN5y/cyO2t3MQ24 i100gwrCtRGLmJo4cTr/57/KO8PkSZaNhWuLOfzLggBMHL6uXUr2Q8FBgTc/hUaX AZTsa1FQAA== -----END CERTIFICATE----- permissions: 0644 - path: /etc/keylime/agent.conf.d/10-config.conf content: | [agent] ip = '0.0.0.0' # This is the IP the agent will listen on, change if needed registrar_ip = '192.168.122.31' # change to the keylime remote attestation server IP uuid = '61388a67-baa4-4f2b-8221-d539b7b4d98b' # Generate an uuid with `uuidgen` or similar permissions: 0640 - name: \"Set keylime owner to /var/lib/keylime\" commands: - chown -R keylime:keylime /var/lib/keylime network: - name: \"Enable keylime_agent service\" # This will request activation but does not ADD the node and starts attestation systemctl: enable: - keylime_agent start: - keylime_agent Lets go a bit into detail of some of the options.\nbind_mounts: This is required for the keylime-agent to store the keys and certificates. It needs to be persisted across reboots. extra_cmdline: This is required to enable the IMA appraisal in the kernel. This is required for keylime to work if you expect to use runtime attestation and a custom IMA policy. users: We add the keylime user as the default keylime agent service will drop privileges to this user. Has to have the tss group as well. /etc/ima/ima-policy: This is the custom IMA policy that the kernel will use. The one provided is just a generic example. /var/lib/keylime ownership: The keylime agent will need to write to this directory. It is important to set the correct ownership. We do it at the end so all the written files are owned by the keylime user. systemctl: We want to enable and start the keylime_agent service so it starts on boot and is running. /etc/keylime/agent.conf.d/10-config.conf: This is the keylime agent configuration. Keylime agent provides a default config and we use this to override those default values. Minimal values that need configuring here are as follows: ip: The IP address the agent will listen on. This should be set to 0.0.0.0 to listen on all interfaces or to the specific interface IP address if you know it on advance. Otherwise it will only listen on the loopback interface and won’t be reachable from the outside. registrar_ip: The IP address of the keylime registrar server. Otherwise the agent will not be able to communicate with the registrar. uuid: The UUID of the agent. This is used to identify the agent in the registrar. This can be any UUID as long as it is unique in the registrar server. If you set it to ‘generate’ it will generate a random UUID for you but that’s not currently supported in Kairos. You can generate a UUID with uuidgen or similar tools. /var/lib/keylime/cv_ca/cacert.crt: This is the CA certificate that the agent will use to verify the registrar server. This is required for the agent to be able to communicate with the registrar server securely. You can get this certificate from the Keylime registrar server. With this values, building a derivative image and installing it should be enough to have the keylime agent running in Kairos. Now you will need to add the agent to the Keylime registrar, as its currently activated but not added. You can do this from the Keylime tenant by running the following command:\n$ keylime_tenant -c add --uuid UID_OF_AGENT --ip IP_OF_AGENT .... 2025-07-08 12:50:19.728 - keylime.tenant - INFO - Agent Info from Verifier (127.0.0.1:8881): {\"61388a67-baa4-4f2b-8221-d539b7b4d98b\": {\"operational_state\": \"Start\", \"v\": null, \"ip\": \"192.168.122.47\", \"port\": 9002, \"tpm_policy\": \"{\\\"mask\\\": \\\"0x0\\\"}\", \"meta_data\": \"{}\", \"has_mb_refstate\": 0, \"has_runtime_policy\": 0, \"accept_tpm_hash_algs\": [\"sha512\", \"sha384\", \"sha256\"], \"accept_tpm_encryption_algs\": [\"ecc\", \"rsa\"], \"accept_tpm_signing_algs\": [\"ecschnorr\", \"rsassa\"], \"hash_alg\": \"\", \"enc_alg\": \"\", \"sign_alg\": \"\", \"verifier_id\": \"default\", \"verifier_ip\": \"127.0.0.1\", \"verifier_port\": 8881, \"severity_level\": null, \"last_event_id\": null, \"attestation_count\": 0, \"last_received_quote\": 0, \"last_successful_attestation\": 0}} 2025-07-08 12:50:19.728 - keylime.tenant - INFO - Agent 61388a67-baa4-4f2b-8221-d539b7b4d98b (192.168.122.47:9002) added to Verifier (127.0.0.1:8881) after 0 tries Where UID_OF_AGENT is the UUID you set in the agent configuration and IP_OF_AGENT is the IP address of the agent.\nyou can add --cert default to provision the node with the default certificates from the registrar, like the revocation certificat and such. This is very helpful so further steps down the line can be done without having to worry about the certificates.\nNow from the tenant you can apply any policy you want to the agent. Note that the agent will start the attestation but there is no actual policy applied by default, so it will not fail the attestation process. It will just continue to run and report its state as valid.\nUsing a runtime policy As an example, you can use keylime-policy to create a runtime policy that will be applied to the agent. This policy can be as simple or as complex as you want, depending on your security requirements.\nA runtime policy in its most basic form is a set of “golden” cryptographic hashes of files’ un-tampered state or of keys that may be loaded onto keyrings for IMA verification\nExclude list that excludes everything except the /usr/local/ and /oem/ directories (perfect for Kairos :D):\n^/(?!oem/|usr/local/) Or an exclude list that just excludes the directories that are not relevant for the runtime policy (files that keep changing, like logs, temporary files, etc.):\n^/var/log .bash_history ^/sys ^/tmp First from the node, we can copy the IMA ascii measure list and scp it to the Keylime tenant server:\n$ scp /sys/kernel/security/ima/ascii_runtime_measurements root@TENANT_IP:/root/runtime_measurements Then on the tenant server, we can generate a policy from this list using the keylime-policy command. This command will create a policy based on the measurements in the file we just copied.\n$ keylime-policy create runtime --ima-measurement-list runtime_measurements -e excludelist.txt -o policy.json -v INFO:keylime.config:Reading configuration from ['/etc/keylime/logging.conf'] 2025-07-10 14:54:30.467 - keylime-policy - DEBUG - Measurement list is runtime_measurements 2025-07-10 14:54:30.468 - keylime-policy - DEBUG - Using digest algorithm 'sha256' obtained from the IMA measurement list Now we can apply policy to the agent. You can use the keylime_tenant command to do this:\nkeylime_tenant --command update -u 61388a67-baa4-4f2b-8221-d539b7b4d98b -t 192.168.122.47 --runtime-policy policy.json INFO:keylime.config:Reading configuration from ['/etc/keylime/logging.conf'] 2025-07-09 15:20:42.578 - keylime.config - INFO - Reading configuration from ['/etc/keylime/tenant.conf'] 2025-07-09 15:20:42.578 - keylime.config - INFO - Applied configuration snippets from /etc/keylime/tenant.conf.d 2025-07-09 15:20:42.578 - keylime.tenant - INFO - Setting up client TLS... 2025-07-09 15:20:42.578 - keylime.tenant - INFO - Using default client_cert option for tenant 2025-07-09 15:20:42.578 - keylime.tenant - INFO - Using default client_key option for tenant 2025-07-09 15:20:42.578 - keylime.tenant - INFO - No value provided in client_key_password option for tenant, assuming the key is unencrypted 2025-07-09 15:20:42.579 - keylime.tenant - INFO - TLS is enabled. 2025-07-09 15:20:42.593 - keylime.cli.policies - INFO - TPM PCR Mask from policy is 0x0 2025-07-09 15:20:42.643 - keylime.tenant - INFO - Agent Info from Verifier (127.0.0.1:8881): {\"61388a67-baa4-4f2b-8221-d539b7b4d98b\": {\"operational_state\": \"Terminated\", \"v\": null, \"ip\": \"192.168.122.47\", \"port\": 9002, \"tpm_policy\": \"{\\\"mask\\\": \\\"0x0\\\"}\", \"meta_data\": \"{}\", \"has_mb_refstate\": 0, \"has_runtime_policy\": 0, \"accept_tpm_hash_algs\": [\"sha512\", \"sha384\", \"sha256\"], \"accept_tpm_encryption_algs\": [\"ecc\", \"rsa\"], \"accept_tpm_signing_algs\": [\"ecschnorr\", \"rsassa\"], \"hash_alg\": \"sha256\", \"enc_alg\": \"rsa\", \"sign_alg\": \"rsassa\", \"verifier_id\": \"default\", \"verifier_ip\": \"127.0.0.1\", \"verifier_port\": 8881, \"severity_level\": null, \"last_event_id\": null, \"attestation_count\": 78, \"last_received_quote\": 1752066297, \"last_successful_attestation\": 1752066297}} {'code': 200, 'status': 'Success', 'results': {'quote': 'r/1RDR4AYACIAC4hWOYdkG4M3v+tpIFyj2HOcpM/hnxJNsfc3rT9r4S5BABRkWnVQd1Rja3VPUjlBeUhyeWRPagAAAAAF6sI1AAAABgAAAAABICQBJQASAAAAAAABAAsDAAABACBYQwK8jVUDbMvZmLPw0tE7unP+d4sVDS8/pqmkT+gkag==:ABQACwEACQih/HZ/9JO3/hCwGmE6LSojc/nYQgskf4t/6OGB3ed/3kyysUMYrSXtw38FoZA+uu/7YTEaW+qWzf/KVPjOU5GJUB1B2StdSv570hTps7InxxDXhphIFH/KI5f1OdEK5c8o09b/IpZ/nGdYmp3GnowAivm+9y/0xarRA5GMUiVWdJT7LhZMQvxRXG/Yhb5zwoewpFecmbcNZ7/t0qpFfEpifwhP/yhp1wPTdUds774hlPhy5PAXFFYnTVcDn8TN0a8tBENtA/tewx3i6pxhIggW/miNyzgdKrQyIBCXgYr8U8zykk0DjTnWVeML3lzEj3CGZ1HTmu3l512juxv32A==:AQAAAAsAAwAAAQAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAQAAAAEAAAAgADOY31045T1AWJQ9mnWd1S0uBPHVWocFH54zRICCX9KnAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA=', 'hash_alg': 'sha256', 'enc_alg': 'rsa', 'sign_alg': 'rsassa', 'pubkey': '-----BEGIN PUBLIC KEY-----\\nMIIBIjANBgkqhkiG9w0BAQEFAAOCAQ8AMIIBCgKCAQEAm2R9EoaB0qhdKg+RbtJy\\nYPbCM3CgxpnscdY0Eyv5WPfmkuJ9/9h/vWnqBDmxfXrZEhYlZIUrfyUjbNUyckz5\\nXGgWpDZjNE+srxT7qwrUrCLpCvYSlHiLko6s4yinUKO0LPKQ1f41ATYd1JO3gXyD\\nx8CabBqorSREz+KDO9uyUJyszL8lK0zLMAKXt/ZB3ZS76OmpmwBcQGpn0VtYHYl1\\n584A4nStMIlNQET6QsQ70QIm5sTgaTGqrLoEHkQP7a1J9RfcHYrxqYg4dEgTEER7\\nkPuLTKAGSdfqYzvOAIDqSsn3BxS1hC1TQ+ln6MprJSKG23VpMK1Q0PyTee7FvxVz\\ntQIDAQAB\\n-----END PUBLIC KEY-----\\n'}} 2025-07-09 15:20:42.849 - keylime.tenant - WARNING - DANGER: EK cert checking is disabled and no additional checks on EKs have been specified with ek_check_script option for Agent 61388a67-baa4-4f2b-8221-d539b7b4d98b (192.168.122.47:9002). Keylime is not secure!! 2025-07-09 15:20:42.849 - keylime.tenant - INFO - Quote from Agent 61388a67-baa4-4f2b-8221-d539b7b4d98b (192.168.122.47:9002) validated 2025-07-09 15:20:43.268 - keylime.tenant - INFO - Agent Info from Verifier default (127.0.0.1:8881): {\"61388a67-baa4-4f2b-8221-d539b7b4d98b\": {\"operational_state\": \"Start\", \"v\": null, \"ip\": \"192.168.122.47\", \"port\": 9002, \"tpm_policy\": \"{\\\"mask\\\": \\\"0x400\\\"}\", \"meta_data\": \"{}\", \"has_mb_refstate\": 0, \"has_runtime_policy\": 1, \"accept_tpm_hash_algs\": [\"sha512\", \"sha384\", \"sha256\"], \"accept_tpm_encryption_algs\": [\"ecc\", \"rsa\"], \"accept_tpm_signing_algs\": [\"ecschnorr\", \"rsassa\"], \"hash_alg\": \"\", \"enc_alg\": \"\", \"sign_alg\": \"\", \"verifier_id\": \"default\", \"verifier_ip\": \"127.0.0.1\", \"verifier_port\": 8881, \"severity_level\": null, \"last_event_id\": null, \"attestation_count\": 0, \"last_received_quote\": 0, \"last_successful_attestation\": 0}} 2025-07-09 15:20:43.268 - keylime.tenant - INFO - Agent 61388a67-baa4-4f2b-8221-d539b7b4d98b (192.168.122.47:9002) added to Verifier default (127.0.0.1:8881) after 0 tries This will apply the runtime policy to the agent and the agent will start the attestation process. You can see this in the agent logs:\nJul 10 12:55:39 localhost keylime_agent[1551]: INFO keylime_agent \u003e GET invoked from \"192.168.122.31\" with uri /v2.1/quotes/integrity?nonce=TMywGq9gEgYy8QaptsB1\u0026mask=0x400\u0026partial=1\u0026ima_ml_entry=1739 Jul 10 12:55:39 localhost keylime_agent[1551]: INFO keylime_agent::quotes_handler \u003e GET integrity quote returning 200 response Jul 10 12:55:41 localhost keylime_agent[1551]: INFO keylime_agent \u003e GET invoked from \"192.168.122.31\" with uri /v2.1/quotes/integrity?nonce=eyI1bo2HpS081kM1FbnQ\u0026mask=0x400\u0026partial=1\u0026ima_ml_entry=1739 Jul 10 12:55:42 localhost keylime_agent[1551]: INFO keylime_agent::quotes_handler \u003e GET integrity quote returning 200 response Jul 10 12:55:44 localhost keylime_agent[1551]: INFO keylime_agent \u003e GET invoked from \"192.168.122.31\" with uri /v2.1/quotes/integrity?nonce=3TStNCteiVm5NC0dCraW\u0026mask=0x400\u0026partial=1\u0026ima_ml_entry=1739 Jul 10 12:55:44 localhost keylime_agent[1551]: INFO keylime_agent::quotes_handler \u003e GET integrity quote returning 200 response You can also check on the verifier logs that the agent is in the correct state:\njul 10 14:57:06 keylime keylime_verifier[1410]: 2025-07-10 14:57:06.216 - keylime.tpm - INFO - Checking IMA measurement list on agent: 61388a67-baa4-4f2b-8221-d539b7b4d98b jul 10 14:57:08 keylime keylime_verifier[1410]: 2025-07-10 14:57:08.449 - keylime.tpm - INFO - Checking IMA measurement list on agent: 61388a67-baa4-4f2b-8221-d539b7b4d98b jul 10 14:57:10 keylime keylime_verifier[1410]: 2025-07-10 14:57:10.637 - keylime.tpm - INFO - Checking IMA measurement list on agent: 61388a67-baa4-4f2b-8221-d539b7b4d98b jul 10 14:57:12 keylime keylime_verifier[1410]: 2025-07-10 14:57:12.854 - keylime.tpm - INFO - Checking IMA measurement list on agent: 61388a67-baa4-4f2b-8221-d539b7b4d98b Now we can test the revocation of the agent once a file is modified. We will change the /oem/90_custom.yaml and change a value in there:\n$ sed -i 's/reboot: true/reboot: false/' /oem/90_custom.yaml Sure enough, in a few seconds the agent will be revoked and you can see this in the verifier logs:\njul 10 15:00:00 keylime keylime_verifier[1410]: 2025-07-10 15:00:00.941 - keylime.tpm - INFO - Checking IMA measurement list on agent: 61388a67-baa4-4f2b-8221-d539b7b4d98b jul 10 15:00:03 keylime keylime_verifier[1410]: 2025-07-10 15:00:03.153 - keylime.tpm - INFO - Checking IMA measurement list on agent: 61388a67-baa4-4f2b-8221-d539b7b4d98b jul 10 15:00:03 keylime keylime_verifier[1410]: 2025-07-10 15:00:03.153 - keylime.ima - WARNING - Hashes for file /oem/90_custom.yaml don't match 97356fb8edc035052bac594306d0fdcef99f28e42703acaafa8d7940868b60d0 not in ['046209cb4acb79d533a9c6a9845766d4f435bc059f45421ed6e1cceb9c2dcc2e'] jul 10 15:00:03 keylime keylime_verifier[1410]: 2025-07-10 15:00:03.153 - keylime.ima - ERROR - IMA ERRORS: Some entries couldn't be validated. Number of failures in modes: ImaNg 1. jul 10 15:00:03 keylime keylime_verifier[1410]: 2025-07-10 15:00:03.187 - keylime.verifier - WARNING - Agent 61388a67-baa4-4f2b-8221-d539b7b4d98b failed, stopping polling You can also see this in the agent logs:\nJul 10 13:00:02 localhost keylime_agent[1551]: INFO keylime_agent \u003e GET invoked from \"192.168.122.31\" with uri /v2.1/quotes/integrity?nonce=9Ovk0YVjEp2bXollI76m\u0026mask=0x400\u0026partial=1\u0026ima_ml_entry=1746 Jul 10 13:00:02 localhost keylime_agent[1551]: INFO keylime_agent::quotes_handler \u003e GET integrity quote returning 200 response Jul 10 13:00:02 localhost keylime_agent[1551]: INFO keylime_agent \u003e POST invoked from \"192.168.122.31\" with uri /v2.1/notifications/revocation Jul 10 13:00:02 localhost keylime_agent[1551]: INFO keylime_agent::notifications_handler \u003e Received revocation Jul 10 13:00:02 localhost keylime_agent[1551]: WARN keylime_agent::revocation \u003e Revocation certificate not yet available Using a TPM policy instead As an example, we add a policy that will only allow the agent to boot if the PCR 15 is equal to a specific value (in this case empty value as we haven’t measured anything into PCR15):\n$ keylime_tenant -c update --uuid UID_OF_AGENT -t IP_OF_AGENT --tpm_policy '{\"15\":[\"0000000000000000000000000000000000000000\",\"0000000000000000000000000000000000000000000000000000000000000000\",\"000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000\"]}' Then the agent will start the attestation process, which you can see both in the agent logs and in the verifier logs. As well as checking with the tenant that the agent is in the correct state.\nThen to test the revocation you can extend the PCR15 manually:\n$ tpm2_pcrextend 15:sha256=f1d2d2f924e986ac86fdf7b36c94bcdf32beec15324234324234234333333333 Then the verifier will see that the agent is not in the correct state and will revoke it. You can see this in the verifier logs:\n2025-07-10 15:05:15.296 - keylime.tenant - INFO - Agent Info from Verifier (127.0.0.1:8881): {\"61388a67-baa4-4f2b-8221-d539b7b4d98b\": {\"operational_state\": \"Invalid Quote\", \"v\": null, \"ip\": \"192.168.122.47\", \"port\": 9002, \"tpm_policy\": \"{\\\"15\\\": [\\\"0000000000000000000000000000000000000000\\\", \\\"0000000000000000000000000000000000000000000000000000000000000000\\\", \\\"000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000\\\"], \\\"mask\\\": \\\"0x8000\\\"}\", \"meta_data\": \"{}\", \"has_mb_refstate\": 0, \"has_runtime_policy\": 0, \"accept_tpm_hash_algs\": [\"sha512\", \"sha384\", \"sha256\"], \"accept_tpm_encryption_algs\": [\"ecc\", \"rsa\"], \"accept_tpm_signing_algs\": [\"ecschnorr\", \"rsassa\"], \"hash_alg\": \"sha256\", \"enc_alg\": \"rsa\", \"sign_alg\": \"rsassa\", \"verifier_id\": \"default\", \"verifier_ip\": \"127.0.0.1\", \"verifier_port\": 8881, \"severity_level\": 6, \"last_event_id\": \"pcr_validation.invalid_pcr_15\", \"attestation_count\": 53, \"last_received_quote\": 1752152713, \"last_successful_attestation\": 1752152711}} You will also see on the agent logs that it has been revoked:\nJul 10 13:05:13 localhost keylime_agent[1741]: INFO keylime_agent::quotes_handler \u003e GET integrity quote returning 200 response Jul 10 13:05:13 localhost keylime_agent[1741]: INFO keylime_agent \u003e POST invoked from \"192.168.122.31\" with uri /v2.1/notifications/revocation Jul 10 13:05:13 localhost keylime_agent[1741]: INFO keylime_agent::notifications_handler \u003e Received revocation Jul 10 13:05:13 localhost keylime_agent[1741]: WARN keylime_agent::revocation \u003e Revocation certificate not yet available Using revocation keys In order to make the agent revocation process more secure, when registering the agent in the Keylime tenant, you need to pass a --cert YOURCERT|default option when adding the node. This will create a secure payload with the proper keys and scripts to be used for revocation and deliver it to the node. The agent will then use this payload to revoke itself when it detects a policy violation.\nJul 11 07:22:12 localhost keylime_agent[1756]: DEBUG keylime_agent::keys_handler \u003e Sent RunPayload message to payloads worker Jul 11 07:22:12 localhost keylime_agent[1756]: DEBUG keylime_agent::keys_handler \u003e Sent RunPayload message to payloads worker Jul 11 07:22:12 localhost keylime_agent[1756]: INFO keylime_agent::payloads \u003e Successfully decrypted payload Jul 11 07:22:12 localhost keylime_agent[1756]: INFO keylime_agent::payloads \u003e Wrote payload decryption key to \"/var/lib/keylime/secure/unzipped/derived_tci_key\" Jul 11 07:22:12 localhost keylime_agent[1756]: INFO keylime_agent::payloads \u003e Wrote decrypted payload to \"/var/lib/keylime/secure/unzipped/decrypted_payload\" Jul 11 07:22:12 localhost keylime_agent[1756]: INFO keylime_agent::payloads \u003e Unzipping payload decrypted_payload to \"/var/lib/keylime/secure/unzipped\" Jul 11 07:22:12 localhost keylime_agent[1756]: INFO keylime_agent::payloads \u003e Payload init script indicated: autorun.sh Jul 11 07:22:12 localhost keylime_agent[1756]: INFO keylime_agent::payloads \u003e Running script: \"/var/lib/keylime/secure/unzipped/autorun.sh\" Jul 11 07:22:12 localhost keylime_agent[1756]: INFO keylime_agent::payloads \u003e No payload script autorun.sh found in /var/lib/keylime/secure/unzipped Jul 11 07:22:12 localhost keylime_agent[1756]: DEBUG keylime_agent::payloads \u003e Sending PayloadDecrypted message to revocation worker Jul 11 07:22:12 localhost keylime_agent[1756]: INFO keylime_agent::payloads \u003e Successfully executed encrypted payload Jul 11 07:22:12 localhost keylime_agent[1756]: INFO keylime_agent::revocation \u003e Loading the revocation certificate from /var/lib/keylime/secure/unzipped/RevocationNotifier-cert.crt The agent will use that certificate to confirm that the revocation order came from the Keylime tenant and not from an attacker. The agent will also use the revocation script to revoke itself, if any was configured.\nJul 11 07:32:07 localhost keylime_agent[1756]: INFO keylime_agent::notifications_handler \u003e Received revocation Jul 11 07:32:07 localhost keylime_agent[1756]: DEBUG keylime_agent::revocation \u003e Revocation signature validated for revocation: {\"agent_id\":\"61388a67-baa4-4f2b-8221-d539b7b4d98b\",\"context\":\"{\\\"message\\\": \\\"Hash not found in runtime policy\\\", \\\"got\\\": \\\"97356fb8edc035052bac594306d0fdcef99f28e42703acaafa8d7940868b60d0\\\", \\\"expected\\\": [\\\"046209cb4acb79d533a9c6a9845766d4f435bc059f45421ed6e1cceb9c2dcc2e\\\"]}\",\"event_id\":\"ima.validation.ima-ng.runtime_policy_hash\",\"event_time\":\"Fri Jul 11 09:32:08 2025\",\"ip\":\"192.168.122.47\",\"meta_data\":\"{\\\"cert_serial\\\": 4, \\\"subject\\\": \\\"OU=53,O=MITLL,L=Lexington,ST=MA,CN=61388a67-baa4-4f2b-8221-d539b7b4d98b,C=US\\\"}\",\"port\":9002,\"severity_label\":\"emergency\",\"tpm_policy\":\"{\\\"mask\\\": \\\"0x400\\\"}\",\"type\":\"revocation\"} This is a very basic example of how to use keylime in Kairos. You can extend this to use more complex policies and more complex attestation mechanisms. As Keylime is a very flexible tool, you can use it in many different ways to secure your infrastructure. Here are some more links to the Keylime documentation to get you started:\nKeylime documentation Red Hat Keylime documentation Suse Keylime documentation ","categories":"","description":"This section describes an example on how to create a custom derivative with the Keylime agent","excerpt":"This section describes an example on how to create a custom derivative …","ref":"/docs/examples/keylime/","tags":"","title":"Keylime agent"},{"body":" Warning This tutorial will download an AI model which is around 4Gib.\nKeep in mind that AI models are performance hungry. Performance on a VM can be poor depending on your host CPU. To get the best performance, use a bare-metal machine.\nWelcome to the guide on using LocalAI with Kairos and K3s on your nodes!\nBut first, what is LocalAI?\nLocalAI is a self-hosted, community-driven simple local OpenAI-compatible API written in go. Can be used as a drop-in replacement for OpenAI, running on CPU with consumer-grade hardware. Supports ggml compatible models, for instance: LLaMA, alpaca, gpt4all, vicuna, koala, gpt4all-j, cerebras. This means that you can have the power of an AI model in your Edge-Kubernetes cluster, and it can all be easily done thanks to GPT4ALL models, LocalAI and Kairos!\nTo get started, you’ll need to use standard images, which include k3s. Follow the Installation documentation, and use the following configuration:\n#cloud-config hostname: localai-{{ trunc 4 .MachineID }} users: - name: kairos passwd: kairos groups: - admin ssh_authorized_keys: - github:mauromorales k3s: enabled: true bundles: - targets: - run://quay.io/kairos/community-bundles:LocalAI_latest localai: serviceType: LoadBalancer There are a few things to note in this configuration file:\nIn the k3s block, we set it as enabled: true because we want Kairos to run k3s for us. In the bundles block, we add a target pointing to the community bundle of LocalAI. We add a localai block, where we specify the serviceType: LoadBalancer so we can access LocalAI’s API outside the cluster. And that’s it! You should now have LocalAI and K3s set up on your Kairos node.\nThe first thing you want to check is which models you have available. By default, the LocalAI Kairos bundle downloads the ggml-gpt4all-j.bin model available from gpt4all.\nWarning Remember to change the IP with your own. $ curl http://192.168.122.177:8080/v1/models {\"object\":\"list\",\"data\":[{\"id\":\"ggml-gpt4all-j.bin\",\"object\":\"model\"}]} With the name of the model, we can now give it a go with:\n$ curl http://192.168.122.177:8080/v1/completions -H \"Content-Type: application/json\" -d '{ \"model\": \"ggml-gpt4all-j.bin\", \"prompt\": \"Is there anybody out there?\", \"temperature\": 0.7 }' {\"model\":\"ggml-gpt4all-j.bin\",\"choices\":[{\"text\":\"As an AI language model, I do not have the ability to determine whether there is anybody out there or not. However, I can say that the concept of \\\"out there\\\" is a philosophical and cultural concept that refers to the idea that there are other beings or entities beyond our own world. Some people believe in the existence of extraterrestrial life, while others do not.\"}]} And voilà! There you have it, a GPT model running on your k3s node.\n","categories":"","description":"This section describe examples on how to deploy Kairos with k3s and LocalAI","excerpt":"This section describe examples on how to deploy Kairos with k3s and …","ref":"/docs/examples/localai/","tags":"","title":"LocalAI"},{"body":"In the example below we will use a bare metal host to provision a Kairos cluster in the local network using one master node and one worker node.\nInstallation For this example we will use a standard image which contains a Kubernetes distribution. You can choose between k0s and k3s as the distribution to use. Follow the Installation documentation with the configurations provided on this page. Make sure to choose the one that matches the image you are using.\nConfiguration On all nodes, we will deploy a kairos user with the password kairos and the admin group. We will also add the public keys of the users that will be allowed to access the nodes.\nMaster node On the master node configuration, we will enable the Kubernetes distribution and configure it. We will also include a manifest with a simple Nginx deployment that will be installed on the cluster once it’s running. You can change the manifest to the one of your own application or remove it if you don’t need it.\nk3s k0s #cloud-config hostname: metal-{{ trunc 4 .MachineID }} users: - name: kairos # Change to your own user passwd: kairos # Change to your own password groups: - admin # This user needs to be part of the admin group ssh_authorized_keys: - github:\u003cYOUR_GITHUB_USER\u003e # replace with your github user k3s: enabled: true args: - --disable=traefik,servicelb # will disable traefik and servicelb write_files: - path: /var/lib/rancher/k3s/server/manifests/nginx.yaml permissions: \"0644\" content: | apiVersion: v1 kind: Namespace metadata: name: nginx --- apiVersion: apps/v1 kind: Deployment metadata: name: nginx-deployment namespace: nginx spec: selector: matchLabels: app: nginx replicas: 3 template: metadata: labels: app: nginx spec: containers: - name: nginx image: nginx:latest ports: - containerPort: 80 #cloud-config hostname: metal-{{ trunc 4 .MachineID }} users: - name: kairos # Change to your own user passwd: kairos # Change to your own password groups: - admin # This user needs to be part of the admin group ssh_authorized_keys: - github:\u003cYOUR_GITHUB_USER\u003e # replace with your github user k0s: enabled: true write_files: - path: /var/lib/k0s/manifests/nginx/nginx.yaml permissions: \"0644\" content: | apiVersion: v1 kind: Namespace metadata: name: nginx --- apiVersion: apps/v1 kind: Deployment metadata: name: nginx-deployment namespace: nginx spec: selector: matchLabels: app: nginx replicas: 3 template: metadata: labels: app: nginx spec: containers: - name: nginx image: nginx:latest ports: - containerPort: 80 Worker nodes With the master node up and running, we can configure the worker nodes\nk3s k0s #cloud-config hostname: metal-{{ trunc 4 .MachineID }} users: - name: kairos # Change to your own user passwd: kairos # Change to your own password groups: - admin # This user needs to be part of the admin group ssh_authorized_keys: - github:\u003cYOUR_GITHUB_USER\u003e # replace with your github user k3s-agent: # Warning: the key is different from the master node one enabled: true args: - --with-node-id # will configure the agent to use the node ID to communicate with the master node env: K3S_TOKEN: \"\u003cMASTER_SERVER_TOKEN\u003e\" # /var/lib/rancher/k3s/server/node-token from the master node K3S_URL: https://\u003cMASTER_SERVER_IP\u003e:6443 # Same IP that you use to log into your master node #cloud-config hostname: metal-{{ trunc 4 .MachineID }} users: - name: kairos # Change to your own user passwd: kairos # Change to your own password groups: - admin # This user needs to be part of the admin group ssh_authorized_keys: - github:\u003cYOUR_GITHUB_USER\u003e # replace with your github user k0s-worker: enabled: true args: - --token-file /etc/k0s/token write_files: - path: /etc/k0s/token permissions: 0644 content: | \u003cTOKEN\u003e # generate it on your master node by running `k0s token create --role=worker` k3s k0s To find out more about args configuration from k3s, follow their server and agent documentation.\nTo learn more about a multi-node setup with k0s, follow their multi-node documentation.\n","categories":"","description":"This section describe examples on how to deploy Kairos as a multi-node cluster","excerpt":"This section describe examples on how to deploy Kairos as a multi-node …","ref":"/docs/examples/multi-node/","tags":"","title":"Manual Multi-Node Cluster"},{"body":" K3s Please refer to the k3s HA documentation. K0s Please refer to the k0s multi-node manual install documentation. Production Considerations This example is for learning purposes. In production environments, it’s recommended to use a load balancer in front of the highly available control plane nodes rather than exposing all control plane nodes directly. For a production-ready setup with a load balancer, see our Self-coordinating P2P Multi-Node Cluster with High Availability and KubeVIP example. This document describes how to configure Kairos with either k3s or k0s by following the same documentation outline. It is implied that you are using a Kairos version with either k3s or k0s included in the standard images.\nNew cluster To run Kairos in this mode, you must have an odd number of server nodes.\nThe first control plane node that we will launch is considered the cluster initializer.\nk3s k0s #cloud-config hostname: metal-{{ trunc 4 .MachineID }} users: - name: kairos # Change to your pass here passwd: kairos groups: - admin #ssh_authorized_keys: ## Add your github user here! #- github:mudler k3s: enabled: true args: - --cluster-init # Token will be generated if not specified at /var/lib/rancher/k3s/server/node-token env: K3S_TOKEN: \"TOKEN_GOES_HERE\" #cloud-config hostname: metal-{{ trunc 4 .MachineID }} users: - name: kairos # Change to your own user passwd: kairos # Change to your own password groups: - admin # This user needs to be part of the admin group ssh_authorized_keys: - github:\u003cYOUR_GITHUB_USER\u003e # replace with your github user k0s: enabled: true After launching the first control plane, join the others\nk3s k0s #cloud-config hostname: metal-{{ trunc 4 .MachineID }} users: - name: kairos # Change to your pass here passwd: kairos groups: - admin ssh_authorized_keys: # Add your github user here! - github:mudler k3s: enabled: true args: - --server https://\u003cip or hostname of server1\u003e:6443 env: K3S_TOKEN: \"TOKEN_GOES_HERE\" #cloud-config hostname: metal-{{ trunc 4 .MachineID }} users: - name: kairos # Change to your own user passwd: kairos # Change to your own password groups: - admin # This user needs to be part of the admin group ssh_authorized_keys: - github:\u003cYOUR_GITHUB_USER\u003e # replace with your github user k0s-worker: enabled: true args: - --token-file /etc/k0s/token write_files: - path: /etc/k0s/token permissions: 0644 content: | \u003cTOKEN\u003e # generate it on your cluster init node by running `k0s token create --role=controller` Now you have a highly available control plane.\nJoining a worker Joining additional worker nodes to the cluster follows the same procedure as a single-node cluster.\nk3s k0s #cloud-config hostname: metal-{{ trunc 4 .MachineID }} users: - name: kairos # Change to your pass here passwd: kairos groups: - admin #ssh_authorized_keys: ## Add your github user here! #- github:mudler k3s-agent: enabled: true env: K3S_TOKEN: \"TOKEN_GOES_HERE\" K3S_URL: \"https://\u003cip or hostname of server1\u003e:6443\" #cloud-config hostname: metal-{{ trunc 4 .MachineID }} users: - name: kairos # Change to your own user passwd: kairos # Change to your own password groups: - admin # This user needs to be part of the admin group ssh_authorized_keys: - github:\u003cYOUR_GITHUB_USER\u003e # replace with your github user k0s-worker: enabled: true args: - --token-file /etc/k0s/token write_files: - path: /etc/k0s/token permissions: 0644 content: | \u003cTOKEN\u003e # generate it on your master node by running `k0s token create --role=worker` External DB K0s This section hasn’t been reworked to be used with the k0s distribution yet. K3s requires two or more server nodes for this HA configuration. See the K3s requirements guide for minimum machine requirements.\nWhen running the k3s as a server, you must set the datastore-endpoint parameter so that K3s knows how to connect to the external datastore.\n#cloud-config hostname: metal-{{ trunc 4 .MachineID }} users: - name: kairos # Change to your pass here passwd: kairos groups: - admin #ssh_authorized_keys: ## Add your github user here! #- github:mudler k3s: enabled: true args: - --datastore-endpoint mysql://username:password@tcp(hostname:3306)/database-name # Token will be generated if not specified at /var/lib/rancher/k3s/server/node-token env: K3S_TOKEN: \"TOKEN_GOES_HERE\" Resources High Availability with Embedded DB High Availability with External DB ","categories":"","description":"This section contains instructions how to deploy Kairos with a High Available control-plane for K3s","excerpt":"This section contains instructions how to deploy Kairos with a High …","ref":"/docs/examples/ha/","tags":"","title":"Manual Multi-Node High Availability Cluster"},{"body":"In the example below we will use a bare metal host to provision a Kairos node in the local network using a single machine.\nInstallation For this example we will use a standard image which contains a Kubernetes distribution. You can choose between k0s and k3s as the distribution to use. Follow the Installation documentation with the configurations provided on this page. Make sure to choose the one that matches the image you are using.\nConfiguration We will deploy a kairos user with the password kairos and the admin group. We will also add the public keys of the users that will be allowed to access the nodes. We will enable the Kubernetes distribution and configure it. And also include a manifest with a simple Nginx deployment that will be installed on the cluster automatically. See docs for more information. You can change the manifest to the one of your own application or remove it if you don’t need it.\nk3s k0s #cloud-config users: - name: kairos # Change to your own user passwd: kairos # Change to your own password groups: - admin # This user needs to be part of the admin group ssh_authorized_keys: - github:\u003cYOUR_GITHUB_USER\u003e # replace with your github user k3s: enabled: true write_files: - path: /var/lib/rancher/k3s/server/manifests/nginx.yaml permissions: \"0644\" content: | apiVersion: v1 kind: Namespace metadata: name: nginx --- apiVersion: apps/v1 kind: Deployment metadata: name: nginx-deployment namespace: nginx spec: selector: matchLabels: app: nginx replicas: 3 template: metadata: labels: app: nginx spec: containers: - name: nginx image: nginx:latest ports: - containerPort: 80 #cloud-config users: - name: kairos # Change to your own user passwd: kairos # Change to your own password groups: - admin # This user needs to be part of the admin group ssh_authorized_keys: - github:\u003cYOUR_GITHUB_USER\u003e # replace with your github user k0s: args: - --single enabled: true write_files: - path: /var/lib/k0s/manifests/my-nginx/my-nginx.yaml permissions: \"0644\" content: | apiVersion: v1 kind: Namespace metadata: name: nginx --- apiVersion: apps/v1 kind: Deployment metadata: name: nginx-deployment namespace: nginx spec: selector: matchLabels: app: nginx replicas: 3 template: metadata: labels: app: nginx spec: containers: - name: nginx image: nginx:latest ports: - containerPort: 80 ","categories":"","description":"This section describe examples on how to deploy Kairos single-node cluster","excerpt":"This section describe examples on how to deploy Kairos single-node …","ref":"/docs/examples/single-node/","tags":"","title":"Manual Single-Node Cluster"},{"body":"Welcome to the guide on using MetalLB with Kairos and K3s on a bare metal host!\nIn this tutorial, we’ll walk through the steps of setting up a Kairos node on your local network using the 192.168.1.10-192.168.1.20 IP range, with MetalLB and K3s.\nBut first, let’s talk a little bit about what MetalLB and K3s are. MetalLB is a load balancer implementation for bare metal Kubernetes clusters that uses standard routing protocols. It’s particularly useful when used with K3s in Kairos, as it provides load balancing for bare metal clusters and helps manage IP addresses within the cluster. K3s is a lightweight Kubernetes distribution that is easy to install and maintain.\nNow that you have an understanding of what we’ll be working with, let’s dive into the installation process.\nCheck out the bundle example to configure MetalLB with bundles. Bundles provides a streamlined way to publish and re-use configuration between nodes.\nTo get started, you’ll need to use the standard images, which include k3s. We’ll be using the k3s manifest method to deploy MetalLB.\nFollow the Installation documentation, and use the following cloud config file with Kairos:\n#cloud-config hostname: metal-{{ trunc 4 .MachineID }} users: - name: kairos # Change to your pass here passwd: kairos groups: - admin ssh_authorized_keys: # Add your github user here! - github:mudler k3s: enabled: true args: - --disable=traefik,servicelb # Additional manifests that are applied by k3s on boot write_files: - path: /var/lib/rancher/k3s/server/manifests/metallb.yaml permissions: \"0644\" content: | apiVersion: v1 kind: Namespace metadata: name: metallb-system --- apiVersion: helm.cattle.io/v1 kind: HelmChart metadata: name: metallb namespace: metallb-system spec: chart: https://github.com/metallb/metallb/releases/download/metallb-chart-0.13.7/metallb-0.13.7.tgz - path: /var/lib/rancher/k3s/server/manifests/addresspool.yaml permissions: \"0644\" content: | apiVersion: metallb.io/v1beta1 kind: IPAddressPool metadata: name: default namespace: metallb-system spec: addresses: - 192.168.1.10-192.168.1.20 --- apiVersion: metallb.io/v1beta1 kind: L2Advertisement metadata: name: default namespace: metallb-system spec: ipAddressPools: - default There are a few things to note in this configuration file:\nIn the k3s block, we use the --disable flag to disable traefik and servicelb, which are the default load balancers for k3s. In the write_files block, we write manifests (in /var/lib/rancher/k3s/server/manifests/ see docs) to deploy MetalLB and configure it to use the 192.168.1.10-192.168.1.20 IP range. Make sure to choose an IP range that doesn’t interfere with your local DHCP network. And that’s it! You should now have MetalLB and K3s set up on your Kairos node.\nResources TNS blog post ","categories":"","description":"This section describe examples on how to deploy Kairos with k3s and MetalLB","excerpt":"This section describe examples on how to deploy Kairos with k3s and …","ref":"/docs/examples/metallb/","tags":"","title":"MetalLB"},{"body":" Network This feature is experimental and has only been tested on local setups. Run in production servers at your own risk. Feedback and bug reports are welcome, as we are improving the p2p aspects of Kairos. Deploying Kubernetes at the Edge can be a complex and time-consuming process, especially when it comes to setting up and managing multiple clusters. To make this process easier, Kairos leverages peer-to-peer technology to automatically coordinate and create Kubernetes clusters without the need of a control management interface.\nTo leverage p2p self-coordination capabilities of Kairos, you will need to configure the network_token under the p2p configuration block in your cloud-config file. Once you have set this, Kairos will handle the configuration of each node.\nDescription In the following example we are going to bootstrap a new multi-node, single cluster with Kairos. We will use at least 2 nodes, one as a master and one as a worker. Note how we don’t specify any role, or either pin any IP in the following configurations.\nWe will first create a cloud config file for our deployment, and then run AuroraBoot locally. We then start 2 VMs configured for netbooting.\nPrepare your cloud-config file Consider the following example, which uses cloud-config to automatically configure the cluster:\nWe start by creating a cloud config file locally, that could look similar to this:\n#cloud-config hostname: kairoslab-{{ trunc 4 .MachineID }} users: - name: kairos passwd: kairos groups: - admin ssh_authorized_keys: # Replace with your github user and un-comment the line below: - github:mudler - github:mauromorales p2p: disable_dht: true # Enable for LAN-only clusters network_token: \"\" As we want the installation to be triggered automatically, we add also the install block:\ninstall: auto: true device: \"auto\" reboot: true In order to leverage p2p and automatic node co-ordination, we need to generate a unique pre-shared token that will be used by all the nodes that we want to be part of our cluster.\nWe can generate a network token by using the edgevpn images, by running it locally:\n$ docker run -ti --rm quay.io/mudler/edgevpn -b -g b3RwOgogIGRodDoKICAgIGludGVydmFsOiA5MDAwCiAgICBrZXk6IGtkdGtoY21sMHVJM2hzVUFUMXpUY1B2aDhBblkzNDZUbHJ3NklVRmUxYUoKICAgIGxlbmd0aDogNDMKICBjcnlwdG86CiAgICBpbnRlcnZhbDogOTAwMAogICAga2V5OiBIcEJGaGxxdlFrcTZVd3BPSTBPVkJWQ1daRjNRYlE3WGdDa1R1bnI0cGV3CiAgICBsZW5ndGg6IDQzCnJvb206IGFBUE5oRTdlODgyZUZhM2NMTW56VkM0ZDZjWFdpTU5EYlhXMDE4Skl2Q3oKcmVuZGV6dm91czogOHVzaGhzNnFrTU92U2ZvQmZXMHZPaEY1ZFlodVZlN1Flc00zRWlMM2pNMwptZG5zOiBJZ0ljaGlvRlVYOFN6V1VKQjNXQ0NyT2UzZXZ3YzE4MWVIWm42SmlYZjloCm1heF9tZXNzYWdlX3NpemU6IDIwOTcxNTIwCg== This command will generate a network token that we can use in the configuration, which now looks like the following:\n#cloud-config install: auto: true device: \"auto\" reboot: true hostname: kairoslab-{{ trunc 4 .MachineID }} users: - name: kairos passwd: kairos groups: - admin ssh_authorized_keys: - github:mudler - github:mauromorales p2p: disable_dht: true #Enabled by default network_token: \"b3RwOgogIGRodDoKICAgIGludGVydmFsOiA5MDAwCiAgICBrZXk6IGtkdGtoY21sMHVJM2hzVUFUMXpUY1B2aDhBblkzNDZUbHJ3NklVRmUxYUoKICAgIGxlbmd0aDogNDMKICBjcnlwdG86CiAgICBpbnRlcnZhbDogOTAwMAogICAga2V5OiBIcEJGaGxxdlFrcTZVd3BPSTBPVkJWQ1daRjNRYlE3WGdDa1R1bnI0cGV3CiAgICBsZW5ndGg6IDQzCnJvb206IGFBUE5oRTdlODgyZUZhM2NMTW56VkM0ZDZjWFdpTU5EYlhXMDE4Skl2Q3oKcmVuZGV6dm91czogOHVzaGhzNnFrTU92U2ZvQmZXMHZPaEY1ZFlodVZlN1Flc00zRWlMM2pNMwptZG5zOiBJZ0ljaGlvRlVYOFN6V1VKQjNXQ0NyT2UzZXZ3YzE4MWVIWm42SmlYZjloCm1heF9tZXNzYWdlX3NpemU6IDIwOTcxNTIwCg==\" Change also accordingly the users that can access to the machine:\nssh_authorized_keys: - github:mudler \u003c--- put your GitHub handle here Provisioning with AuroraBoot We now can run AuroraBoot with quay.io/kairos/@flavor:@flavorRelease-standard-amd64-generic-master-k3sv1.33.4-k3s1 to provision openSUSE Leap machines with those k3s and kairos versions.\nAuroraBoot takes cloud-config files also from STDIN, so we will pipe the configuration file to it, and specify the container image that we want to use for our nodes:\ncat \u003c\u003cEOF | docker run --rm -i --net host quay.io/kairos/auroraboot \\ --cloud-config - \\ --set \"container_image=quay.io/kairos/@flavor:@flavorRelease-standard-amd64-generic-master-k3sv1.33.4-k3s1\" #cloud-config # https://github.com/kairos-io/kairos/issues/885 config_url: \"\" install: auto: true device: \"auto\" reboot: true hostname: kairoslab-{{ trunc 4 .MachineID }} users: - name: kairos passwd: kairos groups: - admin ssh_authorized_keys: - github:mudler - github:mauromorales p2p: disable_dht: true #Enabled by default network_token: \"b3RwOgogIGRodDoKICAgIGludGVydmFsOiA5MDAwCiAgICBrZXk6IGtkdGtoY21sMHVJM2hzVUFUMXpUY1B2aDhBblkzNDZUbHJ3NklVRmUxYUoKICAgIGxlbmd0aDogNDMKICBjcnlwdG86CiAgICBpbnRlcnZhbDogOTAwMAogICAga2V5OiBIcEJGaGxxdlFrcTZVd3BPSTBPVkJWQ1daRjNRYlE3WGdDa1R1bnI0cGV3CiAgICBsZW5ndGg6IDQzCnJvb206IGFBUE5oRTdlODgyZUZhM2NMTW56VkM0ZDZjWFdpTU5EYlhXMDE4Skl2Q3oKcmVuZGV6dm91czogOHVzaGhzNnFrTU92U2ZvQmZXMHZPaEY1ZFlodVZlN1Flc00zRWlMM2pNMwptZG5zOiBJZ0ljaGlvRlVYOFN6V1VKQjNXQ0NyT2UzZXZ3YzE4MWVIWm42SmlYZjloCm1heF9tZXNzYWdlX3NpemU6IDIwOTcxNTIwCg==\" EOF Booting and access the cluster Start the Machines (VM, or baremetal) with Netboot ( see also here ) and wait for the installation to finish.\nAfterward, you should be able to ssh to one of the machines and be able to use your Kubernetes cluster:\n$ ssh kairos@IP $ sudo su - $ k9s Warning If k9s doesn’t automatically pick up the kubeconfig, you can manually fetch it and pass it to k9s:\n$ kairos get-kubeconfig \u003e kubeconfig $ KUBECONFIG=kubeconfig k9s Notes By default, the Kubernetes API endpoint is not exposed outside the VPN. This is an opinionated configuration from Kairos. To check out configurations without VPN, see also the KubeVIP example.\nTroubleshooing During the first-boot, you can check the provisioning status by looking at the kairos-agent logs:\n$ systemctl status kairos-agent $ journalctl -fu kairos-agent See also Installation with p2p P2P Architecture ","categories":"","description":"Full end to end example to bootstrap a self-coordinated cluster with Kairos and AuroraBoot","excerpt":"Full end to end example to bootstrap a self-coordinated cluster with …","ref":"/docs/examples/p2p_e2e/","tags":"","title":"P2P Multi-Node Cluster Provisioned via Netboot"},{"body":" Info This tutorial is based on Debian Bookworm. Unit file configurations vary across distributions, and we are not able to test them all, but they should be easily adaptable from this tutorial. Introduction Some use cases require a stage to run after the K3s servers are up, such as applying manifests to the Kubernetes cluster, sending health checks, or any other use case. Using systemd units, we are able to run a stage once a service is started, which can be used for K3s and K3s-server to run steps after the K3s server is up and ready to accept requests.\nRequirements A custom image that adds a systemd unit file to run our stage Steps Build the custom derivative artifact Build an iso from that artifact Install the iso Check that the stage runs and works Building the custom derivative We will keep this short as there are more docs that go more in-depth into building your derivatives than this tutorial such as the Customizing page.\nThe main step is to create an image that has the systemd units we need to run our stages.\nFROM quay.io/kairos/debian:bookworm-standard-amd64-generic-v3.2.1-k3sv1.29.9-k3s1 # Add unit files # This can be adapted to any service running on the system, such as k3s-server to run a stage when the server is ready RUN \u003c\u003cEOF cat \u003e\u003e /etc/systemd/system/k3s-ready.service [Unit] Description=Kairos k3s booted stage runner (k3s) After=k3s.service [Service] Type=oneshot # provider-kairos.bootstrap.after.k3s-ready is run when k3s is up ExecStart=kairos-agent run-stage provider-kairos.bootstrap.after.k3s-ready TimeoutSec=30 [Install] # Start this service (to run the stage) when the k3s service is ready WantedBy=k3s.service EOF # Enable services. early service is the one on the initramfs RUN systemctl enable k3s-ready.service We use a systemd unit to hook into the lifecycle of the k3s service and run our stage when needed. Crucially, the WantedBy=k3s.service line allows our service to be automatically started when k3s is ready.\nThen we generate a new artifact using that dockerfile:\n$ docker build -t k3s-stage-kairos . [+] Building 0.6s (7/7) FINISHED docker:default =\u003e [internal] load build definition from Dockerfile 0.1s =\u003e =\u003e transferring dockerfile: 793B 0.0s =\u003e [internal] load metadata for quay.io/kairos/debian:bookworm-standard-amd64-generic-v3.2.1-k3sv1.29.9-k3s1 0.2s =\u003e [internal] load .dockerignore 0.1s =\u003e =\u003e transferring context: 2B 0.0s =\u003e [1/3] FROM quay.io/kairos/debian:bookworm-standard-amd64-generic-v3.2.1-k3sv1.29.9-k3s1@sha256:6601bbdfb4c5d2 0.0s =\u003e CACHED [2/3] RUN \u003c\u003cEOF cat \u003e\u003e /etc/systemd/system/k3s-ready.service 0.0s =\u003e CACHED [3/3] RUN systemctl enable k3s-ready.service 0.0s =\u003e exporting to image 0.0s =\u003e =\u003e exporting layers 0.0s =\u003e =\u003e writing image sha256:08b48d7fa78ad75755e47b18d4401c06405174bfc047f43352c13ee84662fd4f 0.0s =\u003e =\u003e naming to docker.io/library/k3s-stage-kairos Build an iso from that artifact Again, this tutorial does not cover this part deeply as there already are docs that provide a deep insight into custom images such as the the AuroraBoot page.\n$ docker run -v \"$PWD\"/build-iso:/tmp/auroraboot -v /var/run/docker.sock:/var/run/docker.sock --rm -ti quay.io/kairos/auroraboot --set container_image=\"docker://k3s-stage-kairos\" --set \"disable_http_server=true\" --set \"disable_netboot=true\" --set \"state_dir=/tmp/auroraboot\" 2:38PM INF Pulling container image 'k3s-stage-kairos' to '/tmp/auroraboot/temp-rootfs' (local: true) 2:38PM INF Generating iso 'kairos' from '/tmp/auroraboot/temp-rootfs' to '/tmp/auroraboot/build' Install the iso Then we burn the resulting ISO to a DVD or USB stick and boot it normally.\nWe can check if our newly implemented stage is run correctly with k3s:\n#cloud-config k3s: enabled: true users: - name: kairos # Change to your pass here passwd: kairos groups: - admin stages: 'provider-kairos.bootstrap.after.k3s-ready': - name: \"Create /tmp/k3s-ready file\" commands: - \"touch /tmp/k3s-ready\" Check that the service is enabled and works Once the system has been installed, the k3s-ready service is run after the k3s service has been started and running. After running, it should be in the status Inactive, and the /tmp/k3s-ready file should exist once k3s has started.\n","categories":"","description":"This section describes a method to run stages with k3s.","excerpt":"This section describes a method to run stages with k3s.","ref":"/docs/examples/k3s-stages/","tags":"","title":"Run stages along with K3s"},{"body":"","categories":"","description":"","excerpt":"","ref":"/search/","tags":"","title":"Search Results"},{"body":" Network This feature is experimental and has only been tested on local setups. Run in production servers at your own risk. Feedback and bug reports are welcome, as we are improving the p2p aspects of Kairos. Join an EdgeVPN network Make sure that you join an existing EdgeVPN network with other machines (not necessarily Kubernetes nodes) otherwise the single-node cluster will never get an EdgeVPN IP and therefore won’t be configurable. Installing Kairos with P2P support on a single-node cluster requires a few specific steps. To begin, it’s important to note that in a single-node scenario, the role must be enforced to a specific role. In a non-HA (high availability) setup, that role can be either master or worker. In a single-node cluster, there will be only one master node that needs to be configured explicitly.\nTo set up a single-node cluster over P2P, consider the following example, which uses cloud-config to automatically configure the cluster:\n#cloud-config hostname: kairoslab-{{ trunc 4 .MachineID }} users: - name: kairos passwd: kairos groups: - admin ssh_authorized_keys: # Add your github user here! - github:mudler p2p: role: \"master\" # Disabling DHT makes co-ordination to discover nodes only in the local network disable_dht: true #Enabled by default # network_token is the shared secret used by the nodes to co-ordinate with p2p. # Setting a network token implies auto.enable = true. # To disable, just set auto.enable = false network_token: \"\" Warning One important note is that this example requires the YAML format when editing the configuration file, and that the indentation needs to be accurate, otherwise the configuration will fail. The above cloud-config configures the hostname, creates a new user kairos, and sets the role to master. Additionally, it disables DHT (distributed hash table) to make the VPN functional only within the local network and use mDNS for discovery. If you wish to make the VPN work across different networks, you can set disable_dht to false or unset it.\nThe network_token field is a shared secret used by the nodes to coordinate with P2P. Setting a network token implies auto.enable. If you wish to disable it, simply set auto.enable to false. To generate a network token, see documentation.\nKeep in mind that, this example is a minimal configuration, and you can add more options depending on your needs. The above configuration can be used as a starting point and can be customized further.\n","categories":"","description":"This documentation page provides instructions on how to install Kairos with P2P support on a single-node cluster","excerpt":"This documentation page provides instructions on how to install Kairos …","ref":"/docs/examples/single-node-p2p/","tags":"","title":"Self-configured P2P Single-Node Cluster"},{"body":" Network This feature is experimental and has only been tested on local setups. Run in production servers at your own risk. Feedback and bug reports are welcome, as we are improving the p2p aspects of Kairos. A multi-node scenario with non-HA is the default peer-to-peer (P2P) configuration in Kairos. To set this up, you will need to configure the network_token under the p2p configuration in your cloud-config file. Once you have set this, Kairos will handle the configuration of each node.\nConsider the following example, which uses cloud-config to automatically configure the cluster:\n#cloud-config hostname: kairoslab-{{ trunc 4 .MachineID }} users: - name: kairos passwd: kairos groups: - admin ssh_authorized_keys: # Replace with your github user and un-comment the line below: # - github:mudler p2p: # Disabling DHT makes co-ordination to discover nodes only in the local network disable_dht: true #Enabled by default # network_token is the shared secret used by the nodes to co-ordinate with p2p. # Setting a network token implies auto.enable = true. # To disable, just set auto.enable = false network_token: \"\" To set up a multi-node P2P scenario with non-HA in Kairos, start by adding your desired network_token under the p2p configuration in the cloud-config file. To generate a network token, see documentation.\nBe sure to set disable_dht to true. This will ensure that coordination to discover nodes only happens on the local network.\nOnce you done with the above step, you can also customize the hostname to your liking by modifying the hostname field, adding your github user to the ssh_authorized_keys field, and adding any other necessary configurations.\n","categories":"","description":"Install Kairos with p2p support, on a multi-node cluster","excerpt":"Install Kairos with p2p support, on a multi-node cluster","ref":"/docs/examples/multi-node-p2p/","tags":"","title":"Self-coordinating P2P multi-node cluster"},{"body":" Network This feature is experimental and has only been tested on local setups. Run in production servers at your own risk. Feedback and bug reports are welcome, as we are improving the p2p aspects of Kairos. Production Considerations This example is for learning purposes. In production environments, it’s recommended to use a load balancer in front of the highly available control plane nodes rather than exposing all control plane nodes directly. For a production-ready setup with a load balancer, see our Self-coordinating P2P Multi-Node Cluster with High Availability and KubeVIP example. To enable automatic HA rollout, enable the p2p.auto.ha.enable option in your cloud-config, and set up a number of master_nodes. The number of master_nodes is the number of additional masters in addition to the initial HA role. There will always be a minimum of 1 master, which is already taken into account. For example, setting up master_nodes to two will result in a total of 3 master nodes in your cluster.\nTo make this process even easier, Kairos automatically configures each node in the cluster from a unique cloud-config. This way, you don’t have to manually configure each node, but provide instead a config file for all of the machines during Installation.\nHere is an example of what your cloud-config might look like:\n#cloud-config hostname: kairoslab-{{ trunc 4 .MachineID }} users: - name: kairos ssh_authorized_keys: # Replace with your github user and un-comment the line below: # - github:mudler p2p: # Disabling DHT makes co-ordination to discover nodes only in the local network disable_dht: true #Enabled by default # network_token is the shared secret used by the nodes to co-ordinate with p2p. # Setting a network token implies auto.enable = true. # To disable, just set auto.enable = false network_token: \"\" # Automatic cluster deployment configuration auto: # Enables Automatic node configuration (self-coordination) # for role assignment enable: true # HA enables automatic HA roles assignment. # A master cluster init is always required, # Any additional master_node is configured as part of the # HA control plane. # If auto is disabled, HA has no effect. ha: # Enables HA control-plane enable: true # Number of HA additional master nodes. # A master node is always required for creating the cluster and is implied. # The setting below adds 2 additional master nodes, for a total of 3. master_nodes: 2 Note: In order for the automatic HA rollout to work, you need to generate a network token. You can find more information on how to do this in the dedicated section.\n","categories":"","description":"Kairos makes it easy to configure automatic High Availability (HA) in your cluster by using cloud-config. With just a few simple steps, you can have a fully-functioning HA setup in your cluster.","excerpt":"Kairos makes it easy to configure automatic High Availability (HA) in …","ref":"/docs/examples/multi-node-p2p-ha/","tags":"","title":"Self-coordinating P2P Multi-Node Cluster with High Availability"},{"body":" Network This feature is experimental and has only been tested on local setups. Run in production servers at your own risk. Feedback and bug reports are welcome, as we are improving the p2p aspects of Kairos. K8s Distribution This feature is only working with the k3s distribution. K3s is a lightweight Kubernetes distribution that is easy to install and operate. It’s a great choice for small and edge deployments, but it can also be used to create a high-availability (HA) cluster with the help of KubeVIP. In this guide, we’ll walk through the process of deploying a highly-available k3s cluster with KubeVIP, which provides a high available ip for the control plane.\nThe first step is to set up the cluster. Kairos automatically deploys an HA k3s cluster with KubeVIP to provide a high available ip for the control plane. KubeVIP allows to setup an ElasticIP that is advertized in the node’s network and, as managed as a daemonset in kubernetes it is already running in HA.\nThe difference between this setup is that we just use the p2p network to automatically co-ordinate nodes, while the connection of the cluster is not being routed to a VPN. The p2p network is used for co-ordination, self-management, and used to add nodes on day 2.\nIn order to deploy this setup you need to configure the cloud-config file. You can see the example of the yaml file below. You need to configure the hostname, user and ssh_authorized_keys. You need also to configure kubevip with the elastic ip and the p2p network with the options you prefer.\n#cloud-config hostname: kairoslab-{{ trunc 4 .MachineID }} users: - name: kairos groups: [ \"admin\" ] ssh_authorized_keys: # Replace with your github user and un-comment the line below: # - github:mudler kubevip: eip: \"192.168.1.110\" p2p: # Disabling DHT makes co-ordination to discover nodes only in the local network disable_dht: true #Enabled by default vpn: create: false # defaults to true use: false # defaults to true # network_token is the shared secret used by the nodes to co-ordinate with p2p. # Setting a network token implies auto.enable = true. # To disable, just set auto.enable = false network_token: \"\" # Automatic cluster deployment configuration auto: # Enables Automatic node configuration (self-coordination) # for role assignment enable: true # HA enables automatic HA roles assignment. # A master cluster init is always required, # Any additional master_node is configured as part of the # HA control plane. # If auto is disabled, HA has no effect. ha: # Enables HA control-plane enable: true # Number of HA additional master nodes. # A master node is always required for creating the cluster and is implied. # The setting below adds 2 additional master nodes, for a total of 3. master_nodes: 2 When configuring the p2p section, start by adding your desired network_token under the p2p configuration in the cloud-config file. To generate a network token, see documentation.\nNext, set up an Elastic IP (kubevip.eip) with a free IP in your network. KubeVIP will advertise this IP, so make sure to select an IP that is available for use on your network.\nIn the VPN configuration, the create and use options are disabled, so the VPN setup is skipped and not used to route any traffic into.\n","categories":"","description":"This guide walks through the process of deploying a highly-available, P2P self-coordinated k3s cluster with KubeVIP, which provides a high available Elastic IP for the control plane.","excerpt":"This guide walks through the process of deploying a highly-available, …","ref":"/docs/examples/multi-node-p2p-ha-kubevip/","tags":"","title":"Self-coordinating P2P Multi-Node Cluster with High Availability and KubeVIP"},{"body":"To install Kairos in “Trusted Boot Mode” the machine needs to meet the following requirements:\nMust have a tpm v2.0 chip Must be able to boot in EFI mode (not “legacy BIOS”) Must have 1Gb of RAM or more Must have 40Gb of disk or more The following steps describe how to create a virtual machine suitable for Kairos trusted boot setup, using VirtualBox. As an example workload, LocalAI will be used.\nCreate an ISO If you don’t already have an ISO to boot, you can create one using the following script:\n#!/bin/bash set -e IMAGE=\"${IMAGE:-quay.io/kairos/ubuntu:24.04-core-amd64-generic-master-uki}\" AURORABOOT_IMAGE=\"quay.io/kairos/auroraboot:latest\" OUTDIR=$PWD/build cleanup() { # Run with docker to avoid sudo docker run --rm --entrypoint /bin/bash -v $PWD/build:/result $AURORABOOT_IMAGE -c 'rm -rf /result/*' rm -rf $OUTDIR } generateEfiKeys() { mkdir -p $OUTDIR/keys docker run --rm -v $OUTDIR/keys:/result $AURORABOOT_IMAGE genkey -e 7 --output /result KairosKeys } generateConfig() { mkdir -p $OUTDIR/config cat \u003c\u003c EOF \u003e \"$OUTDIR/config/config.yaml\" #cloud-config users: - name: kairos passwd: kairos groups: - admin install: auto: true reboot: true stages: initramfs: - files: - path: /etc/systemd/system/localai.service permissions: 0644 content: | [Unit] Description=Local AI server After=network-online.target Wants=network-online.target [Service] Type=simple ExecStart=/usr/bin/localai --models-path=\"/usr/local/models\" Restart=on-failure RestartSec=10 [Install] WantedBy=multi-user.target boot: - name: \"Starting localai\" commands: - | systemctl enable localai.service systemctl start localai.service EOF } buildBaseImage() { docker build -t kairos-localai - \u003c\u003cEOF FROM $IMAGE RUN curl -L -o localai https://github.com/mudler/LocalAI/releases/download/v2.22.1/local-ai-Linux-x86_64 RUN chmod +x localai RUN mv localai /usr/bin/localai EOF } buildISO() { docker run --rm \\ -v /var/run/docker.sock:/var/run/docker.sock \\ -v $OUTDIR:/result \\ -v $OUTDIR/keys/:/keys \\ -v $OUTDIR/config/:/config \\ $AURORABOOT_IMAGE build-uki \\ --output-dir /result \\ --keys /keys \\ --output-type iso \\ --boot-branding \"KairosAI\" \\ --overlay-iso /config \\ --extend-cmdline \"rd.immucore.debug rd.debug rd.shell\" \\ oci://kairos-localai } fixPermissions() { docker run --privileged -e USERID=$(id -u) -e GROUPID=$(id -g) --entrypoint /usr/bin/sh -v $OUTDIR:/workdir --rm $AURORABOOT_IMAGE -c 'chown -R $USERID:$GROUPID /workdir' } moveISOFile() { mv build/kairos*.iso ./kairos.iso } echo \"Cleaning up old artifacts\" \u0026\u0026 cleanup echo \"Generating Config\" \u0026\u0026 generateConfig echo \"Generating UEFI keys\" \u0026\u0026 generateEfiKeys echo \"Building base image\" \u0026\u0026 buildBaseImage echo \"Building ISO\" \u0026\u0026 buildISO echo \"Fixing permissions\" \u0026\u0026 fixPermissions echo \"Moving iso to current dir\" \u0026\u0026 moveISOFile If the script succeeds, you will find a .iso file inside $PWD/build and a keys directory with the UEFI keys used to sign the image.\nCreate a VM Warning On macOS you need to make sure you install the VirtualBox Extension Pack to enable USB 2.0 and USB 3.0 support.\nhttps://www.virtualbox.org/wiki/Downloads\n#!/bin/bash set -e VM_NAME=\"KairosAI\" ISO_PATH=$PWD/kairos.iso DISK_PATH=$PWD/Kairos.vdi cleanup() { if VBoxManage list vms | grep -q \"\\\"$VM_NAME\\\"\"; then echo \"VM '$VM_NAME' found. Proceeding with cleanup.\" # Check if the VM is running if VBoxManage list runningvms | grep -q \"\\\"$VM_NAME\\\"\"; then echo \"VM '$VM_NAME' is running. Powering off...\" VBoxManage controlvm \"$VM_NAME\" poweroff sleep 2 # Wait for a moment to ensure the VM powers off fi # Unregister and delete the VM and all associated files echo \"Unregistering and deleting VM '$VM_NAME'...\" VBoxManage unregistervm \"$VM_NAME\" --delete echo \"Cleanup complete. VM '$VM_NAME' and associated files have been deleted.\" else echo \"No VM named '$VM_NAME' found. No action needed.\" fi } createVM() { if [[ $(uname -m) == \"x86_64\" ]]; then ostype=\"Linux_64\" else ostype=\"Linux_arm64\" fi VBoxManage createvm --name \"$VM_NAME\" --ostype \"$ostype\" --register VBoxManage modifyvm \"$VM_NAME\" \\ --memory 10000 \\ --cpus 2 \\ --chipset piix3 \\ --firmware efi64 \\ --nictype1 82540EM \\ --nic1 nat \\ --natpf1 \"guestssh,tcp,,2222,,22\" \\ --natpf1 \"localai,tcp,,8080,,8080\" \\ --tpm-type \"2.0\" \\ --graphicscontroller vmsvga # These don't work of efi # https://www.virtualbox.org/ticket/19364 #--boot1 disk \\ #--boot2 dvd VBoxManage createmedium disk --filename \"$DISK_PATH\" --size 40960 VBoxManage storagectl \"$VM_NAME\" --name \"SATA Controller\" --add sata --controller IntelAhci VBoxManage storageattach \"$VM_NAME\" --storagectl \"SATA Controller\" --port 0 --device 0 --type hdd --medium \"$DISK_PATH\" # Add an IDE controller for CD-ROM and attach the ISO VBoxManage storagectl \"$VM_NAME\" --name \"IDE Controller\" --add ide VBoxManage storageattach \"$VM_NAME\" --storagectl \"IDE Controller\" --port 0 --device 0 --type dvddrive --medium \"$ISO_PATH\" # Hack to allow enrolling the PK key # Not sure why, but only this works. We allow it to boot once, # probably some default UEFI vars are written on the first boot which then # allow us to just enroll our PK key. # Also, it only works after adding disks and all (that's why it's here at the end). # Probably because it needs the cdrom to enroll some of the keys except the PK (?) VBoxManage startvm \"$VM_NAME\" --type=headless \u0026\u0026 sleep 3 VBoxManage controlvm \"$VM_NAME\" poweroff \u0026\u0026 sleep 3 VBoxManage modifynvram $VM_NAME enrollpk ‑‑platform‑key=$PWD/build/keys/PK.der ‑‑owner‑uuid=KairosKeys } usage() { echo \"Usage: $0 {create|start|stop|cleanup}\" } run() { # Check if a command was provided if [ -z \"$1\" ]; then usage exit 1 fi # Determine which command to run based on the first argument case \"$1\" in create) createVM ;; start) VBoxManage startvm \"$VM_NAME\" ;; stop) VBoxManage controlvm \"$VM_NAME\" poweroff ;; cleanup) cleanup ;; *) echo \"Invalid command: $1\" usage exit 1 ;; esac } run $@ Change boot order Unfortunately, Virtual box doesn’t support changing boot order in efi mode (https://www.virtualbox.org/ticket/19364). This means, after installation, the order needs to change manually, in order to boot from the disk instead of the cdrom (iso).\nUse localai web UI Visit http://127.0.0.1:8080 from your browser to use LocalAI web UI\n","categories":"","description":"This section describes how to use Virtual Box to boot Kairos in \"Trusted boot\" mode","excerpt":"This section describes how to use Virtual Box to boot Kairos in …","ref":"/docs/examples/virtual-box/","tags":"","title":"Kairos' Trusted Boot in Virtual Box"},{"body":"Kairos is a powerful, open-source meta-distribution that allows you to easily deploy and manage nodes on your Immutable infrastructure.\nOne key feature of Kairos is the use of its core images, which are released as part of the kairos-io/kairos repository and can be found in the releases section. These core images serve as the foundation for creating downstream images or as an installer for deploying other images during the installation process. In this guide, we’ll take a closer look at using Kairos core images as an installer to deploy other container images.\nGetting started To begin using Kairos core images as an installer, you’ll need to start by using the artifacts from the Kairos core repository. These images do not include the Kubernetes engine, so you’ll need to configure the container image you want to deploy in the install.source field of your cloud config file. A list of available images can be found in our support matrix.\nFor example, let’s say you want to use a standard image. Your cloud config file might look something like this:\n#cloud-config install: # Here we specify the image that we want to deploy source: \"docker:quay.io/kairos/@flavor:@flavorRelease-standard-amd64-generic-master-k3sv1.33.4-k3s1\" Note Looking to install from a private registry OCI image? Check the Private registry auth page. Once you’ve chosen your image, you can move on to the installation process by following the steps outlined in our Installation documentation.\nFor example, a full cloud-config might look like this:\n#cloud-config install: device: \"auto\" auto: true reboot: true # Here we specify the image that we want to deploy source: \"docker:quay.io/kairos/@flavor:@flavorRelease-standard-amd64-generic-master-k3sv1.33.4-k3s1\" hostname: \"test\" users: - name: \"kairos\" passwd: \"kairos\" ssh_authorized_keys: - github:mudler k3s: enable: true Configuring the installation As you move through the installation process, there are a few key points to keep in mind when configuring your cloud config file:\nWe set install.source to the container image that we want to deploy. This can be an image from our support matrix, a custom image or an image from scratch. After the installation is complete, the configuration in the k3s block will take effect. This is because after the installation, the system will boot into the image specified in the install.source field, which in the example above is an image with the Kairos K3s provider, as such the configuration in the k3s block will become active. With these steps, you should now be able to use Kairos core images as an installer to deploy other container images. The process is straightforward and gives you the flexibility to customize your deployments and build custom images as needed.\nYou can also refer our troubleshooting document if you are facing any issue while following the installation process.\n","categories":"","description":"Core images serve as the foundation for creating downstream images or as an installer for deploying other images during the installation process. In this guide, we'll take a closer look at using Kairos core images as an installer to deploy other container images.","excerpt":"Core images serve as the foundation for creating downstream images or …","ref":"/docs/examples/core/","tags":"","title":"Using Kairos Core Images as an Installer"},{"body":"This example is valid with Alpine.\n#cloud-config hostname: metal-{{ trunc 4 .MachineID }} users: - name: kairos # Change to your pass here passwd: kairos ssh_authorized_keys: # Replace with your github user and un-comment the line below: # - github:mudler stages: initramfs: - name: Setup wireless files: # See https://man.archlinux.org/man/connman-service.config.5 - path: /var/lib/connman/home_wifi.config permissions: 0600 content: | [service_home_wifi] Type = wifi Name = your_ssid Passphrase = your_wifi_password IPv4 = dhcp boot: - name: Enable wireless commands: - connmanctl enable wifi This example is valid with openSUSE on a Raspberry Pi.\n#cloud-config hostname: metal-{{ trunc 4 .MachineID }} users: - name: kairos # Change to your pass here passwd: kairos ssh_authorized_keys: # Replace with your github user and un-comment the line below: # - github:mudler stages: initramfs: - name: \"Setup wireless\" files: - path: /etc/wpa_supplicant/wpa_supplicant.conf content: | # This file should be generated using wpa_passphrase ctrl_interface=/var/run/wpa_supplicant ctrl_interface_group=admin network={ ssid=\"$SSID_GOES_HERE\" psk=\"$PSK_GOES_HERE\" } permissions: 0600 owner: 0 group: 0 - path: /etc/systemd/network/20-dhcp-wlan0.network content: | [Match] Name=wlan0 [Network] DHCP=yes [DHCP] ClientIdentifier=mac permissions: 0644 owner: 0 group: 0 boot: - name: \"Enabling wireless\" commands: - | systemctl enable wpa_supplicant@wlan0 systemctl disable wpa_supplicant systemctl stop wpa_supplicant || : systemctl start wpa_supplicant@wlan0 || : Note This is only an example as there is many ways of configuring wifi via cloud config depending on the software installed on the image. Only Alpine (all architectures) and openSUSE (Raspberry Pi) currently have the necessary packages for WiFi support. ","categories":"","description":"This section describe examples on how to deploy Kairos with WiFi","excerpt":"This section describe examples on how to deploy Kairos with WiFi","ref":"/docs/examples/wifi/","tags":"","title":"Configuring WiFi via Cloud-Config"}]